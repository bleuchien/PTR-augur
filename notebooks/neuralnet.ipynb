{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e465583-fc98-4fda-937a-475748037b12",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "*fancy introduction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12e085-6ce6-41e0-8f1e-1d35b23bf358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library dependencies\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import lzma\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import keras_tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff0f512-caac-4cb6-b469-b2bbd23ad3f9",
   "metadata": {},
   "source": [
    "The following methods read the prepared data files from the pre processing step and return the lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ecd0a7-8f25-4b5b-bfb0-b5375b32c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to store data as serialized binary structure lzma compressed\n",
    "def can_pickles(data, filename):\n",
    "    with lzma.LZMAFile(filename, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.DEFAULT_PROTOCOL)\n",
    "\n",
    "# method to retrieve data from a compressed pickle file (created with the method above)\n",
    "def uncan_pickles(filename):\n",
    "    with lzma.LZMAFile(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c330afba-bf62-4e92-ad84-62921e42d31f",
   "metadata": {},
   "source": [
    "## Basic Neural Network\n",
    "\n",
    "*explain it in more detail*\n",
    "\n",
    "X holds a list of sequences one hot encoded\n",
    "\n",
    "y holds a list of PTR values as floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d1d16d-426c-460f-930d-0e91adba9e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the prepared data back\n",
    "X = uncan_pickles('../data/onehot_x_lung.pickle.xz')\n",
    "y = uncan_pickles('../data/onehot_y_lung.pickle.xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2e91af-1782-4909-8191-87442cac791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad add sequence entries to the same length\n",
    "# done here for simplicity to find a good neural network\n",
    "# X_padded = tf.keras.preprocessing.sequence.pad_sequences(X, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3603897-eca1-41be-a833-79e59f4b5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert type of target values from string to float\n",
    "y = np.array(y).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85086f0d-2d79-4ee8-900d-cad1873c1b64",
   "metadata": {},
   "source": [
    "Random sample from X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41525e09-813f-46b8-9f47-ebbd7381510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85471826-2b3a-4371-8441-ae940ba8e580",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694f3792-6d06-48e0-81af-f689cbb35c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of input sequences\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab69104-daa6-4989-89aa-2f6a71f34b0b",
   "metadata": {},
   "source": [
    "### Baseline PTR\n",
    "\n",
    "There is no common sense approach in finding a baseline for the protein-to-mRNA ratio of a particular mRNA sequence. This is what the *Basic Neural Network* approach is for - to determin a baseline and see if a slightly adapted neural network with feature engineered input can provide better predictions.\n",
    "\n",
    "But what can be done is to simply check the value range of the target PTRs, calculate mean and standard deviation. Given that the standard deviation is  small (12.5% of the value range) one can (stupidly) predict the mean value every time. From that it's possible to calculate the Mean Absolute Error (MAE) and compare that to the following neural network output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfa09b2-b288-4618-88a6-7799e1335da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some idea of the range of the PTR in the selected SAMPLE\n",
    "print(np.min(y), np.max(y), np.mean(y), np.std(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9ba3da-9ab0-43f5-a923-63e94c580a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple/dumb baseline mean absolute error of always predicting 4.974\n",
    "mae = np.mean(np.abs(np.array(y) - 4.974))\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94586678-d302-4884-895d-d166c7e6f933",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Split data in train and test subsets and then split the train subset again in train and validation.\n",
    "\n",
    "A simple verification if the X and y correlation are preserved on the split is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ef8f9-7f36-4c73-a448-5abbf294bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train and test sub sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e1f188-4807-406d-b498-57b5589e6187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad test input (variable input is not accepted)\n",
    "# X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b64b18-bbd8-4d4e-b6db-30d063c7eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the train set again in train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39872e7f-c57d-4122-b0ea-3dbb7b24763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the first unique PTR value that is also in y_train\n",
    "train_idx = 0\n",
    "for i in range(len(y)):\n",
    "    count = 0\n",
    "    for l in range(len(y)):\n",
    "        if i != l and y[i] == y[l]:\n",
    "            count += 1\n",
    "            continue\n",
    "    if count == 0:\n",
    "        for m in range(len(y_train)):\n",
    "            if y[i] == y_train[m]:\n",
    "                train_idx = m\n",
    "                break\n",
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad195bb-8171-4ed0-a232-b561d0942a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample\n",
    "X_train[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa42f36-5ebf-4541-9c2c-ecc7dece6446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the matching target\n",
    "search_y = y_train[train_idx]\n",
    "search_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7175df2-2e75-4fcd-8d66-6adf3f470b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the target value in the raw dataset\n",
    "full_idx = 0\n",
    "for i in range(len(y)):\n",
    "    if y[i] == search_y:\n",
    "        print(i)\n",
    "        full_idx = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9857baf8-466f-4316-8335-e23a69b1c5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare if the raw dataset entry matches the subset entry\n",
    "if X[full_idx].all() == X_train[train_idx].all():\n",
    "    print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa37bd9c-8751-438c-a09a-633a050d5ee7",
   "metadata": {},
   "source": [
    "### Sort Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce734dc-d388-4eaa-89e2-9ff29f0ff59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an inhomogenous numpy array from the training set\n",
    "X_train = np.array(X_train, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e05e3-0b6e-40ec-93f0-ab5bb51b3dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an array containing the sequence lengths\n",
    "sequence_lengths = list(map(lambda x: len(x), X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c1bf6e-6f11-4bb3-b6c7-a113e31f360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the array but only get the indices\n",
    "sorted_indices = np.argsort(sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c9666d-018e-4338-a7cd-c6ac0e3fdabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236ee90-c6bd-4483-9328-09c28bfa3cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now sort the X and y train arrays according to the sorted indicds\n",
    "X_train = X_train[sorted_indices]\n",
    "y_train = y_train[sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ded927-98ca-40e1-b7e1-779ad6c5c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the previously found values still correlate\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == search_y:\n",
    "        print(X_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e51d5a-aaf5-4507-be4c-9d44d06a457d",
   "metadata": {},
   "source": [
    "### Ragged Tensor Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f218654e-0434-4ae2-8472-93003397a641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does not work since the sequences are of different length\n",
    "# X_test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ae758-42e2-4f8b-9d32-7d64f8322c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = tf.ragged.constant(X_train, dtype=tf.int8, ragged_rank=1, row_splits_dtype=tf.int32)\n",
    "X_val_tensor = tf.ragged.constant(X_val, dtype=tf.int8, ragged_rank=1, row_splits_dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8348ae2-0d05-4bf3-b64d-898eb5a9ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dataset = tf.data.Dataset.from_tensor_slices((X_train_tensor, y_train))\n",
    "X_val_dataset = tf.data.Dataset.from_tensor_slices((X_val_tensor, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d3004d-f438-48c2-b079-22fc282122e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf34f643-79a9-469b-9353-7f60950f09f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8502042d-58f5-4538-8578-e403365e1ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded batches from ragged tensors are not supported (yet)\n",
    "# it needs a work around creating a uniform tensor\n",
    "# idea from : https://github.com/tensorflow/tensorflow/issues/39163\n",
    "def reformat(data, label):\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c4a63-47db-4263-a288-5969b7dbb6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dataset = X_train_dataset.map(reformat)\n",
    "X_val_dataset = X_val_dataset.map(reformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70893c0-2060-4ae7-9e2a-0cdbeb97921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataset (again) and create padded batches\n",
    "batch_size = 32\n",
    "X_train_dataset = X_train_dataset.padded_batch(batch_size)\n",
    "X_val_dataset = X_val_dataset.shuffle(buffer_size=len(X_val), seed=1202).padded_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be515ae3-b63a-4282-a614-d550e9d5369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optinally repeat the dataset multiple times -> WHY?\n",
    "# rep = 3\n",
    "# X_train_dataset = X_train_dataset.repeat(rep)\n",
    "# X_val_dataset = X_val_dataset.repeat(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b046a1-22d0-42a9-94c0-7288e217425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalen = []\n",
    "ds_iterator = iter(X_train_dataset)\n",
    "for data, label in ds_iterator:\n",
    "    datalen.append(len(data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbf6d94-f7c8-4613-8834-036bdd4d0190",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalen[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14065709-7726-4aea-9188-83f593189fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing if keras can use the dataset\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=(None,4)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.fit(X_train_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a605e2-7641-40ae-91bb-3aa922bf7643",
   "metadata": {},
   "source": [
    "Layer, Model and Prediction Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7debc6-1868-4b2a-8d44-f414532051bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(None, 4))\n",
    "conv1 = layers.Conv1D(\n",
    "    filters=32,\n",
    "    kernel_size=10,\n",
    "    strides=1,\n",
    "    activation='relu',\n",
    "    padding='valid'\n",
    ")(inputs)\n",
    "norm1 = layers.BatchNormalization()(conv1)\n",
    "drop1 = layers.Dropout(\n",
    "    rate=0.1\n",
    ")(norm1)\n",
    "# pool1 = layers.MaxPooling1D(\n",
    "#     pool_size=4,\n",
    "#     strides=4\n",
    "# )(drop1)\n",
    "pool1 = layers.GlobalMaxPool1D()(drop1)\n",
    "# flat = layers.Flatten()(drop1)\n",
    "dense = layers.Dense(16, activation='relu')(pool1)\n",
    "outputs = layers.Dense(1)(dense)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='Test')\n",
    "model.summary()\n",
    "    \n",
    "model.compile(\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.MeanAbsoluteError()],\n",
    ")\n",
    "    \n",
    "history = model.fit(\n",
    "    X_train_dataset,\n",
    "    epochs=10,\n",
    "    validation_data=X_val_dataset\n",
    ")\n",
    "\n",
    "X_test_padded = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding=\"post\")\n",
    "y_pred = model.predict(X_test_padded)\n",
    "print('Random prediction sample (truth, prediction):', y_test[0], y_pred[0])\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de67ac-0026-42a6-b09b-f1fd60a0d74c",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0555c3-6e36-41eb-bb88-652a14aa79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zrimec_model():\n",
    "    inputs = layers.Input(shape=(None, 4))\n",
    "    # inputs = layers.Input(shape=(7999, 4))\n",
    "\n",
    "    conv1 = layers.Conv1D(\n",
    "        filters=32,\n",
    "        kernel_size=10,\n",
    "        strides=1,\n",
    "        activation='relu',\n",
    "        padding='valid'\n",
    "    )(inputs)\n",
    "    norm1 = layers.BatchNormalization()(conv1)\n",
    "    drop1 = layers.Dropout(\n",
    "        rate=0.1\n",
    "    )(norm1)\n",
    "    pool1 = layers.MaxPooling1D(\n",
    "        pool_size=4,\n",
    "        strides=4\n",
    "    )(drop1)\n",
    "    conv2 = layers.Conv1D(\n",
    "        filters=64,\n",
    "        kernel_size=10,\n",
    "        strides=1,\n",
    "        activation='relu',\n",
    "        padding='valid'\n",
    "    )(pool1)\n",
    "    norm2 = layers.BatchNormalization()(conv2)\n",
    "    drop2 = layers.Dropout(\n",
    "        rate=0.1\n",
    "    )(norm2)\n",
    "    pool2 = layers.MaxPooling1D(\n",
    "        pool_size=4,\n",
    "        strides=4\n",
    "    )(drop2)\n",
    "    conv3 = layers.Conv1D(\n",
    "        filters=128,\n",
    "        kernel_size=10,\n",
    "        strides=1,\n",
    "        activation='relu',\n",
    "        padding='valid'\n",
    "    )(pool2)\n",
    "    norm3 = layers.BatchNormalization()(conv3)\n",
    "    drop3 = layers.Dropout(\n",
    "        rate=0.1\n",
    "    )(norm3)\n",
    "    # pool3 = layers.MaxPooling1D(\n",
    "    #     pool_size=4,\n",
    "    #     strides=4\n",
    "    # )(drop3)\n",
    "    # flat = layers.Flatten()(pool3)\n",
    "    gmp = layers.GlobalMaxPool1D()(drop3)\n",
    "    dense = layers.Dense(\n",
    "        units=64,\n",
    "        activation='relu'\n",
    "    )(gmp)\n",
    "    norm4 = layers.BatchNormalization()(dense)\n",
    "    drop4 = layers.Dropout(\n",
    "        rate=0.1\n",
    "    )(norm4)\n",
    "    outputs = layers.Dense(\n",
    "        units=1\n",
    "    )(drop4)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='ZrimecModel')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb6ecde-954a-47a9-a7ad-ee9bfa13a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def danq_model():\n",
    "    inputs = layers.Input(shape=(None, 4))\n",
    "\n",
    "    conv = layers.Conv1D(\n",
    "        filters=320,\n",
    "        kernel_size=26,\n",
    "        padding='valid',\n",
    "        activation='relu'\n",
    "    )(inputs)\n",
    "    pool = layers.MaxPooling1D(\n",
    "        pool_size=13,\n",
    "        strides=13\n",
    "    )(conv)\n",
    "    drop1 = layers.Dropout(\n",
    "        rate=0.2\n",
    "    )(pool)\n",
    "\n",
    "    forward_layer = layers.LSTM(units=320, return_sequences=True)\n",
    "    backward_layer = layers.LSTM(units=320, return_sequences=True, go_backwards=True)\n",
    "    bilstm = layers.Bidirectional(\n",
    "        forward_layer, backward_layer=backward_layer\n",
    "    )(drop1)\n",
    "    drop2 = layers.Dropout(\n",
    "        rate=0.2\n",
    "    )(bilstm)\n",
    "    # flat = layers.Flatten()(drop2)\n",
    "    gmp = layers.GlobalMaxPool1D()(drop2)\n",
    "    dense1 = layers.Dense(\n",
    "        units=925,\n",
    "        activation='relu'\n",
    "    )(gmp)\n",
    "    outputs = layers.Dense(\n",
    "        units=1\n",
    "    )(dense1)\n",
    "    \n",
    "    # model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='DanQModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cbe2d2-38ec-49cb-8158-cd5a99b1a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def danq_refined_model():\n",
    "    inputs = layers.Input(shape=(None, 4))\n",
    "\n",
    "    conv = layers.Conv1D(\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        padding='valid',\n",
    "        activation='relu'\n",
    "    )(inputs)\n",
    "    pool = layers.MaxPooling1D(\n",
    "        pool_size=4,\n",
    "        strides=4\n",
    "    )(conv)\n",
    "    norm = layers.BatchNormalization()(pool)\n",
    "    # drop1 = layers.Dropout(\n",
    "    #     rate=0.2\n",
    "    # )(pool)\n",
    "\n",
    "    bilstm = layers.Bidirectional(\n",
    "        layers.LSTM(units=64, return_sequences=True, recurrent_dropout=0.25),\n",
    "        # merge_mode='mul'\n",
    "    )(norm)\n",
    "    drop2 = layers.Dropout(\n",
    "        rate=0.5\n",
    "    )(bilstm)\n",
    "    # flat = layers.Flatten()(drop2)\n",
    "    gmp = layers.GlobalMaxPool1D()(drop2)\n",
    "    dense1 = layers.Dense(\n",
    "        units=64,\n",
    "        activation='relu'\n",
    "    )(gmp)\n",
    "    outputs = layers.Dense(\n",
    "        units=1\n",
    "    )(dense1)\n",
    "    \n",
    "    # model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='DanQRefModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4c5168-f720-48b4-b777-01067098205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dan_zrimec_model():\n",
    "    # input\n",
    "    # setting fixed shape since the sequences are padded to the max length (threshold of preproc2)\n",
    "    inputs = layers.Input(shape=(None, 4))\n",
    "    # 1D convolution\n",
    "    conv = layers.Conv1D(\n",
    "        filters=320, \n",
    "        kernel_size=26, \n",
    "        strides=1, \n",
    "        activation='relu'\n",
    "    )(inputs)\n",
    "    # batch normalization\n",
    "    norm = layers.BatchNormalization()(conv)\n",
    "    # maxpool\n",
    "    pool = layers.MaxPooling1D(\n",
    "        pool_size=13,\n",
    "        strides=13\n",
    "    )(norm)\n",
    "    # dropout\n",
    "    drop = layers.Dropout(rate=0.1)(pool)\n",
    "    # bi-directional LSTM\n",
    "    bilstm = layers.Bidirectional(\n",
    "        layers.LSTM(\n",
    "            units=320, \n",
    "            dropout=0,\n",
    "            return_sequences=True,\n",
    "        ),\n",
    "        merge_mode='mul',\n",
    "        # input_shape=(8000, 4),\n",
    "    )(drop)\n",
    "    # maxpool\n",
    "    pool = layers.MaxPooling1D(\n",
    "        pool_size=13,\n",
    "        strides=13,\n",
    "    )(bilstm)\n",
    "    drop = layers.Dropout(rate=0.1)(pool)\n",
    "    # flatten\n",
    "    # flat = layers.Flatten()(drop)\n",
    "    gmp = layers.GlobalMaxPool1D()(drop)\n",
    "    # fully connected\n",
    "    dense = layers.Dense(\n",
    "        units=64,\n",
    "        activation='relu',\n",
    "    )(gmp)\n",
    "    # batch normalization\n",
    "    norm = layers.BatchNormalization()(dense)\n",
    "    # dropout\n",
    "    drop = layers.Dropout(rate=0.1)(norm)\n",
    "    # dense\n",
    "    outputs = layers.Dense(units=1)(drop)\n",
    "\n",
    "    # model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='BaselineModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d468d333-3246-419a-95cd-4eb520fa40ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augur_model():\n",
    "    inputs = layers.Input(shape=(None, 4))\n",
    "    conv = layers.Conv1D(\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        padding='valid',\n",
    "        activation='relu'\n",
    "    )(inputs)\n",
    "    bilstm = layers.Bidirectional(layers.LSTM(units=64, recurrent_dropout=0.25))(conv)\n",
    "    drop1 = layers.Dropout(rate=0.2)(bilstm)\n",
    "    outputs = layers.Dense(units=1)(drop1)\n",
    "    \n",
    "    # model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='AugurModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d46776-4a76-4870-aa3f-565606a6ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(hist, start_epoch=1):\n",
    "    history_dict = hist.history\n",
    "    loss_values = history_dict[\"loss\"][start_epoch-1:]\n",
    "    val_loss_values = history_dict[\"val_loss\"][start_epoch-1:]\n",
    "    epochs = range(start_epoch, len(history_dict[\"loss\"]) + 1)\n",
    "    plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
    "    plt.title(\"Training and validation loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eed410-4510-4756-9c54-b72d2a88b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(model, epochs=5, plot_epoch_start=1):\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        # optimizer=keras.optimizers.Adam(learning_rate=0.1, beta_1=0.999, beta_2=0.99, epsilon=1e-6),\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        metrics=[keras.metrics.MeanAbsolutePercentageError(), keras.metrics.RootMeanSquaredError(), keras.metrics.MeanSquaredError(), keras.metrics.MeanAbsoluteError()],\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        # X_train, \n",
    "        # y_train, \n",
    "        X_train_dataset,\n",
    "        # batch_size=64, \n",
    "        epochs=epochs,\n",
    "        # validation_data=(X_val, y_val), \n",
    "        validation_data=X_val_dataset\n",
    "        # callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    print('Random prediction sample (truth, prediction):', y_test[0], y_pred[0])\n",
    "    \n",
    "    plot_loss(history, plot_epoch_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce9caac-ddde-49e3-8cbf-678939080350",
   "metadata": {},
   "source": [
    "### Preliminary Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362f55a9-a90d-4979-87b8-13399dd499b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test(zrimec_model(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2632714-9505-4512-bebb-372f3d49366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test(danq_model(), 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f55e6-bf26-4172-be44-1159ba096400",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test(danq_refined_model(), 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7873f96-828e-4299-a410-7fb2a3432ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test(dan_zrimec_model(), 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccfbadb-f47e-4c1e-b558-7ab8bfd1f5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test(augur_model(), 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b0a10-c0d7-4b3b-a20c-2a3a1937a807",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63783c6-429b-4b1d-a9fe-880a1bfe9a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def danq_refined_model_hp_search(hp):\n",
    "    inputs = layers.Input(shape=(7999, 4))\n",
    "\n",
    "    conv = layers.Conv1D(\n",
    "        filters=hp.Int('conv_units', min_value=32, max_value=512, step=32),\n",
    "        kernel_size=hp.Int('kernel_size', min_value=3, max_value=36, step=3),\n",
    "        padding='valid',\n",
    "        activation='relu'\n",
    "    )(inputs)\n",
    "    pool = layers.MaxPooling1D(\n",
    "        pool_size=hp.Int('pool_size', min_value=2, max_value=20, step=2),\n",
    "        strides=None\n",
    "    )(conv)\n",
    "    norm = layers.BatchNormalization()(pool)\n",
    "    # drop1 = layers.Dropout(\n",
    "    #     rate=0.2\n",
    "    # )(pool)\n",
    "\n",
    "    bilstm = layers.Bidirectional(\n",
    "        layers.LSTM(\n",
    "            units=hp.Int('lstm_units', min_value=32, max_value=512, step=32), \n",
    "            return_sequences=True, \n",
    "            recurrent_dropout=hp.Float('recurrent_dropout', min_value=0.1, max_value=0.5, step=0.05)\n",
    "        ),\n",
    "        merge_mode=hp.Choice('merge_mode', ['concat', 'sum', 'mul'])\n",
    "    )(norm)\n",
    "    drop2 = layers.Dropout(\n",
    "        rate=hp.Float('rate', min_value=0.1, max_value=0.5, step=0.05)\n",
    "    )(bilstm)\n",
    "    flat = layers.Flatten()(drop2)\n",
    "    dense1 = layers.Dense(\n",
    "        units=hp.Int('dense_units', min_value=32, max_value=128, step=32),\n",
    "        activation='relu'\n",
    "    )(flat)\n",
    "    outputs = layers.Dense(\n",
    "        units=1\n",
    "    )(dense1)\n",
    "    \n",
    "    # model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='DanQRefModel')\n",
    "\n",
    "    model.compile(\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        # optimizer=keras.optimizers.Adam(learning_rate=0.1, beta_1=0.999, beta_2=0.99, epsilon=1e-6),\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        metrics=[keras.metrics.MeanAbsolutePercentageError(), keras.metrics.MeanSquaredError(), keras.metrics.MeanAbsoluteError()],\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f8e838-9aad-4c15-ae11-df1c37f6730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying that the model compiles\n",
    "danq_refined_model_hp_search(keras_tuner.HyperParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7d21df-f31d-40fc-ba0f-071bd51db6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the tuner\n",
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=danq_refined_model_hp_search,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=100,\n",
    "    executions_per_trial=2,\n",
    "    overwrite=True,\n",
    "    directory=\"hp_search\",\n",
    "    project_name=\"PTRaugur\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f73847-82bf-49c7-bf71-d39dcde324c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing an overview of the tunable parameters\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875a280-5676-488c-aad2-36fe9a61e1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up an early stop callback function while tuning\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5),\n",
    "]\n",
    "\n",
    "# running the tuning\n",
    "# tuner.search(\n",
    "#     X_train, y_train,\n",
    "#     batch_size=64, \n",
    "#     epochs=100,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     callbacks=callbacks,\n",
    "#     verbose=2,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9475e359-d923-486a-9867-82166ac93610",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3bcfc1-1a04-4a74-9605-1d1f83757491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# both zrimec and danq have a really bad performance (at least with 5 iterations, danq is slow because it's big)\n",
    "# model = zrimec_model()\n",
    "# model = danq_model()\n",
    "# model = baseline_model()\n",
    "# model = augur_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d846b8-9bdb-453e-b467-1f4f1a518c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b8982-e3d5-4544-9862-349265fce832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.utils.plot_model(model, 'baseline.png')\n",
    "# img = plt.imread('baseline.png')\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.imshow(img)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5172dfc2-f846-4a93-8321-e38a56f857ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(\n",
    "#     loss=keras.losses.MeanSquaredError(),\n",
    "#     optimizer=keras.optimizers.Adam(learning_rate=0.1, beta_1=0.999, beta_2=0.99, epsilon=1e-6),\n",
    "#     metrics=[keras.metrics.MeanAbsolutePercentageError(), keras.metrics.RootMeanSquaredError(), keras.losses.MeanSquaredError(), keras.metrics.MeanAbsoluteError()],\n",
    "# )\n",
    "\n",
    "# LOSS\n",
    "# works keras.losses.MeanSquaredError(),\n",
    "\n",
    "# METRIC\n",
    "# useless keras.metrics.Accuracy(), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be5b09-0c52-4911-a579-c61318999317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [\n",
    "#     keras.callbacks.ModelCheckpoint(\"PTR_baseline.keras\", save_best_only=True)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9c18cf-111f-4045-ab88-851efdab4c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative methode to prepadding the sequences\n",
    "# X_train_ragged = tf.ragged.constant(X_train, dtype=tf.int8, ragged_rank=1, row_splits_dtype=tf.int32)\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(tensor)\n",
    "# dataset = dataset.map(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7843173-f03d-4cdc-9e81-bf033163b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(\n",
    "#     X_train, \n",
    "#     y_train, \n",
    "#     batch_size=64, \n",
    "#     epochs=10, \n",
    "#     validation_data=(X_val, y_val), \n",
    "#     callbacks=callbacks\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5389b58-c88c-491a-bd0b-b1ae96f74559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba8bb01-bbe1-4c9e-b7a7-be82d3a61b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_test[0], y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6872ae84-78d2-4c18-b170-6561f595aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_dict = history.history\n",
    "# history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c82c070-4f3c-47c9-8901-62c10e05b2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b88240-e1e5-4f66-9de6-e549a3ae79a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def full_model():\n",
    "#     # input\n",
    "#     inputs = keras.Input(shape=(None, 10))\n",
    "#     # 1D convolution\n",
    "#     conv = keras.Conv1D(\n",
    "#         filters=128, \n",
    "#         kernel_size=10, \n",
    "#         strides=1, \n",
    "#         activation='relu'\n",
    "#     )(inputs)\n",
    "#     # batch normalization\n",
    "#     norm = keras.BatchNormalization()(conv)\n",
    "#     # maxpool\n",
    "#     pool = keras.MaxPooling1D(\n",
    "#         pool_size=4,\n",
    "#         strides=4\n",
    "#     )(norm)\n",
    "#     # dropout\n",
    "#     drop = keras.Dropout(rate=0.2)(pool)\n",
    "#     # bi-directional LSTM\n",
    "#     bilstm = keras.Bidirectional(\n",
    "#         keras.LSTM(\n",
    "#             units=128,\n",
    "#             dropout=0\n",
    "#         ),\n",
    "#         merge_mode='concat'\n",
    "#     )(drop)\n",
    "#     # batch normalization\n",
    "#     norm = keras.BatchNormalization()(bilstm)\n",
    "#     # maxpool\n",
    "#     pool = keras.MaxPooling1D(\n",
    "#         pool_size=4,\n",
    "#         strides=4\n",
    "#     )(norm)\n",
    "#     # dropout\n",
    "#     drop = keras.Dropout(rate=0.2)(pool)\n",
    "#     # flatten\n",
    "#     flat = keras.Flatten()(drop)\n",
    "\n",
    "#     # second input\n",
    "#     inputs2 = keras.Input(shape=(29,))\n",
    "\n",
    "#     # concatenation\n",
    "#     conc = keras.Concatenate(axis=1)([inputs, inputs2])\n",
    "\n",
    "#     # fully connected\n",
    "#     dense = keras.Dense(\n",
    "#         units=64,\n",
    "#         activation='relu'\n",
    "#     )(conc)\n",
    "#     # batch normalization\n",
    "#     norm = keras.BatchNormalization()(dense)\n",
    "#     # dropout\n",
    "#     drop = keras.Dropout(rate=0.2)(norm)\n",
    "#     # dense\n",
    "#     outputs = keras.Dense(units=1)(drop)\n",
    "\n",
    "#     # model\n",
    "#     model = keras.Model(inputs=[inputs, inputs2], outputs=outputs, name='full_model')\n",
    "    \n",
    "#     return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
