{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e465583-fc98-4fda-937a-475748037b12",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning Augur Model\n",
    "\n",
    "*on ds6*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f12e085-6ce6-41e0-8f1e-1d35b23bf358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 01:39:21.171764: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-19 01:39:21.197361: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-19 01:39:21.197385: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-19 01:39:21.198211: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-19 01:39:21.202625: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-19 01:39:21.203036: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-19 01:39:21.822396: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# library dependencies\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import lzma\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import keras_tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff0f512-caac-4cb6-b469-b2bbd23ad3f9",
   "metadata": {},
   "source": [
    "The following methods read the prepared data files from the pre processing step and return the lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9ecd0a7-8f25-4b5b-bfb0-b5375b32c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to store data as serialized binary structure lzma compressed\n",
    "def can_pickles(data, filename):\n",
    "    with lzma.LZMAFile(filename, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.DEFAULT_PROTOCOL)\n",
    "\n",
    "# method to retrieve data from a compressed pickle file (created with the method above)\n",
    "def uncan_pickles(filename):\n",
    "    with lzma.LZMAFile(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c330afba-bf62-4e92-ad84-62921e42d31f",
   "metadata": {},
   "source": [
    "## Basic Neural Network\n",
    "\n",
    "*explain it in more detail*\n",
    "\n",
    "X holds a list of sequences one hot encoded\n",
    "\n",
    "y holds a list of PTR values as floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3d1d16d-426c-460f-930d-0e91adba9e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the prepared data back\n",
    "X = uncan_pickles('../data/onehot_x_lung.pickle.xz')\n",
    "y = uncan_pickles('../data/onehot_y_lung.pickle.xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b2e91af-1782-4909-8191-87442cac791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad add sequence entries to the same length\n",
    "# done here for simplicity to find a good neural network\n",
    "# X_padded = tf.keras.preprocessing.sequence.pad_sequences(X, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3603897-eca1-41be-a833-79e59f4b5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert type of target values from string to float\n",
    "y = np.array(y).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85086f0d-2d79-4ee8-900d-cad1873c1b64",
   "metadata": {},
   "source": [
    "Random sample from X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41525e09-813f-46b8-9f47-ebbd7381510c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85471826-2b3a-4371-8441-ae940ba8e580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.544"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "694f3792-6d06-48e0-81af-f689cbb35c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8201"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of input sequences\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab69104-daa6-4989-89aa-2f6a71f34b0b",
   "metadata": {},
   "source": [
    "### Baseline PTR\n",
    "\n",
    "There is no common sense approach in finding a baseline for the protein-to-mRNA ratio of a particular mRNA sequence. This is what the *Basic Neural Network* approach is for - to determin a baseline and see if a slightly adapted neural network with feature engineered input can provide better predictions.\n",
    "\n",
    "But what can be done is to simply check the value range of the target PTRs, calculate mean and standard deviation. Given that the standard deviation is  small (12.5% of the value range) one can (stupidly) predict the mean value every time. From that it's possible to calculate the Mean Absolute Error (MAE) and compare that to the following neural network output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bfa09b2-b288-4618-88a6-7799e1335da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.552 8.587 4.973957444214121 0.8835629329175175\n"
     ]
    }
   ],
   "source": [
    "# get some idea of the range of the PTR in the selected SAMPLE\n",
    "print(np.min(y), np.max(y), np.mean(y), np.std(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac9ba3da-9ab0-43f5-a923-63e94c580a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7055145713937325"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple/dumb baseline mean absolute error of always predicting 4.974\n",
    "mae = np.mean(np.abs(np.array(y) - 4.974))\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94586678-d302-4884-895d-d166c7e6f933",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Split data in train and test subsets and then split the train subset again in train and validation.\n",
    "\n",
    "A simple verification if the X and y correlation are preserved on the split is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "271ef8f9-7f36-4c73-a448-5abbf294bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train and test sub sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41e1f188-4807-406d-b498-57b5589e6187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad test input (variable input is not accepted)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90b64b18-bbd8-4d4e-b6db-30d063c7eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the train set again in train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39872e7f-c57d-4122-b0ea-3dbb7b24763d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the first unique PTR value that is also in y_train\n",
    "train_idx = 0\n",
    "for i in range(len(y)):\n",
    "    count = 0\n",
    "    for l in range(len(y)):\n",
    "        if i != l and y[i] == y[l]:\n",
    "            count += 1\n",
    "            continue\n",
    "    if count == 0:\n",
    "        for m in range(len(y_train)):\n",
    "            if y[i] == y_train[m]:\n",
    "                train_idx = m\n",
    "                break\n",
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ad195bb-8171-4ed0-a232-b561d0942a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a sample\n",
    "X_train[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fa42f36-5ebf-4541-9c2c-ecc7dece6446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.377"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the matching target\n",
    "search_y = y_train[train_idx]\n",
    "search_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7175df2-2e75-4fcd-8d66-6adf3f470b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8194\n"
     ]
    }
   ],
   "source": [
    "# find the target value in the raw dataset\n",
    "full_idx = 0\n",
    "for i in range(len(y)):\n",
    "    if y[i] == search_y:\n",
    "        print(i)\n",
    "        full_idx = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9857baf8-466f-4316-8335-e23a69b1c5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# compare if the raw dataset entry matches the subset entry\n",
    "if X[full_idx].all() == X_train[train_idx].all():\n",
    "    print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa37bd9c-8751-438c-a09a-633a050d5ee7",
   "metadata": {},
   "source": [
    "### Sort Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ce734dc-d388-4eaa-89e2-9ff29f0ff59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an inhomogenous numpy array from the training set\n",
    "X_train = np.array(X_train, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "202e05e3-0b6e-40ec-93f0-ab5bb51b3dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an array containing the sequence lengths\n",
    "sequence_lengths = list(map(lambda x: len(x), X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38c1bf6e-6f11-4bb3-b6c7-a113e31f360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the array but only get the indices\n",
    "sorted_indices = np.argsort(sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72c9666d-018e-4338-a7cd-c6ac0e3fdabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1140,  657, 1659, ...,   71, 4128, 5096])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5236ee90-c6bd-4483-9328-09c28bfa3cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now sort the X and y train arrays according to the sorted indicds\n",
    "X_train = X_train[sorted_indices]\n",
    "y_train = y_train[sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5ded927-98ca-40e1-b7e1-779ad6c5c25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " ...\n",
      " [0 0 1 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# check if the previously found values still correlate\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == search_y:\n",
    "        print(X_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e51d5a-aaf5-4507-be4c-9d44d06a457d",
   "metadata": {},
   "source": [
    "### Ragged Tensor Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f218654e-0434-4ae2-8472-93003397a641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does not work since the sequences are of different length\n",
    "# X_test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "169ae758-42e2-4f8b-9d32-7d64f8322c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 11s, sys: 1min 20s, total: 10min 32s\n",
      "Wall time: 10min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_tensor = tf.ragged.constant(X_train, dtype=tf.int8, ragged_rank=1, row_splits_dtype=tf.int32)\n",
    "X_val_tensor = tf.ragged.constant(X_val, dtype=tf.int8, ragged_rank=1, row_splits_dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8348ae2-0d05-4bf3-b64d-898eb5a9ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dataset = tf.data.Dataset.from_tensor_slices((X_train_tensor, y_train))\n",
    "X_val_dataset = tf.data.Dataset.from_tensor_slices((X_val_tensor, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15d3004d-f438-48c2-b079-22fc282122e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RaggedTensorSpec(TensorShape([None, 4]), tf.int8, 0, tf.int32),\n",
       " TensorSpec(shape=(), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf34f643-79a9-469b-9353-7f60950f09f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RaggedTensorSpec(TensorShape([None, 4]), tf.int8, 0, tf.int32),\n",
       " TensorSpec(shape=(), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8502042d-58f5-4538-8578-e403365e1ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded batches from ragged tensors are not supported (yet)\n",
    "# it needs a work around creating a uniform tensor\n",
    "# idea from : https://github.com/tensorflow/tensorflow/issues/39163\n",
    "def reformat(data, label):\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc3c4a63-47db-4263-a288-5969b7dbb6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dataset = X_train_dataset.map(reformat)\n",
    "X_val_dataset = X_val_dataset.map(reformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c70893c0-2060-4ae7-9e2a-0cdbeb97921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataset (again) and create padded batches\n",
    "batch_size = 64\n",
    "X_train_dataset = X_train_dataset.padded_batch(batch_size)\n",
    "X_val_dataset = X_val_dataset.shuffle(buffer_size=len(X_val), seed=1202).padded_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be515ae3-b63a-4282-a614-d550e9d5369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optinally repeat the dataset multiple times -> WHY?\n",
    "# rep = 3\n",
    "# X_train_dataset = X_train_dataset.repeat(rep)\n",
    "# X_val_dataset = X_val_dataset.repeat(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00b046a1-22d0-42a9-94c0-7288e217425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalen = []\n",
    "ds_iterator = iter(X_train_dataset)\n",
    "for data, label in ds_iterator:\n",
    "    datalen.append(len(data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fbf6d94-f7c8-4613-8834-036bdd4d0190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[579, 679, 758, 857, 920]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalen[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14065709-7726-4aea-9188-83f593189fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 3s 33ms/step - loss: 20.7713 - mae: 4.4017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f02c4c43810>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing if keras can use the dataset\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=(None,4)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.fit(X_train_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a605e2-7641-40ae-91bb-3aa922bf7643",
   "metadata": {},
   "source": [
    "## Layer, Model and Prediction Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7debc6-1868-4b2a-8d44-f414532051bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(None, 4))\n",
    "conv1 = layers.Conv1D(\n",
    "    filters=32,\n",
    "    kernel_size=10,\n",
    "    strides=1,\n",
    "    activation='relu',\n",
    "    padding='valid'\n",
    ")(inputs)\n",
    "norm1 = layers.BatchNormalization()(conv1)\n",
    "drop1 = layers.Dropout(\n",
    "    rate=0.1\n",
    ")(norm1)\n",
    "# pool1 = layers.MaxPooling1D(\n",
    "#     pool_size=4,\n",
    "#     strides=4\n",
    "# )(drop1)\n",
    "pool1 = layers.GlobalMaxPool1D()(drop1)\n",
    "# flat = layers.Flatten()(drop1)\n",
    "dense = layers.Dense(16, activation='relu')(pool1)\n",
    "outputs = layers.Dense(1)(dense)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='Test')\n",
    "model.summary()\n",
    "    \n",
    "model.compile(\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.MeanAbsoluteError()],\n",
    ")\n",
    "    \n",
    "history = model.fit(\n",
    "    X_train_dataset,\n",
    "    epochs=20,\n",
    "    validation_data=X_val_dataset\n",
    ")\n",
    "\n",
    "print(model.evaluate(X_test, y_test))\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8820b0-414b-4c03-a6dd-acbc476fe3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(None, 4))\n",
    "conv1 = layers.Conv1D(\n",
    "    filters=32,\n",
    "    kernel_size=10,\n",
    "    strides=1,\n",
    "    activation='relu',\n",
    "    padding='valid'\n",
    ")(inputs)\n",
    "# norm1 = layers.BatchNormalization()(conv1)\n",
    "pool1 = layers.MaxPooling1D(\n",
    "    pool_size=4,\n",
    "    strides=4\n",
    ")(conv1)\n",
    "bilstm = layers.Bidirectional(\n",
    "    layers.LSTM(units=32, return_sequences=True, recurrent_dropout=0.5),\n",
    "    merge_mode='mul'\n",
    ")(pool1)\n",
    "drop1 = layers.Dropout(\n",
    "    rate=0.5\n",
    ")(bilstm)\n",
    "# pool1 = layers.MaxPooling1D(\n",
    "#     pool_size=4,\n",
    "#     strides=4\n",
    "# )(drop1)\n",
    "pool1 = layers.GlobalMaxPool1D()(drop1)\n",
    "# flat = layers.Flatten()(drop1)\n",
    "dense = layers.Dense(16, activation='relu')(pool1)\n",
    "outputs = layers.Dense(1)(dense)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='Test')\n",
    "model.summary()\n",
    "    \n",
    "model.compile(\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.MeanAbsoluteError()],\n",
    ")\n",
    "    \n",
    "history = model.fit(\n",
    "    X_train_dataset,\n",
    "    epochs=20,\n",
    "    validation_data=X_val_dataset\n",
    ")\n",
    "\n",
    "print(model.evaluate(X_test, y_test))\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de67ac-0026-42a6-b09b-f1fd60a0d74c",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0555c3-6e36-41eb-bb88-652a14aa79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zrimec_model():\n",
    "    inputs = layers.Input(shape=(None, 4))\n",
    "    # inputs = layers.Input(shape=(7999, 4))\n",
    "\n",
    "    conv1 = layers.Conv1D(\n",
    "        filters=32,\n",
    "        kernel_size=10,\n",
    "        strides=1,\n",
    "        activation='relu',\n",
    "        padding='valid'\n",
    "    )(inputs)\n",
    "    norm1 = layers.BatchNormalization()(conv1)\n",
    "    drop1 = layers.Dropout(\n",
    "        rate=0.1\n",
    "    )(norm1)\n",
    "    pool1 = layers.MaxPooling1D(\n",
    "        pool_size=4,\n",
    "        strides=4\n",
    "    )(drop1)\n",
    "    conv2 = layers.Conv1D(\n",
    "        filters=64,\n",
    "        kernel_size=10,\n",
    "        strides=1,\n",
    "        activation='relu',\n",
    "        padding='valid'\n",
    "    )(pool1)\n",
    "    norm2 = layers.BatchNormalization()(conv2)\n",
    "    drop2 = layers.Dropout(\n",
    "        rate=0.1\n",
    "    )(norm2)\n",
    "    pool2 = layers.MaxPooling1D(\n",
    "        pool_size=4,\n",
    "        strides=4\n",
    "    )(drop2)\n",
    "    conv3 = layers.Conv1D(\n",
    "        filters=128,\n",
    "        kernel_size=10,\n",
    "        strides=1,\n",
    "        activation='relu',\n",
    "        padding='valid'\n",
    "    )(pool2)\n",
    "    norm3 = layers.BatchNormalization()(conv3)\n",
    "    drop3 = layers.Dropout(\n",
    "        rate=0.1\n",
    "    )(norm3)\n",
    "    # pool3 = layers.MaxPooling1D(\n",
    "    #     pool_size=4,\n",
    "    #     strides=4\n",
    "    # )(drop3)\n",
    "    # flat = layers.Flatten()(pool3)\n",
    "    gmp = layers.GlobalMaxPool1D()(drop3)\n",
    "    dense = layers.Dense(\n",
    "        units=64,\n",
    "        activation='relu'\n",
    "    )(gmp)\n",
    "    norm4 = layers.BatchNormalization()(dense)\n",
    "    drop4 = layers.Dropout(\n",
    "        rate=0.1\n",
    "    )(norm4)\n",
    "    outputs = layers.Dense(\n",
    "        units=1\n",
    "    )(drop4)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='ZrimecModel')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb6ecde-954a-47a9-a7ad-ee9bfa13a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def danq_model():\n",
    "    inputs = layers.Input(shape=(None, 4))\n",
    "\n",
    "    conv = layers.Conv1D(\n",
    "        filters=320,\n",
    "        kernel_size=26,\n",
    "        padding='valid',\n",
    "        activation='relu'\n",
    "    )(inputs)\n",
    "    pool = layers.MaxPooling1D(\n",
    "        pool_size=13,\n",
    "        strides=13\n",
    "    )(conv)\n",
    "    drop1 = layers.Dropout(\n",
    "        rate=0.2\n",
    "    )(pool)\n",
    "\n",
    "    forward_layer = layers.LSTM(units=320, return_sequences=True)\n",
    "    backward_layer = layers.LSTM(units=320, return_sequences=True, go_backwards=True)\n",
    "    bilstm = layers.Bidirectional(\n",
    "        forward_layer, backward_layer=backward_layer\n",
    "    )(drop1)\n",
    "    drop2 = layers.Dropout(\n",
    "        rate=0.2\n",
    "    )(bilstm)\n",
    "    # flat = layers.Flatten()(drop2)\n",
    "    gmp = layers.GlobalMaxPool1D()(drop2)\n",
    "    dense1 = layers.Dense(\n",
    "        units=925,\n",
    "        activation='relu'\n",
    "    )(gmp)\n",
    "    outputs = layers.Dense(\n",
    "        units=1\n",
    "    )(dense1)\n",
    "    \n",
    "    # model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='DanQModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cbe2d2-38ec-49cb-8158-cd5a99b1a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def danq_refined_model():\n",
    "    inputs = layers.Input(shape=(None, 4))\n",
    "\n",
    "    conv = layers.Conv1D(\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        padding='valid',\n",
    "        activation='relu'\n",
    "    )(inputs)\n",
    "    pool = layers.MaxPooling1D(\n",
    "        pool_size=4,\n",
    "        strides=4\n",
    "    )(conv)\n",
    "    norm = layers.BatchNormalization()(pool)\n",
    "    # drop1 = layers.Dropout(\n",
    "    #     rate=0.2\n",
    "    # )(pool)\n",
    "\n",
    "    bilstm = layers.Bidirectional(\n",
    "        layers.LSTM(units=64, return_sequences=True, recurrent_dropout=0.25),\n",
    "        # merge_mode='mul'\n",
    "    )(norm)\n",
    "    drop2 = layers.Dropout(\n",
    "        rate=0.5\n",
    "    )(bilstm)\n",
    "    # flat = layers.Flatten()(drop2)\n",
    "    gmp = layers.GlobalMaxPool1D()(drop2)\n",
    "    dense1 = layers.Dense(\n",
    "        units=64,\n",
    "        activation='relu'\n",
    "    )(gmp)\n",
    "    outputs = layers.Dense(\n",
    "        units=1\n",
    "    )(dense1)\n",
    "    \n",
    "    # model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='DanQRefModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4c5168-f720-48b4-b777-01067098205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dan_zrimec_model():\n",
    "    # input\n",
    "    # setting fixed shape since the sequences are padded to the max length (threshold of preproc2)\n",
    "    inputs = layers.Input(shape=(None, 4))\n",
    "    # 1D convolution\n",
    "    conv = layers.Conv1D(\n",
    "        filters=320, \n",
    "        kernel_size=26, \n",
    "        strides=1, \n",
    "        activation='relu'\n",
    "    )(inputs)\n",
    "    # batch normalization\n",
    "    norm = layers.BatchNormalization()(conv)\n",
    "    # maxpool\n",
    "    pool = layers.MaxPooling1D(\n",
    "        pool_size=13,\n",
    "        strides=13\n",
    "    )(norm)\n",
    "    # dropout\n",
    "    drop = layers.Dropout(rate=0.1)(pool)\n",
    "    # bi-directional LSTM\n",
    "    bilstm = layers.Bidirectional(\n",
    "        layers.LSTM(\n",
    "            units=320, \n",
    "            dropout=0,\n",
    "            return_sequences=True,\n",
    "        ),\n",
    "        merge_mode='mul',\n",
    "        # input_shape=(8000, 4),\n",
    "    )(drop)\n",
    "    # maxpool\n",
    "    pool = layers.MaxPooling1D(\n",
    "        pool_size=13,\n",
    "        strides=13,\n",
    "    )(bilstm)\n",
    "    drop = layers.Dropout(rate=0.1)(pool)\n",
    "    # flatten\n",
    "    # flat = layers.Flatten()(drop)\n",
    "    gmp = layers.GlobalMaxPool1D()(drop)\n",
    "    # fully connected\n",
    "    dense = layers.Dense(\n",
    "        units=64,\n",
    "        activation='relu',\n",
    "    )(gmp)\n",
    "    # batch normalization\n",
    "    norm = layers.BatchNormalization()(dense)\n",
    "    # dropout\n",
    "    drop = layers.Dropout(rate=0.1)(norm)\n",
    "    # dense\n",
    "    outputs = layers.Dense(units=1)(drop)\n",
    "\n",
    "    # model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='BaselineModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d468d333-3246-419a-95cd-4eb520fa40ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augur_model():\n",
    "    inputs = layers.Input(shape=(None, 4))\n",
    "    conv = layers.Conv1D(\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        padding='valid',\n",
    "        activation='relu'\n",
    "    )(inputs)\n",
    "    bilstm = layers.Bidirectional(layers.LSTM(units=64, recurrent_dropout=0.25))(conv)\n",
    "    drop1 = layers.Dropout(rate=0.2)(bilstm)\n",
    "    outputs = layers.Dense(units=1)(drop1)\n",
    "    \n",
    "    # model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='AugurModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d46776-4a76-4870-aa3f-565606a6ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(hist, start_epoch=1):\n",
    "    history_dict = hist.history\n",
    "    loss_values = history_dict[\"loss\"][start_epoch-1:]\n",
    "    val_loss_values = history_dict[\"val_loss\"][start_epoch-1:]\n",
    "    epochs = range(start_epoch, len(history_dict[\"loss\"]) + 1)\n",
    "    plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
    "    plt.title(\"Training and validation loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eed410-4510-4756-9c54-b72d2a88b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(model, epochs=5, plot_epoch_start=1):\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        # optimizer=keras.optimizers.Adam(learning_rate=0.1, beta_1=0.999, beta_2=0.99, epsilon=1e-6),\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        # metrics=[keras.metrics.MeanAbsolutePercentageError(), keras.metrics.RootMeanSquaredError(), keras.metrics.MeanSquaredError(), keras.metrics.MeanAbsoluteError()],\n",
    "        metrics=keras.metrics.MeanAbsoluteError()\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        # X_train, \n",
    "        # y_train, \n",
    "        X_train_dataset,\n",
    "        # batch_size=64, \n",
    "        epochs=epochs,\n",
    "        # validation_data=(X_val, y_val), \n",
    "        validation_data=X_val_dataset\n",
    "        # callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    print('Random prediction sample (truth, prediction):', y_test[0], y_pred[0])\n",
    "    \n",
    "    plot_loss(history, plot_epoch_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce9caac-ddde-49e3-8cbf-678939080350",
   "metadata": {},
   "source": [
    "### Preliminary Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362f55a9-a90d-4979-87b8-13399dd499b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test(zrimec_model(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2632714-9505-4512-bebb-372f3d49366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test(danq_model(), 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f55e6-bf26-4172-be44-1159ba096400",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test(danq_refined_model(), 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7873f96-828e-4299-a410-7fb2a3432ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test(dan_zrimec_model(), 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccfbadb-f47e-4c1e-b558-7ab8bfd1f5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test(augur_model(), 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b0a10-c0d7-4b3b-a20c-2a3a1937a807",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63783c6-429b-4b1d-a9fe-880a1bfe9a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def danq_refined_model_hp_search(hp):\n",
    "    inputs = layers.Input(shape=(7999, 4))\n",
    "\n",
    "    conv = layers.Conv1D(\n",
    "        filters=hp.Int('conv_units', min_value=32, max_value=512, step=32),\n",
    "        kernel_size=hp.Int('kernel_size', min_value=3, max_value=36, step=3),\n",
    "        padding='valid',\n",
    "        activation='relu'\n",
    "    )(inputs)\n",
    "    pool = layers.MaxPooling1D(\n",
    "        pool_size=hp.Int('pool_size', min_value=2, max_value=20, step=2),\n",
    "        strides=None\n",
    "    )(conv)\n",
    "    norm = layers.BatchNormalization()(pool)\n",
    "    # drop1 = layers.Dropout(\n",
    "    #     rate=0.2\n",
    "    # )(pool)\n",
    "\n",
    "    bilstm = layers.Bidirectional(\n",
    "        layers.LSTM(\n",
    "            units=hp.Int('lstm_units', min_value=32, max_value=512, step=32), \n",
    "            return_sequences=True, \n",
    "            recurrent_dropout=hp.Float('recurrent_dropout', min_value=0.1, max_value=0.5, step=0.05)\n",
    "        ),\n",
    "        merge_mode=hp.Choice('merge_mode', ['concat', 'sum', 'mul'])\n",
    "    )(norm)\n",
    "    drop2 = layers.Dropout(\n",
    "        rate=hp.Float('rate', min_value=0.1, max_value=0.5, step=0.05)\n",
    "    )(bilstm)\n",
    "    flat = layers.Flatten()(drop2)\n",
    "    dense1 = layers.Dense(\n",
    "        units=hp.Int('dense_units', min_value=32, max_value=128, step=32),\n",
    "        activation='relu'\n",
    "    )(flat)\n",
    "    outputs = layers.Dense(\n",
    "        units=1\n",
    "    )(dense1)\n",
    "    \n",
    "    # model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='DanQRefModel')\n",
    "\n",
    "    model.compile(\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        # optimizer=keras.optimizers.Adam(learning_rate=0.1, beta_1=0.999, beta_2=0.99, epsilon=1e-6),\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        metrics=[keras.metrics.MeanAbsolutePercentageError(), keras.metrics.MeanSquaredError(), keras.metrics.MeanAbsoluteError()],\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf4a2214-e752-4caa-9402-439916856b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augur2_model_hp_search(hp):\n",
    "    inputs = layers.Input(shape=(None, 4))\n",
    "    conv1 = layers.Conv1D(\n",
    "        filters=hp.Int('conv_units', min_value=16, max_value=512, step=8),\n",
    "        kernel_size=hp.Int('kernel_size', min_value=3, max_value=36, step=3),\n",
    "        # strides=1,\n",
    "        activation='relu',\n",
    "        padding='valid'\n",
    "    )(inputs)\n",
    "    norm1 = layers.BatchNormalization()(conv1)\n",
    "    drop1 = layers.Dropout(\n",
    "        rate=hp.Float('rate', min_value=0.1, max_value=0.8, step=0.05)\n",
    "    )(norm1)\n",
    "    # pool1 = layers.MaxPooling1D(\n",
    "    #     pool_size=4,\n",
    "    #     strides=4\n",
    "    # )(drop1)\n",
    "    pool1 = layers.GlobalMaxPool1D()(drop1)\n",
    "    # flat = layers.Flatten()(drop1)\n",
    "    dense = layers.Dense(\n",
    "        units=hp.Int('dense_units', min_value=16, max_value=128, step=8),\n",
    "        activation='relu'\n",
    "    )(pool1)\n",
    "    outputs = layers.Dense(1)(dense)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='Test')\n",
    "    model.summary()\n",
    "        \n",
    "    model.compile(\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        metrics=[keras.metrics.MeanAbsoluteError()],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98404347-6533-4c16-8cf1-4503316f9b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Test\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, 4)]         0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 16)          208       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, None, 16)          64        \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 16)          0         \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 16)                0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 561 (2.19 KB)\n",
      "Trainable params: 529 (2.07 KB)\n",
      "Non-trainable params: 32 (128.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.functional.Functional at 0x7f025d585bd0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying that the model compiles\n",
    "augur2_model_hp_search(keras_tuner.HyperParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a7d21df-f31d-40fc-ba0f-071bd51db6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Test\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None, 4)]         0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, None, 16)          208       \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, None, 16)          64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, None, 16)          0         \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Gl  (None, 16)                0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 561 (2.19 KB)\n",
      "Trainable params: 529 (2.07 KB)\n",
      "Non-trainable params: 32 (128.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# setting up the tuner\n",
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=augur2_model_hp_search,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=100,\n",
    "    executions_per_trial=2,\n",
    "    overwrite=False,\n",
    "    directory=\"hp_search_augur2\",\n",
    "    project_name=\"PTRaugur\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86f73847-82bf-49c7-bf71-d39dcde324c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 4\n",
      "conv_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 512, 'step': 8, 'sampling': 'linear'}\n",
      "kernel_size (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 3, 'max_value': 36, 'step': 3, 'sampling': 'linear'}\n",
      "rate (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.8, 'step': 0.05, 'sampling': 'linear'}\n",
      "dense_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 128, 'step': 8, 'sampling': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# printing an overview of the tunable parameters\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1875a280-5676-488c-aad2-36fe9a61e1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 100 Complete [01h 01m 15s]\n",
      "val_loss: 0.8649368286132812\n",
      "\n",
      "Best val_loss So Far: 0.7884935140609741\n",
      "Total elapsed time: 2d 04h 39m 04s\n"
     ]
    }
   ],
   "source": [
    "# setting up an early stop callback function while tuning\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5),\n",
    "]\n",
    "\n",
    "# running the tuning\n",
    "tuner.search(\n",
    "    X_train_dataset,\n",
    "    batch_size=64, \n",
    "    epochs=100,\n",
    "    validation_data=X_val_dataset,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b26bfd2-4c2e-4540-a302-a3d9e3a98715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in hp_search_augur2/PTRaugur\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 033 summary\n",
      "Hyperparameters:\n",
      "conv_units: 424\n",
      "kernel_size: 30\n",
      "rate: 0.30000000000000004\n",
      "dense_units: 128\n",
      "Score: 0.7884935140609741\n",
      "\n",
      "Trial 079 summary\n",
      "Hyperparameters:\n",
      "conv_units: 512\n",
      "kernel_size: 36\n",
      "rate: 0.1\n",
      "dense_units: 40\n",
      "Score: 0.8308806419372559\n",
      "\n",
      "Trial 087 summary\n",
      "Hyperparameters:\n",
      "conv_units: 256\n",
      "kernel_size: 33\n",
      "rate: 0.1\n",
      "dense_units: 128\n",
      "Score: 0.8457741141319275\n",
      "\n",
      "Trial 013 summary\n",
      "Hyperparameters:\n",
      "conv_units: 328\n",
      "kernel_size: 27\n",
      "rate: 0.1\n",
      "dense_units: 128\n",
      "Score: 0.8521419167518616\n",
      "\n",
      "Trial 097 summary\n",
      "Hyperparameters:\n",
      "conv_units: 352\n",
      "kernel_size: 36\n",
      "rate: 0.1\n",
      "dense_units: 40\n",
      "Score: 0.8564426302909851\n",
      "\n",
      "Trial 084 summary\n",
      "Hyperparameters:\n",
      "conv_units: 512\n",
      "kernel_size: 36\n",
      "rate: 0.1\n",
      "dense_units: 16\n",
      "Score: 0.8612199425697327\n",
      "\n",
      "Trial 044 summary\n",
      "Hyperparameters:\n",
      "conv_units: 360\n",
      "kernel_size: 30\n",
      "rate: 0.25\n",
      "dense_units: 56\n",
      "Score: 0.8615911304950714\n",
      "\n",
      "Trial 047 summary\n",
      "Hyperparameters:\n",
      "conv_units: 360\n",
      "kernel_size: 36\n",
      "rate: 0.1\n",
      "dense_units: 128\n",
      "Score: 0.863598108291626\n",
      "\n",
      "Trial 064 summary\n",
      "Hyperparameters:\n",
      "conv_units: 408\n",
      "kernel_size: 36\n",
      "rate: 0.1\n",
      "dense_units: 64\n",
      "Score: 0.8641876578330994\n",
      "\n",
      "Trial 099 summary\n",
      "Hyperparameters:\n",
      "conv_units: 512\n",
      "kernel_size: 36\n",
      "rate: 0.1\n",
      "dense_units: 112\n",
      "Score: 0.8649368286132812\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc822cb3-be2a-477c-afd5-e71571ae3ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Test\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, 4)]         0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 424)         51304     \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, None, 424)         1696      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 424)         0         \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 424)               0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               54400     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107529 (420.04 KB)\n",
      "Trainable params: 106681 (416.72 KB)\n",
      "Non-trainable params: 848 (3.31 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58ad5006-d4f9-4ee3-ad25-6711355ff22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Test\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, 4)]         0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 424)         51304     \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, None, 424)         1696      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 424)         0         \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 424)               0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               54400     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107529 (420.04 KB)\n",
      "Trainable params: 106681 (416.72 KB)\n",
      "Non-trainable params: 848 (3.31 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f8567fb-1ff8-4a9f-9956-3d3b3e21867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63687084-5d73-407e-99ef-cf507b4ab7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras_tuner.src.engine.hyperparameters.hyperparameters.HyperParameters at 0x7f00fc54a0d0>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
