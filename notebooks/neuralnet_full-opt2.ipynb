{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ce73b25-d36d-413d-a323-67e9a246b21b",
   "metadata": {},
   "source": [
    "# Full Neural Network Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2874d9ad-1cd5-4f33-bcf6-178552b4ecc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-17 23:16:05.995914: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-17 23:16:06.034120: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-17 23:16:06.034161: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-17 23:16:06.035034: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-17 23:16:06.040730: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-17 23:16:06.041260: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-17 23:16:06.811590: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# library dependencies\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import lzma\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import keras_tuner\n",
    "import math\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b634de2-ee59-4979-ae29-9ce9ab8a2909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to store data as serialized binary structure lzma compressed\n",
    "def can_pickles(data, filename):\n",
    "    with lzma.LZMAFile(filename, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.DEFAULT_PROTOCOL)\n",
    "\n",
    "# method to retrieve data from a compressed pickle file (created with the method above)\n",
    "def uncan_pickles(filename):\n",
    "    with lzma.LZMAFile(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29c2a628-8756-4b25-844b-10352d9ce814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method to create a valid dataset\n",
    "# padded batches from ragged tensors are not supported (yet)\n",
    "# it needs a work around creating a uniform tensor\n",
    "# idea from : https://github.com/tensorflow/tensorflow/issues/39163\n",
    "def reformat(data, label):\n",
    "    return data, label\n",
    "\n",
    "# method to create a TF dataset\n",
    "def create_dataset(X1_np_array, X2_np_array, y_np_array, batch_size=32, sort=False):\n",
    "    # sort the arrays\n",
    "    if sort == True:\n",
    "        # build an array containing the sequence lengths\n",
    "        sequence_lengths = list(map(lambda x: len(x), X1_np_array))\n",
    "        # sort the array but only get the indices\n",
    "        sorted_indices = np.argsort(sequence_lengths)\n",
    "        # now sort the X and y train arrays according to the sorted indicds\n",
    "        X1_np_array = X1_np_array[sorted_indices]\n",
    "        X2_np_array = X2_np_array[sorted_indices]\n",
    "        y_np_array = y_np_array[sorted_indices]\n",
    "\n",
    "    # create ragged tensor from in-homogeneous array\n",
    "    # using .constant is incredibly slow, even slower with parameters\n",
    "    # ie. 100 samples take 7 seconds, with parameters it takes 11 seconds\n",
    "    # using the following method that seems nuts it's a speedup for the previous example to 0.02 seconds\n",
    "    # X_tensor = tf.ragged.constant(X1_np_array, dtype=tf.int8, ragged_rank=1, row_splits_dtype=tf.int32)\n",
    "    # https://github.com/tensorflow/tensorflow/issues/47853\n",
    "    X_tensor = tf.RaggedTensor.from_row_lengths(\n",
    "        values=tf.concat(X1_np_array.tolist(), axis=0),\n",
    "        row_lengths=[len(a) for a in X1_np_array]\n",
    "    )\n",
    "    \n",
    "    # create dataset\n",
    "    ds = tf.data.Dataset.from_tensor_slices(({'inputs_1': X_tensor, 'inputs_2': X2_np_array}, y_np_array))\n",
    "\n",
    "    # create a dataset of dense tensors\n",
    "    ds = ds.map(reformat)\n",
    "\n",
    "    # apply padded batching to the dataset\n",
    "    ds = ds.padded_batch(batch_size)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13c3c1f-0eef-41c7-a4cb-8f1287335b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to plot two MAE arrays\n",
    "def plot_loss(train_mae, val_mae, start_epoch=1):\n",
    "    # get the number of epochs the training ran\n",
    "    epochs = range(start_epoch, len(train_mae) + 1)\n",
    "    # plot the graph\n",
    "    plt.plot(epochs, train_mae[start_epoch - 1:], \"bo\", label=\"Training\")\n",
    "    plt.plot(epochs, val_mae[start_epoch - 1:], \"b\", label=\"Validation\")\n",
    "    plt.title(\"Training and Validation Mean Absolute Error\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ee08f6-464e-4492-9931-69e754ab2d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model to work with\n",
    "# from keras-tuner run:\n",
    "#   conv_units: 424\n",
    "#   kernel_size: 30\n",
    "#   rate: 0.30000000000000004\n",
    "#   dense_units: 128\n",
    "def augur_model():\n",
    "    inputs_1 = layers.Input(shape=(None, 10), name='inputs_1')\n",
    "    \n",
    "    conv1 = layers.Conv1D(\n",
    "        filters=424,\n",
    "        kernel_size=30,\n",
    "        strides=1,\n",
    "        activation='relu',\n",
    "        padding='valid'\n",
    "    )(inputs_1)\n",
    "    norm1 = layers.BatchNormalization()(conv1)\n",
    "    drop1 = layers.Dropout(\n",
    "        rate=0.3\n",
    "    )(norm1)\n",
    "    pool1 = layers.GlobalMaxPool1D()(drop1)\n",
    "    # flat = layers.Flatten()(pool1)\n",
    "\n",
    "    inputs_2 = layers.Input(shape=(29,), name='inputs_2')\n",
    "\n",
    "    conc = layers.Concatenate(axis=1)([pool1, inputs_2])\n",
    "    \n",
    "    dense = layers.Dense(128, activation='relu')(conc)\n",
    "    outputs = layers.Dense(1)(dense)\n",
    "    \n",
    "    model = keras.Model(inputs=[inputs_1, inputs_2], outputs=outputs, name='Test')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "498e1980-40a0-4954-93be-294abc26ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model to work with\n",
    "# from keras-tuner run:\n",
    "#   conv_units: 424\n",
    "#   kernel_size: 30\n",
    "#   rate: 0.30000000000000004\n",
    "#   dense_units: 128\n",
    "def augur_model_v9():\n",
    "    inputs_1  = layers.Input(shape=(None, 10), name='inputs_1')\n",
    "    \n",
    "    conv1 = layers.Conv1D(\n",
    "        filters=424,\n",
    "        kernel_size=30,\n",
    "        strides=1,\n",
    "        activation='relu',\n",
    "        padding='valid'\n",
    "    )(inputs_1)\n",
    "    pool1 = layers.MaxPooling1D(\n",
    "        pool_size=2,\n",
    "        strides=None\n",
    "    )(conv1)\n",
    "    sdrop1 = layers.SpatialDropout1D(\n",
    "        rate=0.3,\n",
    "        seed=1202\n",
    "    )(pool1)\n",
    "    conv2 = layers.Conv1D(\n",
    "        filters=212,\n",
    "        kernel_size=15,\n",
    "        strides=1,\n",
    "        activation='relu',\n",
    "        padding='valid'\n",
    "    )(sdrop1)\n",
    "    pool2 = layers.MaxPooling1D(\n",
    "        pool_size=2,\n",
    "        strides=None\n",
    "    )(conv2)\n",
    "    sdrop2 = layers.SpatialDropout1D(\n",
    "        rate=0.3,\n",
    "        seed=1202\n",
    "    )(pool2)\n",
    "    conv3 = layers.Conv1D(\n",
    "        filters=212,\n",
    "        kernel_size=15,\n",
    "        strides=1,\n",
    "        activation='relu',\n",
    "        padding='valid'\n",
    "    )(sdrop2)\n",
    "    pool3 = layers.MaxPooling1D(\n",
    "        pool_size=2,\n",
    "        strides=None\n",
    "    )(conv3)\n",
    "    sdrop3 = layers.SpatialDropout1D(\n",
    "        rate=0.3,\n",
    "        seed=1202\n",
    "    )(pool3)\n",
    "    pool4 = layers.GlobalMaxPool1D()(sdrop3)\n",
    "\n",
    "    inputs_2 = layers.Input(shape=(29,), name='inputs_2')\n",
    "\n",
    "    conc = layers.Concatenate(axis=1)([pool4, inputs_2])\n",
    "    \n",
    "    dense1 = layers.Dense(128, activation='relu')(conc)\n",
    "    drop2 = layers.Dropout(\n",
    "        rate=0.3\n",
    "    )(dense1)\n",
    "    dense2 = layers.Dense(64, activation='relu')(drop2)\n",
    "    drop3 = layers.Dropout(\n",
    "        rate=0.3\n",
    "    )(dense2)\n",
    "    outputs = layers.Dense(1)(drop3)\n",
    "    \n",
    "    model = keras.Model(inputs=[inputs_1, inputs_2], outputs=outputs, name='Test')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81491a3c-2246-482f-ab53-c8ab281e24da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dan_zrimec_model():\n",
    "    inputs_1 = layers.Input(shape=(None, 10), name='inputs_1')\n",
    "    \n",
    "    # 1D convolution\n",
    "    conv = layers.Conv1D(\n",
    "        filters=280,\n",
    "        kernel_size=12, \n",
    "        strides=1, \n",
    "        activation='relu'\n",
    "    )(inputs_1)\n",
    "    # batch normalization\n",
    "    norm = layers.BatchNormalization()(conv)\n",
    "    # maxpool\n",
    "    pool = layers.MaxPooling1D(\n",
    "        pool_size=2,\n",
    "        strides=None\n",
    "    )(norm)\n",
    "    # dropout\n",
    "    drop = layers.Dropout(\n",
    "        rate=0.15\n",
    "    )(pool)\n",
    "    # bi-directional LSTM\n",
    "    bilstm = layers.Bidirectional(\n",
    "        layers.LSTM(\n",
    "            units=448, \n",
    "            return_sequences=True, \n",
    "            recurrent_dropout=0.3\n",
    "        ),\n",
    "        merge_mode='concat'\n",
    "        # input_shape=(8000, 4),\n",
    "    )(drop)\n",
    "    # maxpool\n",
    "    pool = layers.MaxPooling1D(\n",
    "        pool_size=2,\n",
    "        strides=None\n",
    "    )(bilstm)\n",
    "    drop = layers.Dropout(\n",
    "        rate=0.1\n",
    "    )(pool)\n",
    "    # flatten\n",
    "    # flat = layers.Flatten()(drop)\n",
    "    gmp = layers.GlobalMaxPool1D()(drop)\n",
    "\n",
    "    inputs_2 = layers.Input(shape=(29,), name='inputs_2')\n",
    "\n",
    "    conc = layers.Concatenate(axis=1)([gmp, inputs_2])\n",
    "    \n",
    "    # fully connected\n",
    "    dense = layers.Dense(\n",
    "        units=128,\n",
    "        activation='relu',\n",
    "    )(conc)\n",
    "    # batch normalization\n",
    "    norm = layers.BatchNormalization()(dense)\n",
    "    # dropout\n",
    "    drop = layers.Dropout(\n",
    "        rate=0.45\n",
    "    )(norm)\n",
    "    # dense\n",
    "    outputs = layers.Dense(units=1)(drop)\n",
    "\n",
    "    # model\n",
    "    model = keras.Model(inputs=[inputs_1, inputs_2], outputs=outputs, name='FullModel')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df069115-d601-4306-86e7-0fa0415e1606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Callback To Include in Callbacks List At Training Time\n",
    "class GarbageCollectorCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a69600b6-8ab4-4080-8bce-64dbbb308151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, train_ds, val_ds, epochs=10, start_epoch=1, oneshot=True, verbose=True, lr=0.001):\n",
    "    if verbose: \n",
    "        model.summary()\n",
    "        verbose_fit = 'auto'\n",
    "    else:\n",
    "        verbose_fit = 0\n",
    "    \n",
    "    model.compile(\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        metrics=[keras.metrics.MeanAbsoluteError()],\n",
    "    )\n",
    "\n",
    "    bar_callback = keras.callbacks.BackupAndRestore(backup_dir='./bar', save_freq='epoch')\n",
    "        \n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_ds,\n",
    "        verbose=verbose_fit,\n",
    "        callbacks=[GarbageCollectorCallback(), bar_callback]\n",
    "    )\n",
    "    \n",
    "    if oneshot == True:\n",
    "        plot_loss(\n",
    "            history.history['mean_absolute_error'],\n",
    "            history.history['val_mean_absolute_error'],\n",
    "            start_epoch\n",
    "        )\n",
    "\n",
    "    if val_ds != None:\n",
    "        return history.history['mean_absolute_error'], history.history['val_mean_absolute_error']\n",
    "    else:\n",
    "        return history.history['mean_absolute_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf21cd28-49ae-441a-9daa-8b0712296906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple timer from https://realpython.com/python-timer/\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self._start_time = None\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start a new timer\"\"\"\n",
    "        if self._start_time is not None:\n",
    "            print(f\"Timer is running. Use .stop() to stop it\")\n",
    "        else:\n",
    "            self._start_time = time.perf_counter()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer, and report the elapsed time\"\"\"\n",
    "        if self._start_time is None:\n",
    "            print(f\"Timer is not running. Use .start() to start it\")\n",
    "        else:\n",
    "            elapsed_time = time.perf_counter() - self._start_time\n",
    "            self._start_time = None\n",
    "            print(f\"    elapsed time: {elapsed_time:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf711275-0384-46df-9510-b7beaf2a2463",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "\n",
    "*explain it in more detail*\n",
    "\n",
    "X holds a list of sequences one hot encoded\n",
    "\n",
    "y holds a list of PTR values as floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "198c44f7-050c-4c96-a00b-0b4fcbf2c9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the prepared data back\n",
    "X1_raw = uncan_pickles('../data/multihot_x1.pickle.xz')\n",
    "X2 = uncan_pickles('../data/multihot_x2.pickle.xz')\n",
    "y = uncan_pickles('../data/multihot_y.pickle.xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77c01427-9b6f-4c4d-a2c2-49be0ba27abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refined numpy array build with smaller memory footprint\n",
    "X1 = np.empty(len(X1_raw), dtype=object)\n",
    "for id, seq in enumerate(X1_raw):\n",
    "  X1[id] = np.array(seq, dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a77d5bf-bbc1-4c38-8466-c2904242880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an inhomogenous numpy array from X\n",
    "# X1 = np.array(X1, dtype=object)\n",
    "X2 = np.array(X2, dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "668f2ec1-e6df-4287-b0ab-442001a822b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert type of target values from string to float\n",
    "y = np.array(y, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46f705-a1c2-4c59-ad0b-88ae2c943c7d",
   "metadata": {},
   "source": [
    "Random sample from X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4c1a3a7-3751-4649-9d85-8e17255a4c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]], dtype=int8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32f82f91-4bf2-4f29-8e12-92ba7196ea07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0], dtype=int8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "784bb474-f719-45fe-8836-beaef80dc222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.277"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01e465cd-e5c3-41e1-889c-f82254879828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214853, 214853, 214853)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of features and labels\n",
    "len(X1), len(X2), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0459c2-c83d-4920-a3d0-8c86f348a235",
   "metadata": {},
   "source": [
    "### Baseline PTR\n",
    "\n",
    "There is no common sense approach in finding a baseline for the protein-to-mRNA ratio of a particular mRNA sequence. This is what the *Basic Neural Network* approach is for - to determin a baseline and see if a slightly adapted neural network with feature engineered input can provide better predictions.\n",
    "\n",
    "But what can be done is to simply check the value range of the target PTRs, calculate mean and standard deviation. Given that the standard deviation is  small (12.5% of the value range) one can (stupidly) predict the mean value every time. From that it's possible to calculate the Mean Absolute Error (MAE) and compare that to the following neural network output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "941624fc-a917-4983-bfdd-d58fb88f0a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9085 9.974 4.9798956 0.8879484\n"
     ]
    }
   ],
   "source": [
    "# get some idea of the range of the PTR in the selected SAMPLE\n",
    "print(np.min(y), np.max(y), np.mean(y), np.std(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28d58d39-da1d-4f25-8f43-770dd077ae7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7119917"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple/dumb baseline mean absolute error of always predicting 4.974\n",
    "mae = np.mean(np.abs(np.array(y) - 4.974))\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81105338-e17c-4c50-b9c8-19e022c857eb",
   "metadata": {},
   "source": [
    "### Splits\n",
    "\n",
    "Split data in train and test sub sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3bec3f9-c83c-4690-b56c-ae3b97b877d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train and test sub sets\n",
    "X1_train_full, X1_test, X2_train_full, X2_test, y_train_full, y_test = train_test_split(X1, X2, y, test_size=0.2, random_state=1202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4205acc-8876-4855-9cc9-8b5948dd15ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training set again in train and validation\n",
    "X1_train, X1_val, X2_train, X2_val, y_train, y_val = train_test_split(X1_train_full, X2_train_full, y_train_full, test_size=0.2, random_state=1202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "582618a5-4e35-4998-acdf-84feff4d2f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f34c6b2-ff64-4861-918a-34c83e3a51ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.24 s, sys: 8.16 s, total: 12.4 s\n",
      "Wall time: 4.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build the test dataset\n",
    "test_ds = create_dataset(X1_test, X2_test, y_test, batch_size=batch_size, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa335eb5-5d74-4c78-b192-414edc015645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.2 s, sys: 28.2 s, total: 44.4 s\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build the training dataset\n",
    "train_full_ds = create_dataset(X1_train_full, X2_train_full, y_train_full, batch_size=batch_size, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e139b14-a23f-4899-b85e-5fe7d80c49fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 28.8 s, total: 41.9 s\n",
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build the training dataset\n",
    "train_ds = create_dataset(X1_train, X2_train, y_train, batch_size=batch_size, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "783f52f5-022b-4991-8337-c5fd4062b4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.63 s, sys: 7.59 s, total: 11.2 s\n",
      "Wall time: 3.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build the validation dataset\n",
    "val_ds = create_dataset(X1_val, X2_val, y_val, batch_size=batch_size, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac31aa-4582-45df-957c-d7aa0bb79449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT doing that any more since the speed up of the ragged tensor creation\n",
    "\n",
    "# save the previously generated dataset\n",
    "# since the ragged tensor creation takes a very, very long time\n",
    "# test_ds.save('../data/test_ds.tf.dataset')\n",
    "# train_full_ds.save('../data/train_full_ds.tf.dataset')\n",
    "# train_ds.save('../data/train_ds.tf.dataset')\n",
    "# val_ds.save('../data/val_ds.tf.dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16486337-06cf-4678-8a96-bde976a9df03",
   "metadata": {},
   "source": [
    "## Full Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eb4ce9-b1e5-4850-8bf2-2ea5e526926f",
   "metadata": {},
   "source": [
    "### Augur Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9840923-0fd8-4d3e-9472-bc6753de99a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model for 80\n",
      "Model: \"Test\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " inputs_1 (InputLayer)       [(None, None, 10)]           0         []                            \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, None, 424)            127624    ['inputs_1[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling1d_9 (MaxPoolin  (None, None, 424)            0         ['conv1d_10[0][0]']           \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " spatial_dropout1d_9 (Spati  (None, None, 424)            0         ['max_pooling1d_9[0][0]']     \n",
      " alDropout1D)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)          (None, None, 212)            1348532   ['spatial_dropout1d_9[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling1d_10 (MaxPooli  (None, None, 212)            0         ['conv1d_11[0][0]']           \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " spatial_dropout1d_10 (Spat  (None, None, 212)            0         ['max_pooling1d_10[0][0]']    \n",
      " ialDropout1D)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)          (None, None, 212)            674372    ['spatial_dropout1d_10[0][0]']\n",
      "                                                                                                  \n",
      " max_pooling1d_11 (MaxPooli  (None, None, 212)            0         ['conv1d_12[0][0]']           \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " spatial_dropout1d_11 (Spat  (None, None, 212)            0         ['max_pooling1d_11[0][0]']    \n",
      " ialDropout1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_3 (Gl  (None, 212)                  0         ['spatial_dropout1d_11[0][0]']\n",
      " obalMaxPooling1D)                                                                                \n",
      "                                                                                                  \n",
      " inputs_2 (InputLayer)       [(None, 29)]                 0         []                            \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 241)                  0         ['global_max_pooling1d_3[0][0]\n",
      " )                                                                  ',                            \n",
      "                                                                     'inputs_2[0][0]']            \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 128)                  30976     ['concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 128)                  0         ['dense_9[0][0]']             \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 64)                   8256      ['dropout_6[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 64)                   0         ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 1)                    65        ['dropout_7[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2189825 (8.35 MB)\n",
      "Trainable params: 2189825 (8.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/80\n",
      "2149/2149 [==============================] - 5521s 3s/step - loss: 1.7017 - mean_absolute_error: 0.9927 - val_loss: 0.9785 - val_mean_absolute_error: 0.8077\n",
      "Epoch 2/80\n",
      "2149/2149 [==============================] - 5531s 3s/step - loss: 1.0633 - mean_absolute_error: 0.8242 - val_loss: 0.9431 - val_mean_absolute_error: 0.7918\n",
      "Epoch 3/80\n",
      "2149/2149 [==============================] - 5570s 3s/step - loss: 0.9996 - mean_absolute_error: 0.8000 - val_loss: 0.9299 - val_mean_absolute_error: 0.7857\n",
      "Epoch 4/80\n",
      "2149/2149 [==============================] - 5529s 3s/step - loss: 0.9585 - mean_absolute_error: 0.7831 - val_loss: 0.8817 - val_mean_absolute_error: 0.7633\n",
      "Epoch 5/80\n",
      "2149/2149 [==============================] - 5540s 3s/step - loss: 0.9163 - mean_absolute_error: 0.7665 - val_loss: 0.8670 - val_mean_absolute_error: 0.7566\n",
      "Epoch 6/80\n",
      "2149/2149 [==============================] - 5534s 3s/step - loss: 0.8927 - mean_absolute_error: 0.7560 - val_loss: 0.8591 - val_mean_absolute_error: 0.7521\n",
      "Epoch 7/80\n",
      "2149/2149 [==============================] - 5538s 3s/step - loss: 0.8715 - mean_absolute_error: 0.7477 - val_loss: 0.8605 - val_mean_absolute_error: 0.7530\n",
      "Epoch 8/80\n",
      "2149/2149 [==============================] - 5911s 3s/step - loss: 0.8575 - mean_absolute_error: 0.7413 - val_loss: 0.8548 - val_mean_absolute_error: 0.7503\n",
      "Epoch 9/80\n",
      "2149/2149 [==============================] - 5582s 3s/step - loss: 0.8462 - mean_absolute_error: 0.7368 - val_loss: 0.8554 - val_mean_absolute_error: 0.7505\n",
      "Epoch 10/80\n",
      "2149/2149 [==============================] - 5574s 3s/step - loss: 0.8389 - mean_absolute_error: 0.7337 - val_loss: 0.8533 - val_mean_absolute_error: 0.7493\n",
      "Epoch 11/80\n",
      "2149/2149 [==============================] - 5566s 3s/step - loss: 0.8292 - mean_absolute_error: 0.7288 - val_loss: 0.8528 - val_mean_absolute_error: 0.7490\n",
      "Epoch 12/80\n",
      "2149/2149 [==============================] - 5554s 3s/step - loss: 0.8220 - mean_absolute_error: 0.7260 - val_loss: 0.8502 - val_mean_absolute_error: 0.7477\n",
      "Epoch 13/80\n",
      "2149/2149 [==============================] - 5531s 3s/step - loss: 0.8154 - mean_absolute_error: 0.7237 - val_loss: 0.8476 - val_mean_absolute_error: 0.7463\n",
      "Epoch 14/80\n",
      "2149/2149 [==============================] - 5527s 3s/step - loss: 0.8093 - mean_absolute_error: 0.7204 - val_loss: 0.8472 - val_mean_absolute_error: 0.7462\n",
      "Epoch 15/80\n",
      "2149/2149 [==============================] - 5522s 3s/step - loss: 0.8044 - mean_absolute_error: 0.7186 - val_loss: 0.8457 - val_mean_absolute_error: 0.7454\n",
      "Epoch 16/80\n",
      "2149/2149 [==============================] - 5508s 3s/step - loss: 0.8006 - mean_absolute_error: 0.7168 - val_loss: 0.8461 - val_mean_absolute_error: 0.7455\n",
      "Epoch 17/80\n",
      "2149/2149 [==============================] - 5501s 3s/step - loss: 0.7945 - mean_absolute_error: 0.7139 - val_loss: 0.8410 - val_mean_absolute_error: 0.7430\n",
      "Epoch 18/80\n",
      "2149/2149 [==============================] - 5492s 3s/step - loss: 0.7933 - mean_absolute_error: 0.7133 - val_loss: 0.8366 - val_mean_absolute_error: 0.7407\n",
      "Epoch 19/80\n",
      "2149/2149 [==============================] - 5573s 3s/step - loss: 0.7889 - mean_absolute_error: 0.7117 - val_loss: 0.8329 - val_mean_absolute_error: 0.7386\n",
      "Epoch 20/80\n",
      "2149/2149 [==============================] - 5527s 3s/step - loss: 0.7861 - mean_absolute_error: 0.7099 - val_loss: 0.8336 - val_mean_absolute_error: 0.7390\n",
      "Epoch 21/80\n",
      "2149/2149 [==============================] - 5511s 3s/step - loss: 0.7838 - mean_absolute_error: 0.7092 - val_loss: 0.8483 - val_mean_absolute_error: 0.7469\n",
      "Epoch 22/80\n",
      "2149/2149 [==============================] - 5507s 3s/step - loss: 0.7820 - mean_absolute_error: 0.7084 - val_loss: 0.8380 - val_mean_absolute_error: 0.7414\n",
      "Epoch 23/80\n",
      "2149/2149 [==============================] - 5506s 3s/step - loss: 0.7804 - mean_absolute_error: 0.7077 - val_loss: 0.8398 - val_mean_absolute_error: 0.7423\n",
      "Epoch 24/80\n",
      "2149/2149 [==============================] - 5500s 3s/step - loss: 0.7788 - mean_absolute_error: 0.7068 - val_loss: 0.8374 - val_mean_absolute_error: 0.7411\n",
      "Epoch 25/80\n",
      "2149/2149 [==============================] - 5486s 3s/step - loss: 0.7772 - mean_absolute_error: 0.7060 - val_loss: 0.8376 - val_mean_absolute_error: 0.7412\n",
      "Epoch 26/80\n",
      "2149/2149 [==============================] - 5561s 3s/step - loss: 0.7759 - mean_absolute_error: 0.7057 - val_loss: 0.8375 - val_mean_absolute_error: 0.7411\n",
      "Epoch 27/80\n",
      "2149/2149 [==============================] - 5571s 3s/step - loss: 0.7747 - mean_absolute_error: 0.7046 - val_loss: 0.8313 - val_mean_absolute_error: 0.7378\n",
      "Epoch 28/80\n",
      "2149/2149 [==============================] - 5542s 3s/step - loss: 0.7750 - mean_absolute_error: 0.7051 - val_loss: 0.8207 - val_mean_absolute_error: 0.7321\n",
      "Epoch 29/80\n",
      "2149/2149 [==============================] - 5531s 3s/step - loss: 0.7735 - mean_absolute_error: 0.7043 - val_loss: 0.8551 - val_mean_absolute_error: 0.7313\n",
      "Epoch 30/80\n",
      "2149/2149 [==============================] - 5522s 3s/step - loss: 0.7730 - mean_absolute_error: 0.7039 - val_loss: 0.8323 - val_mean_absolute_error: 0.7384\n",
      "Epoch 31/80\n",
      "2149/2149 [==============================] - 5787s 3s/step - loss: 0.7724 - mean_absolute_error: 0.7038 - val_loss: 0.8293 - val_mean_absolute_error: 0.7367\n",
      "Epoch 32/80\n",
      "2149/2149 [==============================] - 5563s 3s/step - loss: 0.7724 - mean_absolute_error: 0.7038 - val_loss: 0.8078 - val_mean_absolute_error: 0.7210\n",
      "Epoch 33/80\n",
      "2149/2149 [==============================] - 5524s 3s/step - loss: 0.7721 - mean_absolute_error: 0.7034 - val_loss: 0.9405 - val_mean_absolute_error: 0.7638\n",
      "Epoch 34/80\n",
      "2149/2149 [==============================] - 5495s 3s/step - loss: 0.7719 - mean_absolute_error: 0.7034 - val_loss: 0.9100 - val_mean_absolute_error: 0.7551\n",
      "Epoch 35/80\n",
      "2149/2149 [==============================] - 5575s 3s/step - loss: 0.7716 - mean_absolute_error: 0.7033 - val_loss: 0.8041 - val_mean_absolute_error: 0.7194\n",
      "Epoch 36/80\n",
      "2149/2149 [==============================] - 5558s 3s/step - loss: 0.7709 - mean_absolute_error: 0.7029 - val_loss: 2.1798 - val_mean_absolute_error: 1.2146\n",
      "Epoch 37/80\n",
      "2149/2149 [==============================] - 5529s 3s/step - loss: 0.7738 - mean_absolute_error: 0.7042 - val_loss: 0.8229 - val_mean_absolute_error: 0.7333\n",
      "Epoch 38/80\n",
      "2149/2149 [==============================] - 5517s 3s/step - loss: 0.7711 - mean_absolute_error: 0.7030 - val_loss: 0.8054 - val_mean_absolute_error: 0.7227\n",
      "Epoch 39/80\n",
      "2149/2149 [==============================] - 6204s 3s/step - loss: 0.7711 - mean_absolute_error: 0.7030 - val_loss: 0.8092 - val_mean_absolute_error: 0.7250\n",
      "Epoch 40/80\n",
      "2149/2149 [==============================] - 7223s 3s/step - loss: 0.7711 - mean_absolute_error: 0.7030 - val_loss: 0.7987 - val_mean_absolute_error: 0.7184\n",
      "Epoch 41/80\n",
      "2149/2149 [==============================] - 8182s 4s/step - loss: 0.7708 - mean_absolute_error: 0.7029 - val_loss: 0.7993 - val_mean_absolute_error: 0.7184\n",
      "Epoch 42/80\n",
      "2149/2149 [==============================] - 8076s 4s/step - loss: 0.7708 - mean_absolute_error: 0.7029 - val_loss: 0.9040 - val_mean_absolute_error: 0.7505\n",
      "Epoch 43/80\n",
      "2149/2149 [==============================] - 8142s 4s/step - loss: 0.7744 - mean_absolute_error: 0.7044 - val_loss: 0.8373 - val_mean_absolute_error: 0.7410\n",
      "Epoch 44/80\n",
      "2149/2149 [==============================] - 8243s 4s/step - loss: 0.7746 - mean_absolute_error: 0.7048 - val_loss: 0.8169 - val_mean_absolute_error: 0.7300\n",
      "Epoch 45/80\n",
      "1999/2149 [==========================>...] - ETA: 8:07 - loss: 0.7753 - mean_absolute_error: 0.7049"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining model for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m t\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 12\u001b[0m mae, val_mae \u001b[38;5;241m=\u001b[39m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moneshot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m t\u001b[38;5;241m.\u001b[39mstop()\n",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(model, train_ds, val_ds, epochs, start_epoch, oneshot, verbose, lr)\u001b[0m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      9\u001b[0m     loss\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mMeanSquaredError(),\n\u001b[1;32m     10\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlr),\n\u001b[1;32m     11\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[keras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mMeanAbsoluteError()],\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m bar_callback \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mBackupAndRestore(backup_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./bar\u001b[39m\u001b[38;5;124m'\u001b[39m, save_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose_fit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mGarbageCollectorCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbar_callback\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m oneshot \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     plot_loss(\n\u001b[1;32m     26\u001b[0m         history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_absolute_error\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     27\u001b[0m         history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_mean_absolute_error\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     28\u001b[0m         start_epoch\n\u001b[1;32m     29\u001b[0m     )\n",
      "File \u001b[0;32m/scratch/hawk/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/scratch/hawk/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/scratch/hawk/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/scratch/hawk/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/scratch/hawk/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/scratch/hawk/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/hawk/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/scratch/hawk/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/scratch/hawk/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/scratch/hawk/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m/scratch/hawk/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# epochs to run for\n",
    "epochs = 80\n",
    "\n",
    "# timer\n",
    "t = Timer()\n",
    "\n",
    "# current model\n",
    "model = augur_model_v9()\n",
    "# fit the model, validate and plot results\n",
    "print(f'training model for {epochs}')\n",
    "t.start()\n",
    "mae, val_mae = run_model(model, train_ds, val_ds, epochs=epochs, start_epoch=1, oneshot=True, verbose=True)\n",
    "t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83b9275-b52b-4b61-a1e0-07f4ba8d8e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "augur_mae = mae\n",
    "augur_val_mae = val_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9ecdfb-61a4-4dd0-9089-15022d0b72c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train final model\n",
    "\n",
    "# model file name\n",
    "model_name = 'model_v9_full_augur.keras'\n",
    "\n",
    "# timer\n",
    "t = Timer()\n",
    "\n",
    "# number of epochs to train for\n",
    "epochs = 37\n",
    "\n",
    "print(f'training final model {model_name} for {epochs} epochs')\n",
    "\n",
    "print('  building model')\n",
    "model = augur_model()\n",
    "\n",
    "print('  training model')\n",
    "t.start()\n",
    "run_model(model, train_full_ds, None, epochs=epochs, start_epoch=1, oneshot=False, verbose=False)\n",
    "t.stop()\n",
    "\n",
    "print('  evaluating model')\n",
    "mae = model.evaluate(test_ds)\n",
    "print(f'  mean absolute evaluation error is {mae}')\n",
    "\n",
    "print('  saving model')\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e430a16e-f2c6-444c-8892-e2b92b8a11d7",
   "metadata": {},
   "source": [
    "### DZ Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96105e2c-933d-405b-98aa-6576e6e2c792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs to run for\n",
    "epochs = 40\n",
    "\n",
    "# timer\n",
    "t = Timer()\n",
    "\n",
    "# current model\n",
    "model = dan_zrimec_model()\n",
    "# fit the model, validate and plot results\n",
    "print(f'training model for {epochs}')\n",
    "t.start()\n",
    "mae, val_mae = run_model(model, train_ds, val_ds, epochs=epochs, start_epoch=1, oneshot=True, verbose=True)\n",
    "t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16378ed5-1e16-4ee2-a687-c461ec5474b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dz_mae = mae\n",
    "dz_val_mae = val_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92111e58-c5f2-4594-85e4-c0d9ee663256",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(mae, val_mae, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3241db9d-874d-4404-bd59-087ab4edda50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train final model\n",
    "\n",
    "# model file name\n",
    "model_name = 'model_full_dz.keras'\n",
    "\n",
    "# timer\n",
    "t = Timer()\n",
    "\n",
    "# number of epochs to train for\n",
    "epochs = 30\n",
    "\n",
    "print(f'training final model {model_name} for {epochs} epochs')\n",
    "\n",
    "print('  building model')\n",
    "model = dan_zrimec_model()\n",
    "\n",
    "print('  training model')\n",
    "t.start()\n",
    "run_model(model, train_full_ds, None, epochs=epochs, start_epoch=1, oneshot=False, verbose=False)\n",
    "t.stop()\n",
    "\n",
    "print('  evaluating model')\n",
    "mae = model.evaluate(test_ds)\n",
    "print(f'  mean absolute evaluation error is {mae}')\n",
    "\n",
    "print('  saving model')\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd15effe-0d99-4c4b-ab7d-261f42efa608",
   "metadata": {},
   "source": [
    "## k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66d001b-11b1-4d6a-9dc0-90dc7a290af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31421c42-2e54-40a6-be5e-720f33e4dd3d",
   "metadata": {},
   "source": [
    "### Augur Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e139b274-5025-4118-a01f-2fb56657f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# run k-fold cross validation\n",
    "\n",
    "# epochs to run for\n",
    "epochs = 100\n",
    "\n",
    "# timer\n",
    "t = Timer()\n",
    "\n",
    "# training and validation mean absolute error results\n",
    "train_mae = []\n",
    "val_mae = []\n",
    "\n",
    "print(f'k-fold cross validation with {num_splits} splits for {epochs} epochs')\n",
    "kf = KFold(n_splits=num_splits, shuffle=True, random_state=1202)\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "    print(f'  processing fold {i}')\n",
    "\n",
    "    # split the data\n",
    "    print('    splitting data')\n",
    "    t.start()\n",
    "    X_train_kf, X_val_kf, y_train_kf, y_val_kf = X_train[train_index], X_train[val_index], y_train[train_index], y_train[val_index]\n",
    "    t.stop()\n",
    "\n",
    "    # build the datasets\n",
    "    print('    creating training dataset')\n",
    "    t.start()\n",
    "    train_ds = create_dataset(X_train_kf, y_train_kf, batch_size=batch_size, sort=True)\n",
    "    t.stop()\n",
    "    print('    creating validation dataset')\n",
    "    t.start()\n",
    "    val_ds = create_dataset(X_val_kf, y_val_kf, batch_size=batch_size, sort=False)\n",
    "    t.stop()\n",
    "\n",
    "    # fit the model and return mae\n",
    "    print('    fitting model')\n",
    "    t.start()\n",
    "    t_mae, v_mae = run_model(augur_model(), train_ds, val_ds, epochs=epochs, start_epoch=1, oneshot=False, verbose=False)\n",
    "    t.stop()\n",
    "\n",
    "    # add returned mae values to the arrays\n",
    "    train_mae.append(t_mae)\n",
    "    val_mae.append(v_mae)\n",
    "\n",
    "# calculate the average\n",
    "average_train_mae = [ np.mean([ x[i] for x in train_mae ]) for i in range(epochs) ]\n",
    "average_val_mae = [ np.mean([ x[i] for x in val_mae ]) for i in range(epochs) ]\n",
    "\n",
    "# plot\n",
    "plot_loss(average_train_mae, average_val_mae, start_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c833d59-8d0b-4c09-8b27-32841e29e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train final model\n",
    "\n",
    "# model file name\n",
    "model_name = 'model_augur.keras'\n",
    "\n",
    "# timer\n",
    "t = Timer()\n",
    "\n",
    "# training and validation mean absolute error results\n",
    "train_mae = []\n",
    "val_mae = []\n",
    "\n",
    "# number of epochs to train for\n",
    "epochs = 40\n",
    "\n",
    "print(f'training final model {model_name} for {epochs} epochs')\n",
    "\n",
    "print('  creating training dataset')\n",
    "t.start()\n",
    "train_ds = create_dataset(X_train, y_train, batch_size=batch_size, sort=True)\n",
    "t.stop()\n",
    "\n",
    "print('  building model')\n",
    "model = augur_model()\n",
    "\n",
    "print('  training model')\n",
    "t.start()\n",
    "run_model(model, train_ds, None, epochs=epochs, start_epoch=1, oneshot=False, verbose=False)\n",
    "t.stop()\n",
    "\n",
    "print('  evaluating model')\n",
    "mae = model.evaluate(test_ds)\n",
    "print(f'  mean absolute evaluation error is {mae}')\n",
    "\n",
    "print('  saving model')\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e83109b-8577-4591-945e-c2d08491d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# run k-fold cross validation\n",
    "\n",
    "# epochs to run for\n",
    "epochs = 100\n",
    "\n",
    "# timer\n",
    "t = Timer()\n",
    "\n",
    "# training and validation mean absolute error results\n",
    "train_mae = []\n",
    "val_mae = []\n",
    "\n",
    "print(f'k-fold cross validation with {num_splits} splits for {epochs} epochs')\n",
    "kf = KFold(n_splits=num_splits, shuffle=True, random_state=1202)\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "    print(f'  processing fold {i}')\n",
    "\n",
    "    # split the data\n",
    "    print('    splitting data')\n",
    "    t.start()\n",
    "    X_train_kf, X_val_kf, y_train_kf, y_val_kf = X_train[train_index], X_train[val_index], y_train[train_index], y_train[val_index]\n",
    "    t.stop()\n",
    "\n",
    "    # build the datasets\n",
    "    print('    creating training dataset')\n",
    "    t.start()\n",
    "    train_ds = create_dataset(X_train_kf, y_train_kf, batch_size=batch_size, sort=True)\n",
    "    t.stop()\n",
    "    print('    creating validation dataset')\n",
    "    t.start()\n",
    "    val_ds = create_dataset(X_val_kf, y_val_kf, batch_size=batch_size, sort=False)\n",
    "    t.stop()\n",
    "\n",
    "    # fit the model and return mae\n",
    "    print('    fitting model')\n",
    "    t.start()\n",
    "    t_mae, v_mae = run_model(dan_zrimec_model(), train_ds, val_ds, epochs=epochs, start_epoch=1, oneshot=False, verbose=False)\n",
    "    t.stop()\n",
    "\n",
    "    # add returned mae values to the arrays\n",
    "    train_mae.append(t_mae)\n",
    "    val_mae.append(v_mae)\n",
    "\n",
    "# calculate the average\n",
    "average_train_mae = [ np.mean([ x[i] for x in train_mae ]) for i in range(epochs) ]\n",
    "average_val_mae = [ np.mean([ x[i] for x in val_mae ]) for i in range(epochs) ]\n",
    "\n",
    "# plot\n",
    "plot_loss(average_train_mae, average_val_mae, start_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c94735-fe6c-46bc-815d-ef0a7f6def94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# run k-fold cross validation\n",
    "\n",
    "# epochs to run for\n",
    "epochs = 100\n",
    "\n",
    "# timer\n",
    "t = Timer()\n",
    "\n",
    "# training and validation mean absolute error results\n",
    "train_mae = []\n",
    "val_mae = []\n",
    "\n",
    "print(f'k-fold cross validation with {num_splits} splits for {epochs} epochs')\n",
    "kf = KFold(n_splits=num_splits, shuffle=True, random_state=1202)\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "    print(f'  processing fold {i}')\n",
    "\n",
    "    # split the data\n",
    "    print('    splitting data')\n",
    "    t.start()\n",
    "    X_train_kf, X_val_kf, y_train_kf, y_val_kf = X_train[train_index], X_train[val_index], y_train[train_index], y_train[val_index]\n",
    "    t.stop()\n",
    "\n",
    "    # build the datasets\n",
    "    print('    creating training dataset')\n",
    "    t.start()\n",
    "    train_ds = create_dataset(X_train_kf, y_train_kf, batch_size=batch_size, sort=True)\n",
    "    t.stop()\n",
    "    print('    creating validation dataset')\n",
    "    t.start()\n",
    "    val_ds = create_dataset(X_val_kf, y_val_kf, batch_size=batch_size, sort=False)\n",
    "    t.stop()\n",
    "\n",
    "    # fit the model and return mae\n",
    "    print('    fitting model')\n",
    "    t.start()\n",
    "    t_mae, v_mae = run_model(dan_zrimec_model2(), train_ds, val_ds, epochs=epochs, start_epoch=1, oneshot=False, verbose=False)\n",
    "    t.stop()\n",
    "\n",
    "    # add returned mae values to the arrays\n",
    "    train_mae.append(t_mae)\n",
    "    val_mae.append(v_mae)\n",
    "\n",
    "# calculate the average\n",
    "average_train_mae = [ np.mean([ x[i] for x in train_mae ]) for i in range(epochs) ]\n",
    "average_val_mae = [ np.mean([ x[i] for x in val_mae ]) for i in range(epochs) ]\n",
    "\n",
    "# plot\n",
    "plot_loss(average_train_mae, average_val_mae, start_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8053c0-8c5f-4da9-9217-d8bb8a1433b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "181cfeca-7bde-4da8-aa66-3e4379026cd5",
   "metadata": {},
   "source": [
    "# what are the steps?\n",
    "\n",
    "- create ragged tensor from X_test\n",
    "- create dataset from X_test and y_test\n",
    "\n",
    "k-fold cross validation for the baseline neural network\n",
    "- input is X_train, y_train\n",
    "- using KFold this will be split in k folds\n",
    "  - sort the trainin part\n",
    "  - convert X to ragged tensor\n",
    "  - create dataset\n",
    "  - train\n",
    "  - evaluate\n",
    "- evaluate the cross fold performance\n",
    "- rerun model with the whole dataset (test+val)\n",
    "- save the model\n",
    "```\n",
    "kf = KFold(n_splits=num_splits, shuffle=True, random_state=1202)\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    X_train_kf, X_test_kf, y_train_kf, y_test_kf = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
    "    # sort X_train_kf and y_train_kf\n",
    "    # convert X_train_kf to ragged tensor\n",
    "    # create dataset from X_train_kf and y_train_kf\n",
    "    # do the training and evaluation\n",
    "# evaluate the cross fold performance\n",
    "```\n",
    "\n",
    "train/val split for the full neural network -> separate jupyter notebook\n",
    "- input X_train, y_train\n",
    "- split the training test set again in train and val\n",
    "- create ragged tensor from training X\n",
    "- create dataset from training X and y\n",
    "- train\n",
    "- evaluate\n",
    "- rerun the model with the whole dataset (test+val)\n",
    "- save the model\n",
    "\n",
    "\n",
    "what have both in common?\n",
    "- data load and initial prep (up to the first split)\n",
    "- model setup\n",
    "- training setup\n",
    "- visualisation\n",
    "- evaluation -> this is just one command\n",
    "\n",
    "helpful methods\n",
    "- create_dataset(X, y, sort=False)\n",
    "  input X and y, specify if the dataset should be sorted\n",
    "  returns a TF dataset\n",
    "- run_model(model, epochs, plot=True, plot_epoch_start=0)\n",
    "- plot_loss()\n",
    "\n",
    "- callbacks\n",
    "    - earlystopping -> to limit training that doesn't progress, only for the tuner\n",
    "    - backupandrestore -> for the full training as fault tolerance setup\n",
    "    - modelcheckpoint -> to save the best model on the final train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc7e7c-4981-4ce5-9435-b20fea7cd399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b8d99d-4c8c-4af2-b9ce-77df859362a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the train set again in train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c86b943-3471-42b6-bef0-e60a0e376568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the first unique PTR value that is also in y_train\n",
    "train_idx = 0\n",
    "for i in range(len(y)):\n",
    "    count = 0\n",
    "    for l in range(len(y)):\n",
    "        if i != l and y[i] == y[l]:\n",
    "            count += 1\n",
    "            continue\n",
    "    if count == 0:\n",
    "        for m in range(len(y_train)):\n",
    "            if y[i] == y_train[m]:\n",
    "                train_idx = m\n",
    "                break\n",
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f1c55-480e-459b-b295-ddc038493283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample\n",
    "X_train[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82384079-68d4-4ca6-9301-b856d56aeb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the matching target\n",
    "search_y = y_train[train_idx]\n",
    "search_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737386f4-6bdc-446b-8f31-a910f9f015c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the target value in the raw dataset\n",
    "full_idx = 0\n",
    "for i in range(len(y)):\n",
    "    if y[i] == search_y:\n",
    "        print(i)\n",
    "        full_idx = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29af14a0-e1a3-45ab-8d0b-56dc0020917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare if the raw dataset entry matches the subset entry\n",
    "if X[full_idx].all() == X_train[train_idx].all():\n",
    "    print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f525a1-3fef-4b27-b7ee-eb136ff06ebc",
   "metadata": {},
   "source": [
    "### Sort Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fc569c-9d02-4619-82bf-ebf4e4caaf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an inhomogenous numpy array from the training set\n",
    "X_train = np.array(X_train, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc1d8b7-225a-43b4-a3a7-344569269d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an array containing the sequence lengths\n",
    "sequence_lengths = list(map(lambda x: len(x), X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360be4d-b70c-4fe6-9515-a0b904e0fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the array but only get the indices\n",
    "sorted_indices = np.argsort(sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0200e456-a746-417d-bc9c-ca4f75fa973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75abd9cc-7c09-40b0-b5a0-619bfc2e5a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now sort the X and y train arrays according to the sorted indicds\n",
    "X_train = X_train[sorted_indices]\n",
    "y_train = y_train[sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9070a40-d333-4008-9a8b-edb277a5209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the previously found values still correlate\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == search_y:\n",
    "        print(X_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e10faa-ea20-42bb-bd97-75395da099c9",
   "metadata": {},
   "source": [
    "### Ragged Tensor Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86fae6d-0aa3-4bb5-97ec-e2e8920e1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does not work since the sequences are of different length\n",
    "# X_test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d39e886-2279-4bbb-a838-c44e26ba3ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train_tensor = tf.ragged.constant(X_train, dtype=tf.int8, ragged_rank=1, row_splits_dtype=tf.int32)\n",
    "X_val_tensor = tf.ragged.constant(X_val, dtype=tf.int8, ragged_rank=1, row_splits_dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850a17ad-bb52-4365-9424-3447b37529ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dataset = tf.data.Dataset.from_tensor_slices((X_train_tensor, y_train))\n",
    "X_val_dataset = tf.data.Dataset.from_tensor_slices((X_val_tensor, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c506f768-6e15-4c6f-872b-0038d177dd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907d3def-dbf3-4830-aad6-5368d664e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a705eb41-faff-44f3-b262-88b883ec013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded batches from ragged tensors are not supported (yet)\n",
    "# it needs a work around creating a uniform tensor\n",
    "# idea from : https://github.com/tensorflow/tensorflow/issues/39163\n",
    "def reformat(data, label):\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805f9521-15da-4791-9a6a-5cf3dca18a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dataset = X_train_dataset.map(reformat)\n",
    "X_val_dataset = X_val_dataset.map(reformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e1c8a-efb9-455b-a3b3-cabc2d02daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataset (again) and create padded batches\n",
    "batch_size = 64\n",
    "X_train_dataset = X_train_dataset.padded_batch(batch_size)\n",
    "X_val_dataset = X_val_dataset.shuffle(buffer_size=len(X_val), seed=1202).padded_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2fa149-8b00-4210-8a33-2a3b810cbd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optinally repeat the dataset multiple times -> WHY?\n",
    "# rep = 3\n",
    "# X_train_dataset = X_train_dataset.repeat(rep)\n",
    "# X_val_dataset = X_val_dataset.repeat(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49244b0a-a161-457b-88d2-ba4417d89110",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalen = []\n",
    "ds_iterator = iter(X_train_dataset)\n",
    "for data, label in ds_iterator:\n",
    "    datalen.append(len(data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa0eb29-ea9c-4272-8cf9-682c9fe01451",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalen[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ba5416-4ed2-4d5f-8123-5e67e46b4dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing if keras can use the dataset\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=(None,4)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.fit(X_train_dataset, epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
