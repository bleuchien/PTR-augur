{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1ce73b25-d36d-413d-a323-67e9a246b21b",
      "metadata": {
        "id": "1ce73b25-d36d-413d-a323-67e9a246b21b"
      },
      "source": [
        "# Full Neural Network Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = '/content/drive'\n",
        "nb_path = base_path + '/MyDrive/PTR-augur/notebooks'"
      ],
      "metadata": {
        "id": "mSAF7MIQ-z1n"
      },
      "id": "mSAF7MIQ-z1n",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(base_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_Z0HQD-_nXw",
        "outputId": "d14ae6ca-b760-4e51-8e59-8a53cb5fc605"
      },
      "id": "F_Z0HQD-_nXw",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2874d9ad-1cd5-4f33-bcf6-178552b4ecc4",
      "metadata": {
        "id": "2874d9ad-1cd5-4f33-bcf6-178552b4ecc4"
      },
      "outputs": [],
      "source": [
        "# library dependencies\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import lzma\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "import keras\n",
        "from keras import layers\n",
        "import tensorflow as tf\n",
        "# import keras_tuner\n",
        "import math\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7b634de2-ee59-4979-ae29-9ce9ab8a2909",
      "metadata": {
        "id": "7b634de2-ee59-4979-ae29-9ce9ab8a2909"
      },
      "outputs": [],
      "source": [
        "# method to store data as serialized binary structure lzma compressed\n",
        "def can_pickles(data, filename):\n",
        "    with lzma.LZMAFile(filename, 'wb') as f:\n",
        "        pickle.dump(data, f, pickle.DEFAULT_PROTOCOL)\n",
        "\n",
        "# method to retrieve data from a compressed pickle file (created with the method above)\n",
        "def uncan_pickles(filename):\n",
        "    with lzma.LZMAFile(filename, 'rb') as f:\n",
        "        return pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "29c2a628-8756-4b25-844b-10352d9ce814",
      "metadata": {
        "id": "29c2a628-8756-4b25-844b-10352d9ce814"
      },
      "outputs": [],
      "source": [
        "# helper method to create a valid dataset\n",
        "# padded batches from ragged tensors are not supported (yet)\n",
        "# it needs a work around creating a uniform tensor\n",
        "# idea from : https://github.com/tensorflow/tensorflow/issues/39163\n",
        "def reformat(data, label):\n",
        "    return data, label\n",
        "\n",
        "# method to create a TF dataset\n",
        "def create_dataset(X1_np_array, X2_np_array, y_np_array, batch_size=32, sort=False, step_size=5):\n",
        "    # sort the arrays\n",
        "    if sort == True:\n",
        "        # build an array containing the sequence lengths\n",
        "        sequence_lengths = list(map(lambda x: len(x), X1_np_array))\n",
        "        # sort the array but only get the indices\n",
        "        sorted_indices = np.argsort(sequence_lengths)\n",
        "        # now sort the X and y train arrays according to the sorted indicds\n",
        "        X1_np_array = X1_np_array[sorted_indices]\n",
        "        X2_np_array = X2_np_array[sorted_indices]\n",
        "        y_np_array = y_np_array[sorted_indices]\n",
        "\n",
        "    # create ragged tensor from in-homogeneous array\n",
        "    # using .constant is incredibly slow, even slower with parameters\n",
        "    # ie. 100 samples take 7 seconds, with parameters it takes 11 seconds\n",
        "    # using the following method that seems nuts it's a speedup for the previous example to 0.02 seconds\n",
        "    # X_tensor = tf.ragged.constant(X1_np_array, dtype=tf.int8, ragged_rank=1, row_splits_dtype=tf.int32)\n",
        "    # https://github.com/tensorflow/tensorflow/issues/47853\n",
        "    # with tf.device('/device:cpu:0'):\n",
        "    X_tensor = tf.RaggedTensor.from_row_lengths(\n",
        "        values=tf.concat(X1_np_array.tolist(), axis=0),\n",
        "        row_lengths=[len(a) for a in X1_np_array]\n",
        "    )\n",
        "\n",
        "    # hack to work around a GPU out of memory problem\n",
        "    # X_tensor = None\n",
        "    # step_size = step_size\n",
        "    # steps = math.ceil(len(X1_np_array) / step_size)\n",
        "    # for i in range(steps):\n",
        "    #   start = step_size * i\n",
        "    #   stop = step_size * (i + 1)\n",
        "    #   if stop > len(X1_np_array): stop = len(X1_np_array)\n",
        "    #   # print(f'start {start} - stop {stop}')\n",
        "\n",
        "    #   sub_X1_np_array = X1_np_array[start:stop]\n",
        "\n",
        "    #   foo = tf.RaggedTensor.from_row_lengths(\n",
        "    #       values=tf.concat(sub_X1_np_array.tolist(), axis=0),\n",
        "    #       row_lengths=[len(a) for a in sub_X1_np_array]\n",
        "    #   )\n",
        "\n",
        "    #   if X_tensor == None:\n",
        "    #     X_tensor = foo\n",
        "    #   else:\n",
        "    #     X_tensor = tf.concat([X_tensor, foo], axis=0)\n",
        "\n",
        "\n",
        "    # create dataset\n",
        "    ds = tf.data.Dataset.from_tensor_slices(({'inputs_1': X_tensor, 'inputs_2': X2_np_array}, y_np_array))\n",
        "\n",
        "    # create a dataset of dense tensors\n",
        "    ds = ds.map(reformat)\n",
        "\n",
        "    # apply padded batching to the dataset\n",
        "    ds = ds.padded_batch(batch_size)\n",
        "\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d13c3c1f-0eef-41c7-a4cb-8f1287335b64",
      "metadata": {
        "id": "d13c3c1f-0eef-41c7-a4cb-8f1287335b64"
      },
      "outputs": [],
      "source": [
        "_# method to plot two MAE arrays\n",
        "def plot_loss(train_mae, val_mae, start_epoch=1):\n",
        "    # get the number of epochs the training ran\n",
        "    epochs = range(start_epoch, len(train_mae) + 1)\n",
        "    # plot the graph\n",
        "    plt.plot(epochs, train_mae, \"bo\", label=\"Training\")\n",
        "    plt.plot(epochs, val_mae, \"b\", label=\"Validation\")\n",
        "    plt.title(\"Training and Validation Mean Absolute Error\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"MAE\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "57ee08f6-464e-4492-9931-69e754ab2d40",
      "metadata": {
        "id": "57ee08f6-464e-4492-9931-69e754ab2d40"
      },
      "outputs": [],
      "source": [
        "# model to work with\n",
        "# from keras-tuner run:\n",
        "#   conv_units: 424\n",
        "#   kernel_size: 30\n",
        "#   rate: 0.30000000000000004\n",
        "#   dense_units: 128\n",
        "def augur_model_v10():\n",
        "    inputs_1  = layers.Input(shape=(None, 10), name='inputs_1')\n",
        "\n",
        "    conv1 = layers.Conv1D(\n",
        "        filters=424,\n",
        "        kernel_size=30,\n",
        "        strides=1,\n",
        "        activation='relu',\n",
        "        padding='valid'\n",
        "    )(inputs_1)\n",
        "    pool1 = layers.MaxPooling1D(\n",
        "        pool_size=2,\n",
        "        strides=None\n",
        "    )(conv1)\n",
        "    sdrop1 = layers.SpatialDropout1D(\n",
        "        rate=0.3,\n",
        "        seed=1202\n",
        "    )(pool1)\n",
        "    conv2 = layers.Conv1D(\n",
        "        filters=212,\n",
        "        kernel_size=15,\n",
        "        strides=1,\n",
        "        activation='relu',\n",
        "        padding='valid'\n",
        "    )(sdrop1)\n",
        "    pool2 = layers.MaxPooling1D(\n",
        "        pool_size=2,\n",
        "        strides=None\n",
        "    )(conv2)\n",
        "    sdrop2 = layers.SpatialDropout1D(\n",
        "        rate=0.3,\n",
        "        seed=1202\n",
        "    )(pool2)\n",
        "    conv3 = layers.Conv1D(\n",
        "        filters=212,\n",
        "        kernel_size=15,\n",
        "        strides=1,\n",
        "        activation='relu',\n",
        "        padding='valid'\n",
        "    )(sdrop2)\n",
        "    pool3 = layers.MaxPooling1D(\n",
        "        pool_size=2,\n",
        "        strides=None\n",
        "    )(conv3)\n",
        "    sdrop3 = layers.SpatialDropout1D(\n",
        "        rate=0.3,\n",
        "        seed=1202\n",
        "    )(pool3)\n",
        "    bilstm = layers.Bidirectional(\n",
        "        layers.LSTM(\n",
        "            units=512,\n",
        "            return_sequences=True,\n",
        "            recurrent_dropout=0.3\n",
        "        ),\n",
        "        merge_mode='sum'\n",
        "    )(pool3)\n",
        "    pool4 = layers.GlobalMaxPool1D()(bilstm)\n",
        "\n",
        "    inputs_2 = layers.Input(shape=(29,), name='inputs_2')\n",
        "\n",
        "    conc = layers.Concatenate(axis=1)([pool4, inputs_2])\n",
        "\n",
        "    dense1 = layers.Dense(128, activation='relu')(conc)\n",
        "    drop2 = layers.Dropout(\n",
        "        rate=0.3\n",
        "    )(dense1)\n",
        "    dense2 = layers.Dense(64, activation='relu')(drop2)\n",
        "    drop3 = layers.Dropout(\n",
        "        rate=0.3\n",
        "    )(dense2)\n",
        "    outputs = layers.Dense(1)(drop3)\n",
        "\n",
        "    model = keras.Model(inputs=[inputs_1, inputs_2], outputs=outputs, name='Test')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81491a3c-2246-482f-ab53-c8ab281e24da",
      "metadata": {
        "id": "81491a3c-2246-482f-ab53-c8ab281e24da"
      },
      "outputs": [],
      "source": [
        "def dan_zrimec_model():\n",
        "    inputs_1 = layers.Input(shape=(None, 10), name='inputs_1')\n",
        "\n",
        "    # 1D convolution\n",
        "    conv = layers.Conv1D(\n",
        "        filters=280,\n",
        "        kernel_size=12,\n",
        "        strides=1,\n",
        "        activation='relu'\n",
        "    )(inputs_1)\n",
        "    # batch normalization\n",
        "    norm = layers.BatchNormalization()(conv)\n",
        "    # maxpool\n",
        "    pool = layers.MaxPooling1D(\n",
        "        pool_size=2,\n",
        "        strides=None\n",
        "    )(norm)\n",
        "    # dropout\n",
        "    drop = layers.Dropout(\n",
        "        rate=0.15\n",
        "    )(pool)\n",
        "    # bi-directional LSTM\n",
        "    bilstm = layers.Bidirectional(\n",
        "        layers.LSTM(\n",
        "            units=448,\n",
        "            return_sequences=True,\n",
        "            recurrent_dropout=0.3\n",
        "        ),\n",
        "        merge_mode='concat'\n",
        "        # input_shape=(8000, 4),\n",
        "    )(drop)\n",
        "    # maxpool\n",
        "    pool = layers.MaxPooling1D(\n",
        "        pool_size=2,\n",
        "        strides=None\n",
        "    )(bilstm)\n",
        "    drop = layers.Dropout(\n",
        "        rate=0.1\n",
        "    )(pool)\n",
        "    # flatten\n",
        "    # flat = layers.Flatten()(drop)\n",
        "    gmp = layers.GlobalMaxPool1D()(drop)\n",
        "\n",
        "    inputs_2 = layers.Input(shape=(29,), name='inputs_2')\n",
        "\n",
        "    conc = layers.Concatenate(axis=1)([gmp, inputs_2])\n",
        "\n",
        "    # fully connected\n",
        "    dense = layers.Dense(\n",
        "        units=128,\n",
        "        activation='relu',\n",
        "    )(conc)\n",
        "    # batch normalization\n",
        "    norm = layers.BatchNormalization()(dense)\n",
        "    # dropout\n",
        "    drop = layers.Dropout(\n",
        "        rate=0.45\n",
        "    )(norm)\n",
        "    # dense\n",
        "    outputs = layers.Dense(units=1)(drop)\n",
        "\n",
        "    # model\n",
        "    model = keras.Model(inputs=[inputs_1, inputs_2], outputs=outputs, name='FullModel')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a69600b6-8ab4-4080-8bce-64dbbb308151",
      "metadata": {
        "id": "a69600b6-8ab4-4080-8bce-64dbbb308151"
      },
      "outputs": [],
      "source": [
        "def run_model(model, train_ds, val_ds, epochs=10, start_epoch=1, oneshot=True, verbose=True):\n",
        "    if verbose:\n",
        "        model.summary()\n",
        "        verbose_fit = 'auto'\n",
        "    else:\n",
        "        verbose_fit = 0\n",
        "\n",
        "    model.compile(\n",
        "        loss=keras.losses.MeanSquaredError(),\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        metrics=[keras.metrics.MeanAbsoluteError()],\n",
        "    )\n",
        "\n",
        "    callback = keras.callbacks.BackupAndRestore(backup_dir=nb_path + '/bar', save_freq='epoch')\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        epochs=epochs,\n",
        "        validation_data=val_ds,\n",
        "        verbose=verbose_fit,\n",
        "        callbacks=[callback]\n",
        "    )\n",
        "\n",
        "    if oneshot == True:\n",
        "        plot_loss(\n",
        "            history.history['mean_absolute_error'],\n",
        "            history.history['val_mean_absolute_error'],\n",
        "            start_epoch\n",
        "        )\n",
        "\n",
        "    if val_ds != None:\n",
        "        return history.history['mean_absolute_error'], history.history['val_mean_absolute_error']\n",
        "    else:\n",
        "        return history.history['mean_absolute_error']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "bf21cd28-49ae-441a-9daa-8b0712296906",
      "metadata": {
        "id": "bf21cd28-49ae-441a-9daa-8b0712296906"
      },
      "outputs": [],
      "source": [
        "# simple timer from https://realpython.com/python-timer/\n",
        "\n",
        "class Timer:\n",
        "    def __init__(self):\n",
        "        self._start_time = None\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start a new timer\"\"\"\n",
        "        if self._start_time is not None:\n",
        "            print(f\"Timer is running. Use .stop() to stop it\")\n",
        "        else:\n",
        "            self._start_time = time.perf_counter()\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop the timer, and report the elapsed time\"\"\"\n",
        "        if self._start_time is None:\n",
        "            print(f\"Timer is not running. Use .start() to start it\")\n",
        "        else:\n",
        "            elapsed_time = time.perf_counter() - self._start_time\n",
        "            self._start_time = None\n",
        "            print(f\"    elapsed time: {elapsed_time:0.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf711275-0384-46df-9510-b7beaf2a2463",
      "metadata": {
        "id": "cf711275-0384-46df-9510-b7beaf2a2463"
      },
      "source": [
        "## Data Prep\n",
        "\n",
        "*explain it in more detail*\n",
        "\n",
        "X holds a list of sequences one hot encoded\n",
        "\n",
        "y holds a list of PTR values as floats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "198c44f7-050c-4c96-a00b-0b4fcbf2c9fb",
      "metadata": {
        "id": "198c44f7-050c-4c96-a00b-0b4fcbf2c9fb"
      },
      "outputs": [],
      "source": [
        "# read the prepared data back\n",
        "X1_raw = uncan_pickles(nb_path + '/../data/multihot_x1.pickle.xz')\n",
        "X2 = uncan_pickles(nb_path + '/../data/multihot_x2.pickle.xz')\n",
        "y = uncan_pickles(nb_path + '/../data/multihot_y.pickle.xz')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# refined numpy array build with smaller memory footprint\n",
        "X1 = np.empty(len(X1_raw), dtype=object)\n",
        "for id, seq in enumerate(X1_raw):\n",
        "  X1[id] = np.array(seq, dtype=np.int8)"
      ],
      "metadata": {
        "id": "CVtsw23Oj4YB"
      },
      "id": "CVtsw23Oj4YB",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7a77d5bf-bbc1-4c38-8466-c2904242880b",
      "metadata": {
        "id": "7a77d5bf-bbc1-4c38-8466-c2904242880b"
      },
      "outputs": [],
      "source": [
        "# build an inhomogenous numpy array from X\n",
        "# X1 = np.array(X1, dtype=object) # too high memory usage\n",
        "X2 = np.array(X2, dtype=np.int8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "668f2ec1-e6df-4287-b0ab-442001a822b7",
      "metadata": {
        "id": "668f2ec1-e6df-4287-b0ab-442001a822b7"
      },
      "outputs": [],
      "source": [
        "# convert type of target values from string to float\n",
        "y = np.array(y, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d46f705-a1c2-4c59-ad0b-88ae2c943c7d",
      "metadata": {
        "id": "2d46f705-a1c2-4c59-ad0b-88ae2c943c7d"
      },
      "source": [
        "Random sample from X and y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4c1a3a7-3751-4649-9d85-8e17255a4c03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4c1a3a7-3751-4649-9d85-8e17255a4c03",
        "outputId": "511e099b-40f0-468f-b105-147a4dccc998"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 1, 0, ..., 0, 0, 0],\n",
              "       [0, 1, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 1, 0, ..., 0, 0, 0],\n",
              "       [0, 1, 0, ..., 0, 0, 0],\n",
              "       [0, 1, 0, ..., 0, 0, 0]], dtype=int8)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "X1[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32f82f91-4bf2-4f29-8e12-92ba7196ea07",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32f82f91-4bf2-4f29-8e12-92ba7196ea07",
        "outputId": "44ed8f81-edf0-49b7-b859-c4547fa0ab21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0], dtype=int8)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "X2[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "784bb474-f719-45fe-8836-beaef80dc222",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "784bb474-f719-45fe-8836-beaef80dc222",
        "outputId": "91f6b982-827e-40c5-c936-a88a22838d96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.277"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "y[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01e465cd-e5c3-41e1-889c-f82254879828",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01e465cd-e5c3-41e1-889c-f82254879828",
        "outputId": "cf30b4d7-eb14-4c92-e8da-e658573313c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(214853, 214853, 214853)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# count of features and labels\n",
        "len(X1), len(X2), len(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea0459c2-c83d-4920-a3d0-8c86f348a235",
      "metadata": {
        "id": "ea0459c2-c83d-4920-a3d0-8c86f348a235"
      },
      "source": [
        "### Baseline PTR\n",
        "\n",
        "There is no common sense approach in finding a baseline for the protein-to-mRNA ratio of a particular mRNA sequence. This is what the *Basic Neural Network* approach is for - to determin a baseline and see if a slightly adapted neural network with feature engineered input can provide better predictions.\n",
        "\n",
        "But what can be done is to simply check the value range of the target PTRs, calculate mean and standard deviation. Given that the standard deviation is  small (12.5% of the value range) one can (stupidly) predict the mean value every time. From that it's possible to calculate the Mean Absolute Error (MAE) and compare that to the following neural network output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "941624fc-a917-4983-bfdd-d58fb88f0a52",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "941624fc-a917-4983-bfdd-d58fb88f0a52",
        "outputId": "c5335620-469d-49df-9584-4fca3f2e964a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9085 9.974 4.9798956 0.8879484\n"
          ]
        }
      ],
      "source": [
        "# get some idea of the range of the PTR in the selected SAMPLE\n",
        "print(np.min(y), np.max(y), np.mean(y), np.std(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28d58d39-da1d-4f25-8f43-770dd077ae7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28d58d39-da1d-4f25-8f43-770dd077ae7b",
        "outputId": "1ebd0b1f-10a8-4148-998a-252bf06d2461"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7119917"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# simple/dumb baseline mean absolute error of always predicting 4.974\n",
        "mae = np.mean(np.abs(np.array(y) - 4.974))\n",
        "mae"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81105338-e17c-4c50-b9c8-19e022c857eb",
      "metadata": {
        "id": "81105338-e17c-4c50-b9c8-19e022c857eb"
      },
      "source": [
        "### Splits\n",
        "\n",
        "Split data in train and test sub sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f3bec3f9-c83c-4690-b56c-ae3b97b877d4",
      "metadata": {
        "id": "f3bec3f9-c83c-4690-b56c-ae3b97b877d4"
      },
      "outputs": [],
      "source": [
        "# split in train and test sub sets\n",
        "X1_train_full, X1_test, X2_train_full, X2_test, y_train_full, y_test = train_test_split(X1, X2, y, test_size=0.2, random_state=1202)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "f4205acc-8876-4855-9cc9-8b5948dd15ba",
      "metadata": {
        "id": "f4205acc-8876-4855-9cc9-8b5948dd15ba"
      },
      "outputs": [],
      "source": [
        "# split the training set again in train and validation\n",
        "X1_train, X1_val, X2_train, X2_val, y_train, y_val = train_test_split(X1_train_full, X2_train_full, y_train_full, test_size=0.2, random_state=1202)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "582618a5-4e35-4998-acdf-84feff4d2f74",
      "metadata": {
        "id": "582618a5-4e35-4998-acdf-84feff4d2f74"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "# batch_size = 8 # reduced batch size due to memory problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "9f34c6b2-ff64-4861-918a-34c83e3a51ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f34c6b2-ff64-4861-918a-34c83e3a51ae",
        "outputId": "970be603-3573-4e87-cc52-80c1c878248a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4.98 s, sys: 1.5 s, total: 6.48 s\n",
            "Wall time: 5.25 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# build the test dataset\n",
        "test_ds = create_dataset(X1_test, X2_test, y_test, batch_size=batch_size, sort=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "fa335eb5-5d74-4c78-b192-414edc015645",
      "metadata": {
        "id": "fa335eb5-5d74-4c78-b192-414edc015645",
        "outputId": "31815c39-acfa-47f6-da7c-1120b59a33f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 22.3 s, sys: 5.72 s, total: 28 s\n",
            "Wall time: 22.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# build the training dataset\n",
        "train_full_ds = create_dataset(X1_train_full, X2_train_full, y_train_full, batch_size=batch_size, sort=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "6e139b14-a23f-4899-b85e-5fe7d80c49fb",
      "metadata": {
        "id": "6e139b14-a23f-4899-b85e-5fe7d80c49fb",
        "outputId": "b0defbea-1f1d-40bc-d5fe-cf2098dfb616",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 17.6 s, sys: 2.97 s, total: 20.6 s\n",
            "Wall time: 15.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# build the training dataset\n",
        "train_ds = create_dataset(X1_train, X2_train, y_train, batch_size=batch_size, sort=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "783f52f5-022b-4991-8337-c5fd4062b4db",
      "metadata": {
        "id": "783f52f5-022b-4991-8337-c5fd4062b4db",
        "outputId": "6169b6af-0030-45f6-bd4d-d5cb6d64013a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4.52 s, sys: 811 ms, total: 5.34 s\n",
            "Wall time: 4.32 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# build the validation dataset\n",
        "val_ds = create_dataset(X1_val, X2_val, y_val, batch_size=batch_size, sort=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85ac31aa-4582-45df-957c-d7aa0bb79449",
      "metadata": {
        "id": "85ac31aa-4582-45df-957c-d7aa0bb79449"
      },
      "outputs": [],
      "source": [
        "# NOT doing that any more since the speed up of the ragged tensor creation\n",
        "\n",
        "# save the previously generated dataset\n",
        "# since the ragged tensor creation takes a very, very long time\n",
        "# test_ds.save('../data/test_ds.tf.dataset')\n",
        "# train_full_ds.save('../data/train_full_ds.tf.dataset')\n",
        "# train_ds.save('../data/train_ds.tf.dataset')\n",
        "# val_ds.save('../data/val_ds.tf.dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16486337-06cf-4678-8a96-bde976a9df03",
      "metadata": {
        "id": "16486337-06cf-4678-8a96-bde976a9df03"
      },
      "source": [
        "## Full Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9840923-0fd8-4d3e-9472-bc6753de99a9",
      "metadata": {
        "id": "a9840923-0fd8-4d3e-9472-bc6753de99a9",
        "outputId": "dfb33b82-1de7-4273-a1f3-28ce9e7f946a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training model for 60\n",
            "Model: \"Test\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " inputs_1 (InputLayer)       [(None, None, 10)]           0         []                            \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)           (None, None, 424)            127624    ['inputs_1[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling1d_6 (MaxPoolin  (None, None, 424)            0         ['conv1d_6[0][0]']            \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " spatial_dropout1d_6 (Spati  (None, None, 424)            0         ['max_pooling1d_6[0][0]']     \n",
            " alDropout1D)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)           (None, None, 212)            1348532   ['spatial_dropout1d_6[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling1d_7 (MaxPoolin  (None, None, 212)            0         ['conv1d_7[0][0]']            \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " spatial_dropout1d_7 (Spati  (None, None, 212)            0         ['max_pooling1d_7[0][0]']     \n",
            " alDropout1D)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)           (None, None, 212)            674372    ['spatial_dropout1d_7[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling1d_8 (MaxPoolin  (None, None, 212)            0         ['conv1d_8[0][0]']            \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " bidirectional_2 (Bidirecti  (None, None, 512)            2969600   ['max_pooling1d_8[0][0]']     \n",
            " onal)                                                                                            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_2 (Gl  (None, 512)                  0         ['bidirectional_2[0][0]']     \n",
            " obalMaxPooling1D)                                                                                \n",
            "                                                                                                  \n",
            " inputs_2 (InputLayer)       [(None, 29)]                 0         []                            \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 541)                  0         ['global_max_pooling1d_2[0][0]\n",
            " )                                                                  ',                            \n",
            "                                                                     'inputs_2[0][0]']            \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 128)                  69376     ['concatenate_2[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)         (None, 128)                  0         ['dense_6[0][0]']             \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 64)                   8256      ['dropout_4[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)         (None, 64)                   0         ['dense_7[0][0]']             \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 1)                    65        ['dropout_5[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5197825 (19.83 MB)\n",
            "Trainable params: 5197825 (19.83 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/60\n",
            "2149/2149 [==============================] - 9916s 5s/step - loss: 1.4568 - mean_absolute_error: 0.9548 - val_loss: 3.0525 - val_mean_absolute_error: 1.5443\n",
            "Epoch 2/60\n",
            "2149/2149 [==============================] - 4156s 2s/step - loss: 1.1862 - mean_absolute_error: 0.8705 - val_loss: 2.7399 - val_mean_absolute_error: 1.4497\n",
            "Epoch 3/60\n",
            "2149/2149 [==============================] - 4159s 2s/step - loss: 1.0728 - mean_absolute_error: 0.8285 - val_loss: 2.1771 - val_mean_absolute_error: 1.2680\n",
            "Epoch 4/60\n",
            "2149/2149 [==============================] - 4166s 2s/step - loss: 0.9767 - mean_absolute_error: 0.7898 - val_loss: 1.0566 - val_mean_absolute_error: 0.8439\n",
            "Epoch 5/60\n",
            "2149/2149 [==============================] - 4163s 2s/step - loss: 0.8868 - mean_absolute_error: 0.7536 - val_loss: 0.8558 - val_mean_absolute_error: 0.7505\n",
            "Epoch 6/60\n",
            "2149/2149 [==============================] - 4164s 2s/step - loss: 0.8651 - mean_absolute_error: 0.7454 - val_loss: 0.8519 - val_mean_absolute_error: 0.7486\n",
            "Epoch 7/60\n",
            "2149/2149 [==============================] - 4159s 2s/step - loss: 0.8518 - mean_absolute_error: 0.7386 - val_loss: 0.8573 - val_mean_absolute_error: 0.7513\n",
            "Epoch 8/60\n",
            "1312/2149 [=================>............] - ETA: 15:08 - loss: 0.8532 - mean_absolute_error: 0.7406"
          ]
        }
      ],
      "source": [
        "# epochs to run for\n",
        "epochs = 60\n",
        "\n",
        "# timer\n",
        "t = Timer()\n",
        "\n",
        "# current model\n",
        "model = augur_model_v10()\n",
        "# fit the model, validate and plot results\n",
        "print(f'training model for {epochs}')\n",
        "t.start()\n",
        "mae, val_mae = run_model(model, train_ds, val_ds, epochs=epochs, start_epoch=1, oneshot=True, verbose=True)\n",
        "t.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f83b9275-b52b-4b61-a1e0-07f4ba8d8e0a",
      "metadata": {
        "id": "f83b9275-b52b-4b61-a1e0-07f4ba8d8e0a"
      },
      "outputs": [],
      "source": [
        "augur_mae = mae\n",
        "augur_val_mae = val_mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d9ecdfb-61a4-4dd0-9089-15022d0b72c7",
      "metadata": {
        "id": "9d9ecdfb-61a4-4dd0-9089-15022d0b72c7",
        "outputId": "2c4a5e31-1a09-46df-ebd6-1e7728738aa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training final model model_full_augur.keras for 37 epochs\n",
            "  building model\n",
            "  training model\n",
            "    elapsed time: 66046.5175 seconds\n",
            "  evaluating model\n",
            "672/672 [==============================] - 100s 149ms/step - loss: 3.1555 - mean_absolute_error: 1.6659\n",
            "  mean absolute evaluation error is [3.1555373668670654, 1.6658642292022705]\n",
            "  saving model\n"
          ]
        }
      ],
      "source": [
        "# train final model\n",
        "\n",
        "# model file name\n",
        "model_name = nb_path + 'model_full_augur_v10_colab.keras'\n",
        "\n",
        "# timer\n",
        "t = Timer()\n",
        "\n",
        "# number of epochs to train for\n",
        "epochs = 37\n",
        "\n",
        "print(f'training final model {model_name} for {epochs} epochs')\n",
        "\n",
        "print('  building model')\n",
        "model = augur_model_v10()\n",
        "\n",
        "print('  training model')\n",
        "t.start()\n",
        "run_model(model, train_full_ds, None, epochs=epochs, start_epoch=1, oneshot=False, verbose=False)\n",
        "t.stop()\n",
        "\n",
        "print('  evaluating model')\n",
        "mae = model.evaluate(test_ds)\n",
        "print(f'  mean absolute evaluation error is {mae}')\n",
        "\n",
        "print('  saving model')\n",
        "model.save(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96105e2c-933d-405b-98aa-6576e6e2c792",
      "metadata": {
        "id": "96105e2c-933d-405b-98aa-6576e6e2c792",
        "outputId": "1d15cbf8-c2fb-40f2-81a4-89fc0e72370a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training model for 40\n",
            "Model: \"FullModel\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " inputs_1 (InputLayer)       [(None, None, 10)]           0         []                            \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)             (None, None, 280)            33880     ['inputs_1[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, None, 280)            1120      ['conv1d[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1  (None, None, 280)            0         ['batch_normalization[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, None, 280)            0         ['max_pooling1d[0][0]']       \n",
            "                                                                                                  \n",
            " bidirectional (Bidirection  (None, None, 896)            2612736   ['dropout[0][0]']             \n",
            " al)                                                                                              \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPoolin  (None, None, 896)            0         ['bidirectional[0][0]']       \n",
            " g1D)                                                                                             \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, None, 896)            0         ['max_pooling1d_1[0][0]']     \n",
            "                                                                                                  \n",
            " global_max_pooling1d (Glob  (None, 896)                  0         ['dropout_1[0][0]']           \n",
            " alMaxPooling1D)                                                                                  \n",
            "                                                                                                  \n",
            " inputs_2 (InputLayer)       [(None, 29)]                 0         []                            \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 925)                  0         ['global_max_pooling1d[0][0]',\n",
            "                                                                     'inputs_2[0][0]']            \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 128)                  118528    ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, 128)                  512       ['dense[0][0]']               \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 128)                  0         ['batch_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 1)                    129       ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2766905 (10.55 MB)\n",
            "Trainable params: 2766089 (10.55 MB)\n",
            "Non-trainable params: 816 (3.19 KB)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/40\n",
            "14626/17189 [========================>.....] - ETA: 4:11:50 - loss: 1.1661 - mean_absolute_error: 0.8229"
          ]
        }
      ],
      "source": [
        "# epochs to run for\n",
        "epochs = 40\n",
        "\n",
        "# timer\n",
        "t = Timer()\n",
        "\n",
        "# current model\n",
        "model = dan_zrimec_model()\n",
        "# fit the model, validate and plot results\n",
        "print(f'training model for {epochs}')\n",
        "t.start()\n",
        "mae, val_mae = run_model(model, train_ds, val_ds, epochs=epochs, start_epoch=1, oneshot=True, verbose=True)\n",
        "t.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16378ed5-1e16-4ee2-a687-c461ec5474b1",
      "metadata": {
        "id": "16378ed5-1e16-4ee2-a687-c461ec5474b1"
      },
      "outputs": [],
      "source": [
        "dz_mae = mae\n",
        "dz_val_mae = val_mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3241db9d-874d-4404-bd59-087ab4edda50",
      "metadata": {
        "id": "3241db9d-874d-4404-bd59-087ab4edda50"
      },
      "outputs": [],
      "source": [
        "# train final model\n",
        "\n",
        "# model file name\n",
        "model_name = nb_path + 'model_full_dz_colab.keras'\n",
        "\n",
        "# timer\n",
        "t = Timer()\n",
        "\n",
        "# number of epochs to train for\n",
        "epochs = 37\n",
        "\n",
        "print(f'training final model {model_name} for {epochs} epochs')\n",
        "\n",
        "print('  building model')\n",
        "model = dan_zrimec_model()\n",
        "\n",
        "print('  training model')\n",
        "t.start()\n",
        "run_model(model, train_full_ds, None, epochs=epochs, start_epoch=1, oneshot=False, verbose=False)\n",
        "t.stop()\n",
        "\n",
        "print('  evaluating model')\n",
        "mae = model.evaluate(test_ds)\n",
        "print(f'  mean absolute evaluation error is {mae}')\n",
        "\n",
        "print('  saving model')\n",
        "model.save(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd15effe-0d99-4c4b-ab7d-261f42efa608",
      "metadata": {
        "id": "dd15effe-0d99-4c4b-ab7d-261f42efa608"
      },
      "source": [
        "## k-Fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b66d001b-11b1-4d6a-9dc0-90dc7a290af5",
      "metadata": {
        "id": "b66d001b-11b1-4d6a-9dc0-90dc7a290af5"
      },
      "outputs": [],
      "source": [
        "num_splits = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31421c42-2e54-40a6-be5e-720f33e4dd3d",
      "metadata": {
        "id": "31421c42-2e54-40a6-be5e-720f33e4dd3d"
      },
      "source": [
        "### Augur Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e139b274-5025-4118-a01f-2fb56657f7d5",
      "metadata": {
        "id": "e139b274-5025-4118-a01f-2fb56657f7d5"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# run k-fold cross validation\n",
        "\n",
        "# epochs to run for\n",
        "epochs = 100\n",
        "\n",
        "# timer\n",
        "t = Timer()\n",
        "\n",
        "# training and validation mean absolute error results\n",
        "train_mae = []\n",
        "val_mae = []\n",
        "\n",
        "print(f'k-fold cross validation with {num_splits} splits for {epochs} epochs')\n",
        "kf = KFold(n_splits=num_splits, shuffle=True, random_state=1202)\n",
        "for i, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
        "    print(f'  processing fold {i}')\n",
        "\n",
        "    # split the data\n",
        "    print('    splitting data')\n",
        "    t.start()\n",
        "    X_train_kf, X_val_kf, y_train_kf, y_val_kf = X_train[train_index], X_train[val_index], y_train[train_index], y_train[val_index]\n",
        "    t.stop()\n",
        "\n",
        "    # build the datasets\n",
        "    print('    creating training dataset')\n",
        "    t.start()\n",
        "    train_ds = create_dataset(X_train_kf, y_train_kf, batch_size=batch_size, sort=True)\n",
        "    t.stop()\n",
        "    print('    creating validation dataset')\n",
        "    t.start()\n",
        "    val_ds = create_dataset(X_val_kf, y_val_kf, batch_size=batch_size, sort=False)\n",
        "    t.stop()\n",
        "\n",
        "    # fit the model and return mae\n",
        "    print('    fitting model')\n",
        "    t.start()\n",
        "    t_mae, v_mae = run_model(augur_model(), train_ds, val_ds, epochs=epochs, start_epoch=1, oneshot=False, verbose=False)\n",
        "    t.stop()\n",
        "\n",
        "    # add returned mae values to the arrays\n",
        "    train_mae.append(t_mae)\n",
        "    val_mae.append(v_mae)\n",
        "\n",
        "# calculate the average\n",
        "average_train_mae = [ np.mean([ x[i] for x in train_mae ]) for i in range(epochs) ]\n",
        "average_val_mae = [ np.mean([ x[i] for x in val_mae ]) for i in range(epochs) ]\n",
        "\n",
        "# plot\n",
        "plot_loss(average_train_mae, average_val_mae, start_epoch=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c833d59-8d0b-4c09-8b27-32841e29e43d",
      "metadata": {
        "id": "7c833d59-8d0b-4c09-8b27-32841e29e43d"
      },
      "outputs": [],
      "source": [
        "# train final model\n",
        "\n",
        "# model file name\n",
        "model_name = 'model_augur.keras'\n",
        "\n",
        "# timer\n",
        "t = Timer()\n",
        "\n",
        "# training and validation mean absolute error results\n",
        "train_mae = []\n",
        "val_mae = []\n",
        "\n",
        "# number of epochs to train for\n",
        "epochs = 40\n",
        "\n",
        "print(f'training final model {model_name} for {epochs} epochs')\n",
        "\n",
        "print('  creating training dataset')\n",
        "t.start()\n",
        "train_ds = create_dataset(X_train, y_train, batch_size=batch_size, sort=True)\n",
        "t.stop()\n",
        "\n",
        "print('  building model')\n",
        "model = augur_model()\n",
        "\n",
        "print('  training model')\n",
        "t.start()\n",
        "run_model(model, train_ds, None, epochs=epochs, start_epoch=1, oneshot=False, verbose=False)\n",
        "t.stop()\n",
        "\n",
        "print('  evaluating model')\n",
        "mae = model.evaluate(test_ds)\n",
        "print(f'  mean absolute evaluation error is {mae}')\n",
        "\n",
        "print('  saving model')\n",
        "model.save(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e83109b-8577-4591-945e-c2d08491d1a8",
      "metadata": {
        "id": "5e83109b-8577-4591-945e-c2d08491d1a8"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# run k-fold cross validation\n",
        "\n",
        "# epochs to run for\n",
        "epochs = 100\n",
        "\n",
        "# timer\n",
        "t = Timer()\n",
        "\n",
        "# training and validation mean absolute error results\n",
        "train_mae = []\n",
        "val_mae = []\n",
        "\n",
        "print(f'k-fold cross validation with {num_splits} splits for {epochs} epochs')\n",
        "kf = KFold(n_splits=num_splits, shuffle=True, random_state=1202)\n",
        "for i, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
        "    print(f'  processing fold {i}')\n",
        "\n",
        "    # split the data\n",
        "    print('    splitting data')\n",
        "    t.start()\n",
        "    X_train_kf, X_val_kf, y_train_kf, y_val_kf = X_train[train_index], X_train[val_index], y_train[train_index], y_train[val_index]\n",
        "    t.stop()\n",
        "\n",
        "    # build the datasets\n",
        "    print('    creating training dataset')\n",
        "    t.start()\n",
        "    train_ds = create_dataset(X_train_kf, y_train_kf, batch_size=batch_size, sort=True)\n",
        "    t.stop()\n",
        "    print('    creating validation dataset')\n",
        "    t.start()\n",
        "    val_ds = create_dataset(X_val_kf, y_val_kf, batch_size=batch_size, sort=False)\n",
        "    t.stop()\n",
        "\n",
        "    # fit the model and return mae\n",
        "    print('    fitting model')\n",
        "    t.start()\n",
        "    t_mae, v_mae = run_model(dan_zrimec_model(), train_ds, val_ds, epochs=epochs, start_epoch=1, oneshot=False, verbose=False)\n",
        "    t.stop()\n",
        "\n",
        "    # add returned mae values to the arrays\n",
        "    train_mae.append(t_mae)\n",
        "    val_mae.append(v_mae)\n",
        "\n",
        "# calculate the average\n",
        "average_train_mae = [ np.mean([ x[i] for x in train_mae ]) for i in range(epochs) ]\n",
        "average_val_mae = [ np.mean([ x[i] for x in val_mae ]) for i in range(epochs) ]\n",
        "\n",
        "# plot\n",
        "plot_loss(average_train_mae, average_val_mae, start_epoch=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77c94735-fe6c-46bc-815d-ef0a7f6def94",
      "metadata": {
        "id": "77c94735-fe6c-46bc-815d-ef0a7f6def94"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# run k-fold cross validation\n",
        "\n",
        "# epochs to run for\n",
        "epochs = 100\n",
        "\n",
        "# timer\n",
        "t = Timer()\n",
        "\n",
        "# training and validation mean absolute error results\n",
        "train_mae = []\n",
        "val_mae = []\n",
        "\n",
        "print(f'k-fold cross validation with {num_splits} splits for {epochs} epochs')\n",
        "kf = KFold(n_splits=num_splits, shuffle=True, random_state=1202)\n",
        "for i, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
        "    print(f'  processing fold {i}')\n",
        "\n",
        "    # split the data\n",
        "    print('    splitting data')\n",
        "    t.start()\n",
        "    X_train_kf, X_val_kf, y_train_kf, y_val_kf = X_train[train_index], X_train[val_index], y_train[train_index], y_train[val_index]\n",
        "    t.stop()\n",
        "\n",
        "    # build the datasets\n",
        "    print('    creating training dataset')\n",
        "    t.start()\n",
        "    train_ds = create_dataset(X_train_kf, y_train_kf, batch_size=batch_size, sort=True)\n",
        "    t.stop()\n",
        "    print('    creating validation dataset')\n",
        "    t.start()\n",
        "    val_ds = create_dataset(X_val_kf, y_val_kf, batch_size=batch_size, sort=False)\n",
        "    t.stop()\n",
        "\n",
        "    # fit the model and return mae\n",
        "    print('    fitting model')\n",
        "    t.start()\n",
        "    t_mae, v_mae = run_model(dan_zrimec_model2(), train_ds, val_ds, epochs=epochs, start_epoch=1, oneshot=False, verbose=False)\n",
        "    t.stop()\n",
        "\n",
        "    # add returned mae values to the arrays\n",
        "    train_mae.append(t_mae)\n",
        "    val_mae.append(v_mae)\n",
        "\n",
        "# calculate the average\n",
        "average_train_mae = [ np.mean([ x[i] for x in train_mae ]) for i in range(epochs) ]\n",
        "average_val_mae = [ np.mean([ x[i] for x in val_mae ]) for i in range(epochs) ]\n",
        "\n",
        "# plot\n",
        "plot_loss(average_train_mae, average_val_mae, start_epoch=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff8053c0-8c5f-4da9-9217-d8bb8a1433b6",
      "metadata": {
        "id": "ff8053c0-8c5f-4da9-9217-d8bb8a1433b6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "181cfeca-7bde-4da8-aa66-3e4379026cd5",
      "metadata": {
        "id": "181cfeca-7bde-4da8-aa66-3e4379026cd5"
      },
      "source": [
        "# what are the steps?\n",
        "\n",
        "- create ragged tensor from X_test\n",
        "- create dataset from X_test and y_test\n",
        "\n",
        "k-fold cross validation for the baseline neural network\n",
        "- input is X_train, y_train\n",
        "- using KFold this will be split in k folds\n",
        "  - sort the trainin part\n",
        "  - convert X to ragged tensor\n",
        "  - create dataset\n",
        "  - train\n",
        "  - evaluate\n",
        "- evaluate the cross fold performance\n",
        "- rerun model with the whole dataset (test+val)\n",
        "- save the model\n",
        "```\n",
        "kf = KFold(n_splits=num_splits, shuffle=True, random_state=1202)\n",
        "for i, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
        "    print(f\"Fold {i}:\")\n",
        "    X_train_kf, X_test_kf, y_train_kf, y_test_kf = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
        "    # sort X_train_kf and y_train_kf\n",
        "    # convert X_train_kf to ragged tensor\n",
        "    # create dataset from X_train_kf and y_train_kf\n",
        "    # do the training and evaluation\n",
        "# evaluate the cross fold performance\n",
        "```\n",
        "\n",
        "train/val split for the full neural network -> separate jupyter notebook\n",
        "- input X_train, y_train\n",
        "- split the training test set again in train and val\n",
        "- create ragged tensor from training X\n",
        "- create dataset from training X and y\n",
        "- train\n",
        "- evaluate\n",
        "- rerun the model with the whole dataset (test+val)\n",
        "- save the model\n",
        "\n",
        "\n",
        "what have both in common?\n",
        "- data load and initial prep (up to the first split)\n",
        "- model setup\n",
        "- training setup\n",
        "- visualisation\n",
        "- evaluation -> this is just one command\n",
        "\n",
        "helpful methods\n",
        "- create_dataset(X, y, sort=False)\n",
        "  input X and y, specify if the dataset should be sorted\n",
        "  returns a TF dataset\n",
        "- run_model(model, epochs, plot=True, plot_epoch_start=0)\n",
        "- plot_loss()\n",
        "\n",
        "- callbacks\n",
        "    - earlystopping -> to limit training that doesn't progress, only for the tuner\n",
        "    - backupandrestore -> for the full training as fault tolerance setup\n",
        "    - modelcheckpoint -> to save the best model on the final train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcbc7e7c-4981-4ce5-9435-b20fea7cd399",
      "metadata": {
        "id": "bcbc7e7c-4981-4ce5-9435-b20fea7cd399"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45b8d99d-4c8c-4af2-b9ce-77df859362a3",
      "metadata": {
        "id": "45b8d99d-4c8c-4af2-b9ce-77df859362a3"
      },
      "outputs": [],
      "source": [
        "# split the train set again in train and validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1202)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c86b943-3471-42b6-bef0-e60a0e376568",
      "metadata": {
        "id": "6c86b943-3471-42b6-bef0-e60a0e376568"
      },
      "outputs": [],
      "source": [
        "# find the first unique PTR value that is also in y_train\n",
        "train_idx = 0\n",
        "for i in range(len(y)):\n",
        "    count = 0\n",
        "    for l in range(len(y)):\n",
        "        if i != l and y[i] == y[l]:\n",
        "            count += 1\n",
        "            continue\n",
        "    if count == 0:\n",
        "        for m in range(len(y_train)):\n",
        "            if y[i] == y_train[m]:\n",
        "                train_idx = m\n",
        "                break\n",
        "train_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "387f1c55-480e-459b-b295-ddc038493283",
      "metadata": {
        "id": "387f1c55-480e-459b-b295-ddc038493283"
      },
      "outputs": [],
      "source": [
        "# get a sample\n",
        "X_train[train_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82384079-68d4-4ca6-9301-b856d56aeb4d",
      "metadata": {
        "id": "82384079-68d4-4ca6-9301-b856d56aeb4d"
      },
      "outputs": [],
      "source": [
        "# get the matching target\n",
        "search_y = y_train[train_idx]\n",
        "search_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "737386f4-6bdc-446b-8f31-a910f9f015c7",
      "metadata": {
        "id": "737386f4-6bdc-446b-8f31-a910f9f015c7"
      },
      "outputs": [],
      "source": [
        "# find the target value in the raw dataset\n",
        "full_idx = 0\n",
        "for i in range(len(y)):\n",
        "    if y[i] == search_y:\n",
        "        print(i)\n",
        "        full_idx = i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29af14a0-e1a3-45ab-8d0b-56dc0020917b",
      "metadata": {
        "id": "29af14a0-e1a3-45ab-8d0b-56dc0020917b"
      },
      "outputs": [],
      "source": [
        "# compare if the raw dataset entry matches the subset entry\n",
        "if X[full_idx].all() == X_train[train_idx].all():\n",
        "    print('OK')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4f525a1-3fef-4b27-b7ee-eb136ff06ebc",
      "metadata": {
        "id": "f4f525a1-3fef-4b27-b7ee-eb136ff06ebc"
      },
      "source": [
        "### Sort Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94fc569c-9d02-4619-82bf-ebf4e4caaf49",
      "metadata": {
        "id": "94fc569c-9d02-4619-82bf-ebf4e4caaf49"
      },
      "outputs": [],
      "source": [
        "# build an inhomogenous numpy array from the training set\n",
        "X_train = np.array(X_train, dtype=object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbc1d8b7-225a-43b4-a3a7-344569269d8a",
      "metadata": {
        "id": "bbc1d8b7-225a-43b4-a3a7-344569269d8a"
      },
      "outputs": [],
      "source": [
        "# build an array containing the sequence lengths\n",
        "sequence_lengths = list(map(lambda x: len(x), X_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5360be4d-b70c-4fe6-9515-a0b904e0fa3d",
      "metadata": {
        "id": "5360be4d-b70c-4fe6-9515-a0b904e0fa3d"
      },
      "outputs": [],
      "source": [
        "# sort the array but only get the indices\n",
        "sorted_indices = np.argsort(sequence_lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0200e456-a746-417d-bc9c-ca4f75fa973b",
      "metadata": {
        "id": "0200e456-a746-417d-bc9c-ca4f75fa973b"
      },
      "outputs": [],
      "source": [
        "sorted_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75abd9cc-7c09-40b0-b5a0-619bfc2e5a6b",
      "metadata": {
        "id": "75abd9cc-7c09-40b0-b5a0-619bfc2e5a6b"
      },
      "outputs": [],
      "source": [
        "# now sort the X and y train arrays according to the sorted indicds\n",
        "X_train = X_train[sorted_indices]\n",
        "y_train = y_train[sorted_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9070a40-d333-4008-9a8b-edb277a5209c",
      "metadata": {
        "id": "d9070a40-d333-4008-9a8b-edb277a5209c"
      },
      "outputs": [],
      "source": [
        "# check if the previously found values still correlate\n",
        "for i in range(len(y_train)):\n",
        "    if y_train[i] == search_y:\n",
        "        print(X_train[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48e10faa-ea20-42bb-bd97-75395da099c9",
      "metadata": {
        "id": "48e10faa-ea20-42bb-bd97-75395da099c9"
      },
      "source": [
        "### Ragged Tensor Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b86fae6d-0aa3-4bb5-97ec-e2e8920e1b23",
      "metadata": {
        "id": "b86fae6d-0aa3-4bb5-97ec-e2e8920e1b23"
      },
      "outputs": [],
      "source": [
        "# this does not work since the sequences are of different length\n",
        "# X_test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d39e886-2279-4bbb-a838-c44e26ba3ef6",
      "metadata": {
        "id": "3d39e886-2279-4bbb-a838-c44e26ba3ef6"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "X_train_tensor = tf.ragged.constant(X_train, dtype=tf.int8, ragged_rank=1, row_splits_dtype=tf.int32)\n",
        "X_val_tensor = tf.ragged.constant(X_val, dtype=tf.int8, ragged_rank=1, row_splits_dtype=tf.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "850a17ad-bb52-4365-9424-3447b37529ab",
      "metadata": {
        "id": "850a17ad-bb52-4365-9424-3447b37529ab"
      },
      "outputs": [],
      "source": [
        "X_train_dataset = tf.data.Dataset.from_tensor_slices((X_train_tensor, y_train))\n",
        "X_val_dataset = tf.data.Dataset.from_tensor_slices((X_val_tensor, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c506f768-6e15-4c6f-872b-0038d177dd57",
      "metadata": {
        "id": "c506f768-6e15-4c6f-872b-0038d177dd57"
      },
      "outputs": [],
      "source": [
        "X_train_dataset.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "907d3def-dbf3-4830-aad6-5368d664e235",
      "metadata": {
        "id": "907d3def-dbf3-4830-aad6-5368d664e235"
      },
      "outputs": [],
      "source": [
        "X_val_dataset.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a705eb41-faff-44f3-b262-88b883ec013a",
      "metadata": {
        "id": "a705eb41-faff-44f3-b262-88b883ec013a"
      },
      "outputs": [],
      "source": [
        "# padded batches from ragged tensors are not supported (yet)\n",
        "# it needs a work around creating a uniform tensor\n",
        "# idea from : https://github.com/tensorflow/tensorflow/issues/39163\n",
        "def reformat(data, label):\n",
        "    return data, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "805f9521-15da-4791-9a6a-5cf3dca18a31",
      "metadata": {
        "id": "805f9521-15da-4791-9a6a-5cf3dca18a31"
      },
      "outputs": [],
      "source": [
        "X_train_dataset = X_train_dataset.map(reformat)\n",
        "X_val_dataset = X_val_dataset.map(reformat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e6e1c8a-efb9-455b-a3b3-cabc2d02daa3",
      "metadata": {
        "id": "8e6e1c8a-efb9-455b-a3b3-cabc2d02daa3"
      },
      "outputs": [],
      "source": [
        "# shuffle the dataset (again) and create padded batches\n",
        "batch_size = 64\n",
        "X_train_dataset = X_train_dataset.padded_batch(batch_size)\n",
        "X_val_dataset = X_val_dataset.shuffle(buffer_size=len(X_val), seed=1202).padded_batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b2fa149-8b00-4210-8a33-2a3b810cbd49",
      "metadata": {
        "id": "2b2fa149-8b00-4210-8a33-2a3b810cbd49"
      },
      "outputs": [],
      "source": [
        "# optinally repeat the dataset multiple times -> WHY?\n",
        "# rep = 3\n",
        "# X_train_dataset = X_train_dataset.repeat(rep)\n",
        "# X_val_dataset = X_val_dataset.repeat(rep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49244b0a-a161-457b-88d2-ba4417d89110",
      "metadata": {
        "id": "49244b0a-a161-457b-88d2-ba4417d89110"
      },
      "outputs": [],
      "source": [
        "datalen = []\n",
        "ds_iterator = iter(X_train_dataset)\n",
        "for data, label in ds_iterator:\n",
        "    datalen.append(len(data[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fa0eb29-ea9c-4272-8cf9-682c9fe01451",
      "metadata": {
        "id": "6fa0eb29-ea9c-4272-8cf9-682c9fe01451"
      },
      "outputs": [],
      "source": [
        "datalen[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5ba5416-4ed2-4d5f-8123-5e67e46b4dd5",
      "metadata": {
        "id": "f5ba5416-4ed2-4d5f-8123-5e67e46b4dd5"
      },
      "outputs": [],
      "source": [
        "# testing if keras can use the dataset\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(16, activation='relu', input_shape=(None,4)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "model.fit(X_train_dataset, epochs=1)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}