{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ce73b25-d36d-413d-a323-67e9a246b21b",
   "metadata": {},
   "source": [
    "# Baseline Neural Network Setup using k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2874d9ad-1cd5-4f33-bcf6-178552b4ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library dependencies\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import lzma\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import keras_tuner\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b634de2-ee59-4979-ae29-9ce9ab8a2909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to store data as serialized binary structure lzma compressed\n",
    "def can_pickles(data, filename):\n",
    "    with lzma.LZMAFile(filename, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.DEFAULT_PROTOCOL)\n",
    "\n",
    "# method to retrieve data from a compressed pickle file (created with the method above)\n",
    "def uncan_pickles(filename):\n",
    "    with lzma.LZMAFile(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29c2a628-8756-4b25-844b-10352d9ce814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method to create a valid dataset\n",
    "# padded batches from ragged tensors are not supported (yet)\n",
    "# it needs a work around creating a uniform tensor\n",
    "# idea from : https://github.com/tensorflow/tensorflow/issues/39163\n",
    "def reformat(data, label):\n",
    "    return data, label\n",
    "\n",
    "# method to create a TF dataset\n",
    "def create_dataset(X_np_array, y_np_array, batch_size=32, sort=False):\n",
    "    # sort the arrays\n",
    "    if sort == True:\n",
    "        # build an array containing the sequence lengths\n",
    "        sequence_lengths = list(map(lambda x: len(x), X_np_array))\n",
    "        # sort the array but only get the indices\n",
    "        sorted_indices = np.argsort(sequence_lengths)\n",
    "        # now sort the X and y train arrays according to the sorted indicds\n",
    "        X_np_array = X_np_array[sorted_indices]\n",
    "        y_np_array = y_np_array[sorted_indices]\n",
    "\n",
    "    # create ragged tensor from in-homogeneous array\n",
    "    X_tensor = tf.ragged.constant(X_np_array, dtype=tf.int8, ragged_rank=1, row_splits_dtype=tf.int32)\n",
    "    \n",
    "    # create dataset\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X_tensor, y_np_array))\n",
    "\n",
    "    # create a dataset of dense tensors\n",
    "    ds = ds.map(reformat)\n",
    "\n",
    "    # apply padded batching to the dataset\n",
    "    ds = ds.padded_batch(batch_size)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13c3c1f-0eef-41c7-a4cb-8f1287335b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to plot two MAE arrays\n",
    "def plot_loss(train_mae, val_mae, start_epoch=1):\n",
    "    # get the number of epochs the training ran\n",
    "    epochs = range(start_epoch, len(train_mae) + 1)\n",
    "    # plot the graph\n",
    "    plt.plot(epochs, train_mae, \"bo\", label=\"Training\")\n",
    "    plt.plot(epochs, val_mae, \"b\", label=\"Validation\")\n",
    "    plt.title(\"Training and Validation Mean Absolute Error\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57ee08f6-464e-4492-9931-69e754ab2d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model to work with\n",
    "# from keras-tuner run:\n",
    "#   conv_units: 424\n",
    "#   kernel_size: 30\n",
    "#   rate: 0.30000000000000004\n",
    "#   dense_units: 128\n",
    "def augur_model():\n",
    "    inputs = layers.Input(shape=(None, 4))\n",
    "    conv1 = layers.Conv1D(\n",
    "        filters=424,\n",
    "        kernel_size=30,\n",
    "        strides=1,\n",
    "        activation='relu',\n",
    "        padding='valid'\n",
    "    )(inputs)\n",
    "    norm1 = layers.BatchNormalization()(conv1)\n",
    "    drop1 = layers.Dropout(\n",
    "        rate=0.3\n",
    "    )(norm1)\n",
    "    # pool1 = layers.MaxPooling1D(\n",
    "    #     pool_size=4,\n",
    "    #     strides=4\n",
    "    # )(drop1)\n",
    "    pool1 = layers.GlobalMaxPool1D()(drop1)\n",
    "    # flat = layers.Flatten()(drop1)\n",
    "    dense = layers.Dense(128, activation='relu')(pool1)\n",
    "    outputs = layers.Dense(1)(dense)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='Test')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a69600b6-8ab4-4080-8bce-64dbbb308151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, train_ds, val_ds, epochs=10, start_epoch=1, oneshot=True, verbose=True):\n",
    "    if verbose: \n",
    "        model.summary()\n",
    "        verbose_fit = 'auto'\n",
    "    else:\n",
    "        verbose_fit = 0\n",
    "    \n",
    "    model.compile(\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        metrics=[keras.metrics.MeanAbsoluteError()],\n",
    "    )\n",
    "        \n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_ds,\n",
    "        verbose=verbose_fit\n",
    "    )\n",
    "    \n",
    "    if oneshot == True:\n",
    "        plot_loss(\n",
    "            history.history['mean_absolute_error'],\n",
    "            history.history['val_mean_absolute_error'],\n",
    "            start_epoch\n",
    "        )\n",
    "    else:\n",
    "        return history.history['mean_absolute_error'], history.history['val_mean_absolute_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf21cd28-49ae-441a-9daa-8b0712296906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple timer from https://realpython.com/python-timer/\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self._start_time = None\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start a new timer\"\"\"\n",
    "        if self._start_time is not None:\n",
    "            print(f\"Timer is running. Use .stop() to stop it\")\n",
    "        else:\n",
    "            self._start_time = time.perf_counter()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer, and report the elapsed time\"\"\"\n",
    "        if self._start_time is None:\n",
    "            print(f\"Timer is not running. Use .start() to start it\")\n",
    "        else:\n",
    "            elapsed_time = time.perf_counter() - self._start_time\n",
    "            self._start_time = None\n",
    "            print(f\"    elapsed time: {elapsed_time:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf711275-0384-46df-9510-b7beaf2a2463",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "\n",
    "*explain it in more detail*\n",
    "\n",
    "X holds a list of sequences one hot encoded\n",
    "\n",
    "y holds a list of PTR values as floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "198c44f7-050c-4c96-a00b-0b4fcbf2c9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the prepared data back\n",
    "X = uncan_pickles('../data/onehot_x_lung.pickle.xz')\n",
    "y = uncan_pickles('../data/onehot_y_lung.pickle.xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a77d5bf-bbc1-4c38-8466-c2904242880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an inhomogenous numpy array from X\n",
    "X = np.array(X, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "668f2ec1-e6df-4287-b0ab-442001a822b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert type of target values from string to float\n",
    "y = np.array(y).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46f705-a1c2-4c59-ad0b-88ae2c943c7d",
   "metadata": {},
   "source": [
    "Random sample from X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4c1a3a7-3751-4649-9d85-8e17255a4c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "784bb474-f719-45fe-8836-beaef80dc222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.544"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01e465cd-e5c3-41e1-889c-f82254879828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8201"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of input sequences\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0459c2-c83d-4920-a3d0-8c86f348a235",
   "metadata": {},
   "source": [
    "### Baseline PTR\n",
    "\n",
    "There is no common sense approach in finding a baseline for the protein-to-mRNA ratio of a particular mRNA sequence. This is what the *Basic Neural Network* approach is for - to determin a baseline and see if a slightly adapted neural network with feature engineered input can provide better predictions.\n",
    "\n",
    "But what can be done is to simply check the value range of the target PTRs, calculate mean and standard deviation. Given that the standard deviation is  small (12.5% of the value range) one can (stupidly) predict the mean value every time. From that it's possible to calculate the Mean Absolute Error (MAE) and compare that to the following neural network output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "941624fc-a917-4983-bfdd-d58fb88f0a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.552 8.587 4.973957444214121 0.8835629329175175\n"
     ]
    }
   ],
   "source": [
    "# get some idea of the range of the PTR in the selected SAMPLE\n",
    "print(np.min(y), np.max(y), np.mean(y), np.std(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28d58d39-da1d-4f25-8f43-770dd077ae7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7055145713937325"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple/dumb baseline mean absolute error of always predicting 4.974\n",
    "mae = np.mean(np.abs(np.array(y) - 4.974))\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81105338-e17c-4c50-b9c8-19e022c857eb",
   "metadata": {},
   "source": [
    "### Splits\n",
    "\n",
    "Split data in train and test sub sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3bec3f9-c83c-4690-b56c-ae3b97b877d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train and test sub sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "582618a5-4e35-4998-acdf-84feff4d2f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f34c6b2-ff64-4861-918a-34c83e3a51ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the test dataset\n",
    "test_ds = create_dataset(X_test, y_test, batch_size=batch_size, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd15effe-0d99-4c4b-ab7d-261f42efa608",
   "metadata": {},
   "source": [
    "### k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b66d001b-11b1-4d6a-9dc0-90dc7a290af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 5\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e139b274-5025-4118-a01f-2fb56657f7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold cross validation with 5 splits for 100 epochs\n",
      "  processing fold 0\n",
      "    splitting data\n",
      "    elapsed time: 0.0001 seconds\n",
      "    creating training dataset\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:24\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# run k-fold cross validation\n",
    "# timer\n",
    "t = Timer()\n",
    "\n",
    "# training and validation mean absolute error results\n",
    "train_mae = []\n",
    "val_mae = []\n",
    "\n",
    "print(f'k-fold cross validation with {num_splits} splits for {epochs} epochs')\n",
    "kf = KFold(n_splits=num_splits, shuffle=True, random_state=1202)\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "    print(f'  processing fold {i}')\n",
    "\n",
    "    # split the data\n",
    "    print('    splitting data')\n",
    "    t.start()\n",
    "    X_train_kf, X_val_kf, y_train_kf, y_val_kf = X_train[train_index], X_train[val_index], y_train[train_index], y_train[val_index]\n",
    "    t.stop()\n",
    "\n",
    "    # build the datasets\n",
    "    print('    creating training dataset')\n",
    "    t.start()\n",
    "    train_ds = create_dataset(X_train_kf, y_train_kf, batch_size=batch_size, sort=True)\n",
    "    t.stop()\n",
    "    print('    creating validation dataset')\n",
    "    t.start()\n",
    "    val_ds = create_dataset(X_val_kf, y_val_kf, batch_size=batch_size, sort=False)\n",
    "    t.stop()\n",
    "\n",
    "    # fit the model and return mae\n",
    "    print('    fitting model')\n",
    "    t.start()\n",
    "    t_mae, v_mae = run_model(augur_model(), train_ds, val_ds, epochs=epochs, start_epoch=1, oneshot=False, verbose=False)\n",
    "    t.stop()\n",
    "\n",
    "    # add returned mae values to the arrays\n",
    "    train_mae.append(t_mae)\n",
    "    val_mae.append(v_mae)\n",
    "\n",
    "# calculate the average\n",
    "average_train_mae = [ np.mean([ x[i] for x in train_mae ]) for i in range(epochs) ]\n",
    "average_val_mae = [ np.mean([ x[i] for x in val_mae ]) for i in range(epochs) ]\n",
    "\n",
    "# plot\n",
    "plot_loss(average_train_mae, average_val_mae, start_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8053c0-8c5f-4da9-9217-d8bb8a1433b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "181cfeca-7bde-4da8-aa66-3e4379026cd5",
   "metadata": {},
   "source": [
    "# what are the steps?\n",
    "\n",
    "- create ragged tensor from X_test\n",
    "- create dataset from X_test and y_test\n",
    "\n",
    "k-fold cross validation for the baseline neural network\n",
    "- input is X_train, y_train\n",
    "- using KFold this will be split in k folds\n",
    "  - sort the trainin part\n",
    "  - convert X to ragged tensor\n",
    "  - create dataset\n",
    "  - train\n",
    "  - evaluate\n",
    "- evaluate the cross fold performance\n",
    "- rerun model with the whole dataset (test+val)\n",
    "- save the model\n",
    "```\n",
    "kf = KFold(n_splits=num_splits, shuffle=True, random_state=1202)\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    X_train_kf, X_test_kf, y_train_kf, y_test_kf = X_train[train_index], X_train[test_index], y_train[train_index], y_train[test_index]\n",
    "    # sort X_train_kf and y_train_kf\n",
    "    # convert X_train_kf to ragged tensor\n",
    "    # create dataset from X_train_kf and y_train_kf\n",
    "    # do the training and evaluation\n",
    "# evaluate the cross fold performance\n",
    "```\n",
    "\n",
    "train/val split for the full neural network -> separate jupyter notebook\n",
    "- input X_train, y_train\n",
    "- split the training test set again in train and val\n",
    "- create ragged tensor from training X\n",
    "- create dataset from training X and y\n",
    "- train\n",
    "- evaluate\n",
    "- rerun the model with the whole dataset (test+val)\n",
    "- save the model\n",
    "\n",
    "\n",
    "what have both in common?\n",
    "- data load and initial prep (up to the first split)\n",
    "- model setup\n",
    "- training setup\n",
    "- visualisation\n",
    "- evaluation -> this is just one command\n",
    "\n",
    "helpful methods\n",
    "- create_dataset(X, y, sort=False)\n",
    "  input X and y, specify if the dataset should be sorted\n",
    "  returns a TF dataset\n",
    "- run_model(model, epochs, plot=True, plot_epoch_start=0)\n",
    "- plot_loss()\n",
    "\n",
    "- callbacks\n",
    "    - earlystopping -> to limit training that doesn't progress, only for the tuner\n",
    "    - backupandrestore -> for the full training as fault tolerance setup\n",
    "    - modelcheckpoint -> to save the best model on the final train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc7e7c-4981-4ce5-9435-b20fea7cd399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b8d99d-4c8c-4af2-b9ce-77df859362a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the train set again in train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c86b943-3471-42b6-bef0-e60a0e376568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the first unique PTR value that is also in y_train\n",
    "train_idx = 0\n",
    "for i in range(len(y)):\n",
    "    count = 0\n",
    "    for l in range(len(y)):\n",
    "        if i != l and y[i] == y[l]:\n",
    "            count += 1\n",
    "            continue\n",
    "    if count == 0:\n",
    "        for m in range(len(y_train)):\n",
    "            if y[i] == y_train[m]:\n",
    "                train_idx = m\n",
    "                break\n",
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f1c55-480e-459b-b295-ddc038493283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample\n",
    "X_train[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82384079-68d4-4ca6-9301-b856d56aeb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the matching target\n",
    "search_y = y_train[train_idx]\n",
    "search_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737386f4-6bdc-446b-8f31-a910f9f015c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the target value in the raw dataset\n",
    "full_idx = 0\n",
    "for i in range(len(y)):\n",
    "    if y[i] == search_y:\n",
    "        print(i)\n",
    "        full_idx = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29af14a0-e1a3-45ab-8d0b-56dc0020917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare if the raw dataset entry matches the subset entry\n",
    "if X[full_idx].all() == X_train[train_idx].all():\n",
    "    print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f525a1-3fef-4b27-b7ee-eb136ff06ebc",
   "metadata": {},
   "source": [
    "### Sort Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fc569c-9d02-4619-82bf-ebf4e4caaf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an inhomogenous numpy array from the training set\n",
    "X_train = np.array(X_train, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc1d8b7-225a-43b4-a3a7-344569269d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an array containing the sequence lengths\n",
    "sequence_lengths = list(map(lambda x: len(x), X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360be4d-b70c-4fe6-9515-a0b904e0fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the array but only get the indices\n",
    "sorted_indices = np.argsort(sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0200e456-a746-417d-bc9c-ca4f75fa973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75abd9cc-7c09-40b0-b5a0-619bfc2e5a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now sort the X and y train arrays according to the sorted indicds\n",
    "X_train = X_train[sorted_indices]\n",
    "y_train = y_train[sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9070a40-d333-4008-9a8b-edb277a5209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the previously found values still correlate\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == search_y:\n",
    "        print(X_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e10faa-ea20-42bb-bd97-75395da099c9",
   "metadata": {},
   "source": [
    "### Ragged Tensor Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86fae6d-0aa3-4bb5-97ec-e2e8920e1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does not work since the sequences are of different length\n",
    "# X_test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d39e886-2279-4bbb-a838-c44e26ba3ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train_tensor = tf.ragged.constant(X_train, dtype=tf.int8, ragged_rank=1, row_splits_dtype=tf.int32)\n",
    "X_val_tensor = tf.ragged.constant(X_val, dtype=tf.int8, ragged_rank=1, row_splits_dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850a17ad-bb52-4365-9424-3447b37529ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dataset = tf.data.Dataset.from_tensor_slices((X_train_tensor, y_train))\n",
    "X_val_dataset = tf.data.Dataset.from_tensor_slices((X_val_tensor, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c506f768-6e15-4c6f-872b-0038d177dd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907d3def-dbf3-4830-aad6-5368d664e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a705eb41-faff-44f3-b262-88b883ec013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded batches from ragged tensors are not supported (yet)\n",
    "# it needs a work around creating a uniform tensor\n",
    "# idea from : https://github.com/tensorflow/tensorflow/issues/39163\n",
    "def reformat(data, label):\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805f9521-15da-4791-9a6a-5cf3dca18a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dataset = X_train_dataset.map(reformat)\n",
    "X_val_dataset = X_val_dataset.map(reformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e1c8a-efb9-455b-a3b3-cabc2d02daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataset (again) and create padded batches\n",
    "batch_size = 64\n",
    "X_train_dataset = X_train_dataset.padded_batch(batch_size)\n",
    "X_val_dataset = X_val_dataset.shuffle(buffer_size=len(X_val), seed=1202).padded_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2fa149-8b00-4210-8a33-2a3b810cbd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optinally repeat the dataset multiple times -> WHY?\n",
    "# rep = 3\n",
    "# X_train_dataset = X_train_dataset.repeat(rep)\n",
    "# X_val_dataset = X_val_dataset.repeat(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49244b0a-a161-457b-88d2-ba4417d89110",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalen = []\n",
    "ds_iterator = iter(X_train_dataset)\n",
    "for data, label in ds_iterator:\n",
    "    datalen.append(len(data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa0eb29-ea9c-4272-8cf9-682c9fe01451",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalen[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ba5416-4ed2-4d5f-8123-5e67e46b4dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing if keras can use the dataset\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=(None,4)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.fit(X_train_dataset, epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
