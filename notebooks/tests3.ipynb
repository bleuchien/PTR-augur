{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c369dca2-081c-4091-b8c2-22de545c1742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e77f26-729a-4fd7-95af-3ef1ca5f82b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 00:20:18.725349: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-18 00:20:18.726803: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-18 00:20:18.746503: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-18 00:20:18.746531: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-18 00:20:18.747245: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-18 00:20:18.750945: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-18 00:20:18.751268: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-18 00:20:19.162772: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# library dependencies\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import lzma\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import keras_tuner\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "744d8c8b-c5b6-4160-b27f-951984e281e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to store data as serialized binary structure lzma compressed\n",
    "def can_pickles(data, filename):\n",
    "    with lzma.LZMAFile(filename, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.DEFAULT_PROTOCOL)\n",
    "\n",
    "# method to retrieve data from a compressed pickle file (created with the method above)\n",
    "def uncan_pickles(filename):\n",
    "    with lzma.LZMAFile(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a158d917-4db6-4345-9f91-c90a9838d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the prepared data back\n",
    "X = uncan_pickles('../data/onehot_x_lung.pickle.xz')\n",
    "y = uncan_pickles('../data/onehot_y_lung.pickle.xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c81ee3-0b94-4066-b2b2-a34a255254eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an inhomogenous numpy array from the training set\n",
    "X = np.array(X, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8fdff1e-7c6e-46b3-8cac-d0d4328c7032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert type of target values from string to float\n",
    "y = np.array(y).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f8fc7e2-db72-4675-b30c-876b9a8f218c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8201, 8201)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ba1c1-776b-44c3-bbb5-5b310a55b9b0",
   "metadata": {},
   "source": [
    "### Rewrite to only use TF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fdd5f31-f8c3-48d7-a710-ddc24dabcf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data before building a dataset\n",
    "# helps to make sure that the input data is shuffled without getting knots in the brain due to lazy execution\n",
    "rng = np.random.default_rng(1202)\n",
    "rand_idx = np.arange(len(X))\n",
    "rng.shuffle(rand_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0c4afef-cfa9-424c-a843-7d607ec34e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4301, 1552, 6386, ..., 3572, 5989, 6785])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1be8801-d515-4bc0-8358-0f763369f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now sort the X and y train arrays according to the sorted indicds\n",
    "X = X[rand_idx]\n",
    "y = y[rand_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f348ac90-7a48-424d-a669-d924187e85be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 3s, sys: 3min 5s, total: 21min 9s\n",
      "Wall time: 21min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = tf.ragged.constant(X, dtype=tf.int8, ragged_rank=1, row_splits_dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7d3a1f93-7fe0-4f99-97e1-d603f15c8f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ds = tf.data.Dataset.from_tensor_slices(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "50cf7a73-f887-46a8-93b4-7662efef1c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ds = tf.data.Dataset.from_tensor_slices(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3e281294-fc8a-4298-94a2-5e113abaa429",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "TypeError: object of type 'RaggedTensor' has no len()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/scratch/hawk/uolbsc/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/data/util/structure.py:105\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    104\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     spec \u001b[38;5;241m=\u001b[39m \u001b[43mtype_spec_from_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m   \u001b[38;5;66;03m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[1;32m    108\u001b[0m   \u001b[38;5;66;03m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/hawk/uolbsc/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/data/util/structure.py:514\u001b[0m, in \u001b[0;36mtype_spec_from_value\u001b[0;34m(element, use_fallback)\u001b[0m\n\u001b[1;32m    511\u001b[0m     logging\u001b[38;5;241m.\u001b[39mvlog(\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to convert \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m to tensor: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mtype\u001b[39m(element)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, e))\n\u001b[0;32m--> 514\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not build a `TypeSpec` for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    515\u001b[0m     element,\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mtype\u001b[39m(element)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not build a `TypeSpec` for [<tf.RaggedTensor [[[1, 0, 0, 0],\n  [0, 0, 1, 0],\n  [1, 0, 0, 0],\n  ...,\n  [1, 0, 0, 0],\n  [1, 0, 0, 0],\n  [1, 0, 0, 0]], [[1, 0, 0, 0],\n                  [0, 0, 1, 0],\n                  [0, 1, 0, 0],\n                  ...,\n                  [0, 1, 0, 0],\n                  [0, 1, 0, 0],\n                  [1, 0, 0, 0]], [[0, 0, 1, 0],\n                                  [0, 0, 0, 1],\n                                  [0, 0, 0, 1],\n                                  ...,\n                                  [0, 1, 0, 0],\n                                  [1, 0, 0, 0],\n                                  [1, 0, 0, 0]], ..., [[0, 0, 1, 0],\n                                                       [0, 0, 0, 1],\n                                                       [0, 0, 1, 0],\n                                                       ...,\n                                                       [0, 1, 0, 0],\n                                                       [0, 0, 1, 0],\n                                                       [0, 0, 0, 1]],\n [[0, 0, 0, 1],\n  [0, 1, 0, 0],\n  [0, 0, 0, 1],\n  ...,\n  [0, 1, 0, 0],\n  [0, 0, 0, 1],\n  [0, 1, 0, 0]], [[1, 0, 0, 0],\n                  [0, 0, 0, 1],\n                  [0, 1, 0, 0],\n                  ...,\n                  [1, 0, 0, 0],\n                  [0, 0, 0, 1],\n                  [0, 1, 0, 0]]]>, array([4.195, 5.272, 5.73 , ..., 5.591, 5.996, 5.452])] with type list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# full_dataset = tf.data.Dataset.from_tensor_slices((X, y))\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# full_dataset = tf.data.Dataset.zip((X_ds, y_ds))\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m full_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/hawk/uolbsc/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:825\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# from_tensor_slices_op -> dataset_ops).\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_tensor_slices_op\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_tensor_slices_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/hawk/uolbsc/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:25\u001b[0m, in \u001b[0;36m_from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 25\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/hawk/uolbsc/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:33\u001b[0m, in \u001b[0;36m_TensorSliceDataset.__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, is_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m   element \u001b[38;5;241m=\u001b[39m \u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m   batched_spec \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[1;32m     35\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[0;32m/scratch/hawk/uolbsc/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/data/util/structure.py:110\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    105\u001b[0m     spec \u001b[38;5;241m=\u001b[39m type_spec_from_value(t, use_fallback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m   \u001b[38;5;66;03m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[1;32m    108\u001b[0m   \u001b[38;5;66;03m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m   normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 110\u001b[0m       \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m   \u001b[38;5;66;03m# To avoid a circular dependency between dataset_ops and structure,\u001b[39;00m\n\u001b[1;32m    113\u001b[0m   \u001b[38;5;66;03m# we check the class name instead of using `isinstance`.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetSpec\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/scratch/hawk/uolbsc/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/hawk/uolbsc/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/framework/ops.py:696\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[1;32m    695\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[0;32m--> 696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/hawk/uolbsc/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/hawk/uolbsc/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/framework/constant_op.py:335\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    333\u001b[0m                                          as_ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    334\u001b[0m   _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[0;32m--> 335\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/hawk/uolbsc/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m/scratch/hawk/uolbsc/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/framework/constant_op.py:271\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(\n\u001b[1;32m    174\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[1;32m    176\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/hawk/uolbsc/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/framework/constant_op.py:284\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    283\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 284\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[1;32m    288\u001b[0m )\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[0;32m/scratch/hawk/uolbsc/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/framework/constant_op.py:296\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(\n\u001b[1;32m    293\u001b[0m     ctx, value, dtype, shape, verify_shape\n\u001b[1;32m    294\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase:\n\u001b[1;32m    295\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m/scratch/hawk/uolbsc/PTR-augur/notebooks/.venv/lib64/python3.11/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: TypeError: object of type 'RaggedTensor' has no len()\n"
     ]
    }
   ],
   "source": [
    "# full_dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "# full_dataset = tf.data.Dataset.zip((X_ds, y_ds))\n",
    "full_dataset = tf.data.Dataset.from_tensor_slices([X, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "621714f3-4808-473f-8536-d4d453800c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RaggedTensorSpec(TensorShape([None, 4]), tf.int8, 0, tf.int32),\n",
       " TensorSpec(shape=(), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd7849cd-fa7e-4cf2-9644-b50d4dcbd32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded batches from ragged tensors are not supported (yet)\n",
    "# it needs a work around creating a uniform tensor\n",
    "# idea from : https://github.com/tensorflow/tensorflow/issues/39163\n",
    "def reformat(data, label):\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d9201ea-bd7e-414a-a205-c23df03f36a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = full_dataset.map(reformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e27bb74-d287-4b85-a036-186ed1da6901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 4), dtype=tf.int8, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9aa1c0a-e6a4-45f5-9f5c-fdf322828ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_dataset = full_dataset.take(math.ceil(len(full_dataset) * 0.8))\n",
    "test_dataset = full_dataset.skip(math.ceil(len(full_dataset) * 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9041f63b-e44c-4a91-b9a5-a33e95a8452b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8201, 6561, 1640, 8201)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_full = len(full_dataset)\n",
    "l_train = len(train_all_dataset)\n",
    "l_test = len(test_dataset)\n",
    "l_full, l_train, l_test, l_train + l_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a513f6c-c646-44ee-8565-d2b772d72a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "239615db-63a3-4f5c-a01b-88605b6b975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for one shot training sort before apply padding\n",
    "train_dataset = train_all_dataset.take(math.ceil(len(train_all_dataset) * 0.8))\n",
    "val_dataset = train_all_dataset.skip(math.ceil(len(train_all_dataset) * 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9939cf-d621-494f-929d-c41d59a21765",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c31bcf1c-c446-4e24-9a83-9f7df9969291",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'padded_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(train_dataset, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m----> 2\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadded_batch\u001b[49m(batch_size)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'padded_batch'"
     ]
    }
   ],
   "source": [
    "train_dataset = sorted(train_dataset, key=lambda x: len(x[0]))\n",
    "train_dataset = train_dataset.padded_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865e02bd-a21e-4f0b-8c9d-e583e9ebd565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def danq_model():\n",
    "    inputs = layers.Input(shape=(None, 4))\n",
    "\n",
    "    conv = layers.Conv1D(\n",
    "        filters=320,\n",
    "        kernel_size=26,\n",
    "        padding='valid',\n",
    "        activation='relu'\n",
    "    )(inputs)\n",
    "    pool = layers.MaxPooling1D(\n",
    "        pool_size=13,\n",
    "        strides=13\n",
    "    )(conv)\n",
    "    drop1 = layers.Dropout(\n",
    "        rate=0.2\n",
    "    )(pool)\n",
    "\n",
    "    forward_layer = layers.LSTM(units=320, return_sequences=True)\n",
    "    backward_layer = layers.LSTM(units=320, return_sequences=True, go_backwards=True)\n",
    "    bilstm = layers.Bidirectional(\n",
    "        forward_layer, backward_layer=backward_layer\n",
    "    )(drop1)\n",
    "    drop2 = layers.Dropout(\n",
    "        rate=0.2\n",
    "    )(bilstm)\n",
    "    # flat = layers.Flatten()(drop2)\n",
    "    gmp = layers.GlobalMaxPool1D()(drop2)\n",
    "    dense1 = layers.Dense(\n",
    "        units=925,\n",
    "        activation='relu'\n",
    "    )(gmp)\n",
    "    outputs = layers.Dense(\n",
    "        units=1\n",
    "    )(dense1)\n",
    "    \n",
    "    # model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='DanQModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e34b1cd-668b-43c2-bfe1-c03b0b366e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, tds, vds, epochs=100):\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        metrics=keras.metrics.MeanAbsoluteError()\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        tds,\n",
    "        epochs=epochs,\n",
    "        validation_data=vds,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    val_mse, val_mae = model.evalute(test_dataset)\n",
    "\n",
    "    return val_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "899fb345-6bfd-4495-a6e4-cb784d05ed37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.take_op._TakeDataset"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_all_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "54361993-11ff-4679-b867-e4fa6b302d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PaddedBatchDataset element_spec=(TensorSpec(shape=(None, None, 4), dtype=tf.int8, name=None), TensorSpec(shape=(None,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = train_all_dataset.padded_batch(batch_size)\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "467b0cbb-1373-4ce4-ac7f-081c7b8c687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bbb3c329-9173-4505-bcf6-b5c12b42cb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_WindowDataset element_spec=(DatasetSpec(TensorSpec(shape=(None, 4), dtype=tf.int8, name=None), TensorShape([])), DatasetSpec(TensorSpec(shape=(), dtype=tf.float64, name=None), TensorShape([])))>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all_dataset.window(math.ceil(len(train_all_dataset) / num_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6e317791-9978-4df8-a9b6-46b3a22c7458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<_VariantDataset element_spec=TensorSpec(shape=(None, 4), dtype=tf.int8, name=None)>, <_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.float64, name=None)>)\n",
      "TensorSpec(shape=(None, 4), dtype=tf.int8, name=None)\n",
      "<class 'tuple'>\n",
      "<class 'tensorflow.python.data.ops.zip_op._ZipDataset'>\n"
     ]
    }
   ],
   "source": [
    "for w in train_all_dataset.window(math.ceil(len(train_all_dataset) / num_splits)):\n",
    "    print(w)\n",
    "    print(w[0].element_spec)\n",
    "    print(type(w))\n",
    "    tmp = tf.data.Dataset.zip(w)\n",
    "    print(type(tmp))\n",
    "    tmp = tmp.padded_batch(batch_size)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0548609e-bd61-49ac-a9c4-9d5a65bebae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cross validation split train\n",
    "all_scores = []\n",
    "num_splits = 5\n",
    "splits = train_all_dataset.window(math.ceil(len(train_all_dataset) / num_splits))\n",
    "\n",
    "print('k-fold cross validation with', num_splits)\n",
    "\n",
    "for i in range(num_splits):\n",
    "    print('  split:', i)\n",
    "    \n",
    "    train_split_id = (i + 1) % num_splits\n",
    "    for l, p in enumerate(splits):\n",
    "        if l == i:\n",
    "            val_ds = tf.data.Dataset.from_tensor(p)\n",
    "        elif l == train_split_id:\n",
    "            train_ds = tf.data.Dataset.from_tensor(p)\n",
    "        else:\n",
    "            # train_ds.concatenate(tf.data.Dataset.zip(p))\n",
    "            train_ds.concatenate(p)\n",
    "            \n",
    "    # train_ds = tf.data.Dataset.from_tensors(train_ds)\n",
    "    # train_ds = tf.data.experimental.from_variant(train_ds, structure=(tf.TensorSpec(shape=(None, 4), dtype=tf.int8, name=None), tf.TensorSpec(shape=(), dtype=tf.float64, name=None)))\n",
    "    # train_ds = tf.data.Dataset.from_variant_tensor_slices(train_ds) # not available in V2 any more\n",
    "    # train_ds = tf.data.Dataset(train_ds)\n",
    "\n",
    "    print(val_ds)\n",
    "    \n",
    "    for argh in train_ds:\n",
    "        print(argh)\n",
    "        break\n",
    "\n",
    "    train_ds = sorted(train_ds, key=lambda x: len(x[0]))\n",
    "    train_ds = train_ds.padded_batch(batch_size)\n",
    "\n",
    "    # score = run_model(danq_model, train_ds, val_ds)\n",
    "    print('  score:', score)\n",
    "    \n",
    "    all_scores.append(score)\n",
    "\n",
    "# calculate score\n",
    "mean_score = np.mean(all_scores)\n",
    "print('mean score:', mean_score)\n",
    "\n",
    "# rerun training with train_all_dataset\n",
    "# train_all_dataset = sorted(train_all_dataset, key=lambda x: len(x[0]))\n",
    "# train_all_dataset = train_all_dataset.padded_batch(batch_size)\n",
    "# run training\n",
    "# score = run_model(danq_model, train_all_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f9e27c-367e-4a55-acca-b4769c112f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(7).window(3)\n",
    "print(dataset.element_spec)\n",
    "for window in dataset:\n",
    "    print(window)\n",
    "    print(window.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e005af1-0ab8-431f-8082-9fa65651e650",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Split data in train and test subsets and then split the train subset again in train and validation.\n",
    "\n",
    "A simple verification if the X and y correlation are preserved on the split is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2887350-10ba-419f-a459-e67ecfe76be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train and test sub sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7893d01f-3bab-4688-a74e-f4410481752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad test input (variable input is not accepted)\n",
    "# X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c006f1-3ccd-4c62-b35f-a25bec734858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the train set again in train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b798c961-77f9-44b6-b9d5-831c3b7548d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the first unique PTR value that is also in y_train\n",
    "train_idx = 0\n",
    "for i in range(len(y)):\n",
    "    count = 0\n",
    "    for l in range(len(y)):\n",
    "        if i != l and y[i] == y[l]:\n",
    "            count += 1\n",
    "            continue\n",
    "    if count == 0:\n",
    "        for m in range(len(y_train)):\n",
    "            if y[i] == y_train[m]:\n",
    "                train_idx = m\n",
    "                break\n",
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234c4bb9-7c12-4e8e-b93b-c4f9405f14df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample\n",
    "X_train[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657da951-2406-445d-9ab8-d41aa046a2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the matching target\n",
    "search_y = y_train[train_idx]\n",
    "search_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5188a4b-2bf1-4f44-bd45-52ecf0ea5f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the target value in the raw dataset\n",
    "full_idx = 0\n",
    "for i in range(len(y)):\n",
    "    if y[i] == search_y:\n",
    "        print(i)\n",
    "        full_idx = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb46dfb-092e-436d-bab7-d5300f0f8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare if the raw dataset entry matches the subset entry\n",
    "if X[full_idx].all() == X_train[train_idx].all():\n",
    "    print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeca4a7d-b028-4acb-8a76-faf7497681de",
   "metadata": {},
   "source": [
    "### Sort Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea6f45-febe-4cd0-964d-c886806e8a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an inhomogenous numpy array from the training set\n",
    "X_train = np.array(X_train, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e02a837-8104-4f68-aad2-49ca125dea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an array containing the sequence lengths\n",
    "sequence_lengths = list(map(lambda x: len(x), X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2913cbe6-1f14-4462-bf34-590822895f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the array but only get the indices\n",
    "sorted_indices = np.argsort(sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d55e0-0b14-4a29-85fa-c2b5fe000ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ca42ee-bec7-43b2-b3ea-1cd2fddfb185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now sort the X and y train arrays according to the sorted indicds\n",
    "X_train = X_train[sorted_indices]\n",
    "y_train = y_train[sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f465866f-98b3-4e4b-8cc9-f5d85b224c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the previously found values still correlate\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == search_y:\n",
    "        print(X_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ac5caf-4878-43e8-aecf-cf92eb8a412c",
   "metadata": {},
   "source": [
    "### Ragged Tensor Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb368d0d-7709-4f26-985b-1e10b864631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does not work since the sequences are of different length\n",
    "# X_test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd3ec9-0b95-4324-a1da-924cf138fc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = tf.ragged.constant(X_train, dtype=tf.int8, ragged_rank=1, row_splits_dtype=tf.int32)\n",
    "X_val_tensor = tf.ragged.constant(X_val, dtype=tf.int8, ragged_rank=1, row_splits_dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fbd6df-8108-4388-bc84-0d1230d9141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dataset = tf.data.Dataset.from_tensor_slices((X_train_tensor, y_train))\n",
    "X_val_dataset = tf.data.Dataset.from_tensor_slices((X_val_tensor, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea964ea-8d40-442f-ac72-c0d9eb8a673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fbca89-f81f-470d-b4b1-1a5391e47787",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa5a420-106a-4139-944e-0bae3d3d4f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded batches from ragged tensors are not supported (yet)\n",
    "# it needs a work around creating a uniform tensor\n",
    "# idea from : https://github.com/tensorflow/tensorflow/issues/39163\n",
    "def reformat(data, label):\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70796b37-e246-470d-9113-4280a686c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dataset = X_train_dataset.map(reformat)\n",
    "X_val_dataset = X_val_dataset.map(reformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26958f0b-070f-4efc-8cc4-077afcd4bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = sorted(X_train_dataset, key=lambda x: len(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f16fd08-5894-444b-b34d-630fd6815455",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = X_val_dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df40066-be16-4e7f-b0a3-c7b0ede37059",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for x, y in bar:\n",
    "    print(y)\n",
    "    if count == 5:\n",
    "        break    \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8552d5c-71b2-4efb-b769-6c32e1fbdde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "moo, boo = zip(*bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a31312-7800-4925-8813-f093a428d563",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = tf.ragged.constant(moo, dtype=tf.int8, ragged_rank=1, row_splits_dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da3d8d-7ef4-419b-a82a-dd8bd65845ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e6ad2a-a009-42d6-8bbd-dde271d10d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = math.ceil(len(X_val_dataset) * 0.3)\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffec683-4605-4b19-99b6-5a586ed819b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = X_val_dataset.window(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63b203-b396-4234-8e2a-3f957c5f5703",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in foo:\n",
    "    print(w)\n",
    "    print(len(w[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe997a61-a349-4779-9ad1-5993bacbd953",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, s in enumerate(foo):\n",
    "    print(i, s)\n",
    "    s.concatenate(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476135f3-5707-4cca-a16a-fde48cce81c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10347d93-c845-4fb2-bd6a-8f4c793b83f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataset (again) and create padded batches\n",
    "batch_size = 32\n",
    "X_train_dataset = X_train_dataset.shuffle(buffer_size=len(X_train), seed=1202).padded_batch(batch_size)\n",
    "X_val_dataset = X_val_dataset.shuffle(buffer_size=len(X_val), seed=1202).padded_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c4b469-a34e-4081-9b88-dc2871357fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optinally repeat the dataset multiple times -> WHY?\n",
    "# rep = 3\n",
    "# X_train_dataset = X_train_dataset.repeat(rep)\n",
    "# X_val_dataset = X_val_dataset.repeat(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e786e-bcec-4fb6-8016-9496f8649f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalen = []\n",
    "ds_iterator = iter(X_train_dataset)\n",
    "for data, label in ds_iterator:\n",
    "    datalen.append(len(data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e27a52-e9e6-4de2-8f7c-5a25af320f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalen[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a047d1ab-f416-4bf0-acbf-f4b863cebc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing if keras can use the dataset\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=(None,4)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.fit(X_train_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2253c3a-538c-4fec-a818-38267aaf87f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
