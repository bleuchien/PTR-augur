{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c369dca2-081c-4091-b8c2-22de545c1742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e77f26-729a-4fd7-95af-3ef1ca5f82b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 22:01:38.959739: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-18 22:01:38.961056: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-18 22:01:38.978652: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-18 22:01:38.978668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-18 22:01:38.979169: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-18 22:01:38.982147: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-18 22:01:38.982458: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-18 22:01:39.384462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# library dependencies\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import lzma\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import keras_tuner\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "744d8c8b-c5b6-4160-b27f-951984e281e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to store data as serialized binary structure lzma compressed\n",
    "def can_pickles(data, filename):\n",
    "    with lzma.LZMAFile(filename, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.DEFAULT_PROTOCOL)\n",
    "\n",
    "# method to retrieve data from a compressed pickle file (created with the method above)\n",
    "def uncan_pickles(filename):\n",
    "    with lzma.LZMAFile(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a158d917-4db6-4345-9f91-c90a9838d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the prepared data\n",
    "X = uncan_pickles('../data/onehot_x_lung.pickle.xz')\n",
    "y = uncan_pickles('../data/onehot_y_lung.pickle.xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c81ee3-0b94-4066-b2b2-a34a255254eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an inhomogenous numpy array from the training set\n",
    "X = np.array(X, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8fdff1e-7c6e-46b3-8cac-d0d4328c7032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert type of target values from string to float\n",
    "y = np.array(y).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f8fc7e2-db72-4675-b30c-876b9a8f218c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8201, 8201)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ba1c1-776b-44c3-bbb5-5b310a55b9b0",
   "metadata": {},
   "source": [
    "### Rewrite to only use TF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fdd5f31-f8c3-48d7-a710-ddc24dabcf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data before building a dataset\n",
    "# helps to make sure that the input data is shuffled without getting knots in the brain due to lazy execution\n",
    "rng = np.random.default_rng(1202)\n",
    "rand_idx = np.arange(len(X))\n",
    "rng.shuffle(rand_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0c4afef-cfa9-424c-a843-7d607ec34e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4301, 1552, 6386, ..., 3572, 5989, 6785])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show that the list of integers is randomized\n",
    "rand_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1be8801-d515-4bc0-8358-0f763369f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now sort the X and y train arrays according to the sorted indicds\n",
    "X = X[rand_idx]\n",
    "y = y[rand_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f348ac90-7a48-424d-a669-d924187e85be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 52s, sys: 2min 47s, total: 18min 40s\n",
      "Wall time: 18min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create the ragged tensor from the feature array\n",
    "X = tf.ragged.constant(X, dtype=tf.int8, ragged_rank=1, row_splits_dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a1f93-7fe0-4f99-97e1-d603f15c8f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following creates datasets from feature and label array\n",
    "# this is an alternative version to create the full dataset\n",
    "# X_ds = tf.data.Dataset.from_tensor_slices(X)\n",
    "# y_ds = tf.data.Dataset.from_tensor_slices(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e281294-fc8a-4298-94a2-5e113abaa429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the TF documentation\n",
    "full_dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "\n",
    "# this version where each component is a dataset anlready that is then joined works too\n",
    "# full_dataset = tf.data.Dataset.zip((X_ds, y_ds))\n",
    "\n",
    "# this was suggested on the TF forum for this problem\n",
    "# but does not work because the ragged tensor doesn't have a len\n",
    "# full_dataset = tf.data.Dataset.from_tensor_slices([X, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "621714f3-4808-473f-8536-d4d453800c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RaggedTensorSpec(TensorShape([None, 4]), tf.int8, 0, tf.int32),\n",
       " TensorSpec(shape=(), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at what the dataset elements look like\n",
    "full_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd7849cd-fa7e-4cf2-9644-b50d4dcbd32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded batches from ragged tensors are not supported (yet)\n",
    "# it needs a work around creating a uniform tensor\n",
    "# idea from : https://github.com/tensorflow/tensorflow/issues/39163\n",
    "def reformat(data, label):\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d9201ea-bd7e-414a-a205-c23df03f36a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the transformation\n",
    "full_dataset = full_dataset.map(reformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e27bb74-d287-4b85-a036-186ed1da6901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 4), dtype=tf.int8, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now look again - the ragged tensor is gone in this version\n",
    "full_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "466b2cf7-500e-4e04-8a71-0293652e67a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 0 0 0]\n",
      " [0 0 1 0]\n",
      " [1 0 0 0]\n",
      " ...\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]], shape=(3882, 4), dtype=int8)\n",
      "tf.Tensor(4.195, shape=(), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[1 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 1 0 0]\n",
      " ...\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]], shape=(537, 4), dtype=int8)\n",
      "tf.Tensor(5.272, shape=(), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " ...\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]], shape=(1764, 4), dtype=int8)\n",
      "tf.Tensor(5.73, shape=(), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " ...\n",
      " [0 0 1 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]], shape=(6298, 4), dtype=int8)\n",
      "tf.Tensor(4.298, shape=(), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[1 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " ...\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]], shape=(3394, 4), dtype=int8)\n",
      "tf.Tensor(4.069, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# check the content of the dataset again\n",
    "count = 0\n",
    "for f, l in full_dataset:\n",
    "    print(f)\n",
    "    print(l)\n",
    "    count += 1\n",
    "    if count == 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9aa1c0a-e6a4-45f5-9f5c-fdf322828ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the full dataset in two sub sets\n",
    "train_all_dataset = full_dataset.take(math.ceil(len(full_dataset) * 0.8))\n",
    "test_dataset = full_dataset.skip(math.ceil(len(full_dataset) * 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9041f63b-e44c-4a91-b9a5-a33e95a8452b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8201, 6561, 1640, 8201)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify the length of the datasets\n",
    "l_full = len(full_dataset)\n",
    "l_train = len(train_all_dataset)\n",
    "l_test = len(test_dataset)\n",
    "l_full, l_train, l_test, l_train + l_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a513f6c-c646-44ee-8565-d2b772d72a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of each batch\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a177d5-ff0f-424e-bd91-54507f71eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following two cells are _ONLY_ for TEST and VALIDATION split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239615db-63a3-4f5c-a01b-88605b6b975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for one shot training sort before apply padding\n",
    "train_dataset = train_all_dataset.take(math.ceil(len(train_all_dataset) * 0.8))\n",
    "val_dataset = train_all_dataset.skip(math.ceil(len(train_all_dataset) * 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31bcf1c-c446-4e24-9a83-9f7df9969291",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = sorted(train_dataset, key=lambda x: len(x[0]))\n",
    "train_dataset = train_dataset.padded_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf22f89-7da3-41bf-9833-7411ad478709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "865e02bd-a21e-4f0b-8c9d-e583e9ebd565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    inputs = layers.Input(shape=(None, 4))\n",
    "    conv1 = layers.Conv1D(\n",
    "        filters=32,\n",
    "        kernel_size=10,\n",
    "        strides=1,\n",
    "        activation='relu',\n",
    "        padding='valid'\n",
    "    )(inputs)\n",
    "    norm1 = layers.BatchNormalization()(conv1)\n",
    "    drop1 = layers.Dropout(\n",
    "        rate=0.1\n",
    "    )(norm1)\n",
    "    pool1 = layers.GlobalMaxPool1D()(drop1)\n",
    "    dense = layers.Dense(16, activation='relu')(pool1)\n",
    "    outputs = layers.Dense(1)(dense)\n",
    "    \n",
    "    # model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='Test')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e34b1cd-668b-43c2-bfe1-c03b0b366e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, tds, vds, epochs=100):\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        metrics=keras.metrics.MeanAbsoluteError()\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        tds,\n",
    "        epochs=epochs,\n",
    "        validation_data=vds,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    val_mse, val_mae = model.evalute(test_dataset)\n",
    "\n",
    "    return val_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899fb345-6bfd-4495-a6e4-cb784d05ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_all_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54361993-11ff-4679-b867-e4fa6b302d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = train_all_dataset.padded_batch(batch_size)\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b0cbb-1373-4ce4-ac7f-081c7b8c687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb3c329-9173-4505-bcf6-b5c12b42cb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_dataset.window(math.ceil(len(train_all_dataset) / num_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e317791-9978-4df8-a9b6-46b3a22c7458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<_VariantDataset element_spec=TensorSpec(shape=(None, 4), dtype=tf.int8, name=None)>, <_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.float64, name=None)>)\n",
      "TensorSpec(shape=(None, 4), dtype=tf.int8, name=None)\n",
      "<class 'tuple'>\n",
      "<class 'tensorflow.python.data.ops.zip_op._ZipDataset'>\n",
      "(<_VariantDataset element_spec=TensorSpec(shape=(None, 4), dtype=tf.int8, name=None)>, <_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.float64, name=None)>)\n",
      "TensorSpec(shape=(None, 4), dtype=tf.int8, name=None)\n",
      "<class 'tuple'>\n",
      "<class 'tensorflow.python.data.ops.zip_op._ZipDataset'>\n",
      "(<_VariantDataset element_spec=TensorSpec(shape=(None, 4), dtype=tf.int8, name=None)>, <_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.float64, name=None)>)\n",
      "TensorSpec(shape=(None, 4), dtype=tf.int8, name=None)\n",
      "<class 'tuple'>\n",
      "<class 'tensorflow.python.data.ops.zip_op._ZipDataset'>\n",
      "(<_VariantDataset element_spec=TensorSpec(shape=(None, 4), dtype=tf.int8, name=None)>, <_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.float64, name=None)>)\n",
      "TensorSpec(shape=(None, 4), dtype=tf.int8, name=None)\n",
      "<class 'tuple'>\n",
      "<class 'tensorflow.python.data.ops.zip_op._ZipDataset'>\n",
      "(<_VariantDataset element_spec=TensorSpec(shape=(None, 4), dtype=tf.int8, name=None)>, <_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.float64, name=None)>)\n",
      "TensorSpec(shape=(None, 4), dtype=tf.int8, name=None)\n",
      "<class 'tuple'>\n",
      "<class 'tensorflow.python.data.ops.zip_op._ZipDataset'>\n"
     ]
    }
   ],
   "source": [
    "conc_ds = None\n",
    "for w in train_all_dataset.window(math.ceil(len(train_all_dataset) / num_splits)):\n",
    "    print(w)\n",
    "    print(w[0].element_spec)\n",
    "    print(type(w))\n",
    "    tmp = tf.data.Dataset.zip(w)\n",
    "    print(type(tmp))\n",
    "    if conc_ds == None:\n",
    "        conc_ds = tmp\n",
    "    else:\n",
    "        conc_ds.concatenate(tmp)\n",
    "        \n",
    "conc_ds = conc_ds.padded_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0548609e-bd61-49ac-a9c4-9d5a65bebae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold cross validation with 5 splits\n",
      "  split: 0\n",
      "<class 'tensorflow.python.data.ops.zip_op._ZipDataset'>\n",
      "<class 'tensorflow.python.data.ops.zip_op._ZipDataset'>\n",
      "(<tf.Tensor: shape=(2987, 4), dtype=int8, numpy=\n",
      "array([[0, 0, 1, 0],\n",
      "       [0, 0, 0, 1],\n",
      "       [1, 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 0, 1, 0],\n",
      "       [0, 0, 0, 1]], dtype=int8)>, <tf.Tensor: shape=(), dtype=float64, numpy=5.135>)\n",
      "  score: 0\n",
      "  split: 1\n",
      "<class 'tensorflow.python.data.ops.zip_op._ZipDataset'>\n",
      "<class 'tensorflow.python.data.ops.zip_op._ZipDataset'>\n",
      "(<tf.Tensor: shape=(3882, 4), dtype=int8, numpy=\n",
      "array([[1, 0, 0, 0],\n",
      "       [0, 0, 1, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 0, 0, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [1, 0, 0, 0]], dtype=int8)>, <tf.Tensor: shape=(), dtype=float64, numpy=4.195>)\n",
      "  score: 0\n",
      "  split: 2\n",
      "<class 'tensorflow.python.data.ops.zip_op._ZipDataset'>\n",
      "<class 'tensorflow.python.data.ops.zip_op._ZipDataset'>\n",
      "(<tf.Tensor: shape=(3882, 4), dtype=int8, numpy=\n",
      "array([[1, 0, 0, 0],\n",
      "       [0, 0, 1, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 0, 0, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [1, 0, 0, 0]], dtype=int8)>, <tf.Tensor: shape=(), dtype=float64, numpy=4.195>)\n",
      "  score: 0\n",
      "  split: 3\n",
      "<class 'tensorflow.python.data.ops.zip_op._ZipDataset'>\n",
      "<class 'tensorflow.python.data.ops.zip_op._ZipDataset'>\n",
      "(<tf.Tensor: shape=(3882, 4), dtype=int8, numpy=\n",
      "array([[1, 0, 0, 0],\n",
      "       [0, 0, 1, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 0, 0, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [1, 0, 0, 0]], dtype=int8)>, <tf.Tensor: shape=(), dtype=float64, numpy=4.195>)\n",
      "  score: 0\n",
      "  split: 4\n",
      "<class 'tensorflow.python.data.ops.zip_op._ZipDataset'>\n",
      "<class 'tensorflow.python.data.ops.zip_op._ZipDataset'>\n",
      "(<tf.Tensor: shape=(3882, 4), dtype=int8, numpy=\n",
      "array([[1, 0, 0, 0],\n",
      "       [0, 0, 1, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 0, 0, 0],\n",
      "       [1, 0, 0, 0],\n",
      "       [1, 0, 0, 0]], dtype=int8)>, <tf.Tensor: shape=(), dtype=float64, numpy=4.195>)\n",
      "  score: 0\n",
      "mean score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 22:48:46.459483: W tensorflow/core/framework/dataset.cc:959] Input of Window will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    }
   ],
   "source": [
    "# for cross validation split train\n",
    "\n",
    "# list of scores of each fold\n",
    "all_scores = []\n",
    "# number of splits to perform\n",
    "num_splits = 5\n",
    "# size of each batch\n",
    "batch_size = 64\n",
    "\n",
    "# create the split datasets\n",
    "splits = train_all_dataset.window(math.ceil(len(train_all_dataset) / num_splits))\n",
    "\n",
    "print('k-fold cross validation with', num_splits, 'splits')\n",
    "\n",
    "# loop over the splits\n",
    "for i in range(num_splits):\n",
    "    print('  split:', i)\n",
    "\n",
    "    # initialize the training and validation dataset\n",
    "    val_ds = None\n",
    "    train_ds = None\n",
    "\n",
    "    # loop over all dataset splits\n",
    "    for l, p in enumerate(splits):\n",
    "        # create a temporary dataset from the tuple returned\n",
    "        # weird, probably ugly way to do this but for some reason the window operation\n",
    "        # does not return a valid dataset but a tuple of datasets\n",
    "        tmp_ds = tf.data.Dataset.zip(p)\n",
    "        \n",
    "        # use the DS with the same id as the current split run as validation set\n",
    "        if l == i:\n",
    "            val_ds = tmp_ds\n",
    "        # use everything else for training\n",
    "        elif train_ds == None:\n",
    "            train_ds = tmp_ds\n",
    "        else:\n",
    "            train_ds.concatenate(tmp_ds)\n",
    "\n",
    "    print(type(val_ds))\n",
    "    print(type(train_ds))\n",
    "    \n",
    "    for argh in train_ds:\n",
    "        print(argh)\n",
    "        break\n",
    "\n",
    "    # train_ds = sorted(train_ds, key=lambda x: len(x[0]))\n",
    "    train_ds = train_ds.padded_batch(batch_size)\n",
    "\n",
    "    score = 0\n",
    "    # score = run_model(test_model, train_ds, val_ds)\n",
    "    print('  score:', score)\n",
    "    \n",
    "    all_scores.append(score)\n",
    "\n",
    "# calculate score\n",
    "mean_score = np.mean(all_scores)\n",
    "print('mean score:', mean_score)\n",
    "\n",
    "# rerun training with train_all_dataset\n",
    "# train_all_dataset = sorted(train_all_dataset, key=lambda x: len(x[0]))\n",
    "# train_all_dataset = train_all_dataset.padded_batch(batch_size)\n",
    "# run training\n",
    "# score = run_model(danq_model, train_all_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f9e27c-367e-4a55-acca-b4769c112f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(7).window(3)\n",
    "print(dataset.element_spec)\n",
    "for window in dataset:\n",
    "    print(window)\n",
    "    print(window.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e005af1-0ab8-431f-8082-9fa65651e650",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Split data in train and test subsets and then split the train subset again in train and validation.\n",
    "\n",
    "A simple verification if the X and y correlation are preserved on the split is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2887350-10ba-419f-a459-e67ecfe76be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train and test sub sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7893d01f-3bab-4688-a74e-f4410481752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad test input (variable input is not accepted)\n",
    "# X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c006f1-3ccd-4c62-b35f-a25bec734858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the train set again in train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b798c961-77f9-44b6-b9d5-831c3b7548d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the first unique PTR value that is also in y_train\n",
    "train_idx = 0\n",
    "for i in range(len(y)):\n",
    "    count = 0\n",
    "    for l in range(len(y)):\n",
    "        if i != l and y[i] == y[l]:\n",
    "            count += 1\n",
    "            continue\n",
    "    if count == 0:\n",
    "        for m in range(len(y_train)):\n",
    "            if y[i] == y_train[m]:\n",
    "                train_idx = m\n",
    "                break\n",
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234c4bb9-7c12-4e8e-b93b-c4f9405f14df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample\n",
    "X_train[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657da951-2406-445d-9ab8-d41aa046a2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the matching target\n",
    "search_y = y_train[train_idx]\n",
    "search_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5188a4b-2bf1-4f44-bd45-52ecf0ea5f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the target value in the raw dataset\n",
    "full_idx = 0\n",
    "for i in range(len(y)):\n",
    "    if y[i] == search_y:\n",
    "        print(i)\n",
    "        full_idx = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb46dfb-092e-436d-bab7-d5300f0f8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare if the raw dataset entry matches the subset entry\n",
    "if X[full_idx].all() == X_train[train_idx].all():\n",
    "    print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeca4a7d-b028-4acb-8a76-faf7497681de",
   "metadata": {},
   "source": [
    "### Sort Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea6f45-febe-4cd0-964d-c886806e8a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an inhomogenous numpy array from the training set\n",
    "X_train = np.array(X_train, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e02a837-8104-4f68-aad2-49ca125dea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an array containing the sequence lengths\n",
    "sequence_lengths = list(map(lambda x: len(x), X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2913cbe6-1f14-4462-bf34-590822895f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the array but only get the indices\n",
    "sorted_indices = np.argsort(sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d55e0-0b14-4a29-85fa-c2b5fe000ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ca42ee-bec7-43b2-b3ea-1cd2fddfb185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now sort the X and y train arrays according to the sorted indicds\n",
    "X_train = X_train[sorted_indices]\n",
    "y_train = y_train[sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f465866f-98b3-4e4b-8cc9-f5d85b224c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the previously found values still correlate\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == search_y:\n",
    "        print(X_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ac5caf-4878-43e8-aecf-cf92eb8a412c",
   "metadata": {},
   "source": [
    "### Ragged Tensor Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb368d0d-7709-4f26-985b-1e10b864631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does not work since the sequences are of different length\n",
    "# X_test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd3ec9-0b95-4324-a1da-924cf138fc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = tf.ragged.constant(X_train, dtype=tf.int8, ragged_rank=1, row_splits_dtype=tf.int32)\n",
    "X_val_tensor = tf.ragged.constant(X_val, dtype=tf.int8, ragged_rank=1, row_splits_dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fbd6df-8108-4388-bc84-0d1230d9141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dataset = tf.data.Dataset.from_tensor_slices((X_train_tensor, y_train))\n",
    "X_val_dataset = tf.data.Dataset.from_tensor_slices((X_val_tensor, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea964ea-8d40-442f-ac72-c0d9eb8a673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fbca89-f81f-470d-b4b1-1a5391e47787",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa5a420-106a-4139-944e-0bae3d3d4f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded batches from ragged tensors are not supported (yet)\n",
    "# it needs a work around creating a uniform tensor\n",
    "# idea from : https://github.com/tensorflow/tensorflow/issues/39163\n",
    "def reformat(data, label):\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70796b37-e246-470d-9113-4280a686c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dataset = X_train_dataset.map(reformat)\n",
    "X_val_dataset = X_val_dataset.map(reformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26958f0b-070f-4efc-8cc4-077afcd4bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = sorted(X_train_dataset, key=lambda x: len(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f16fd08-5894-444b-b34d-630fd6815455",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = X_val_dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df40066-be16-4e7f-b0a3-c7b0ede37059",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for x, y in bar:\n",
    "    print(y)\n",
    "    if count == 5:\n",
    "        break    \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8552d5c-71b2-4efb-b769-6c32e1fbdde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "moo, boo = zip(*bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a31312-7800-4925-8813-f093a428d563",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = tf.ragged.constant(moo, dtype=tf.int8, ragged_rank=1, row_splits_dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da3d8d-7ef4-419b-a82a-dd8bd65845ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e6ad2a-a009-42d6-8bbd-dde271d10d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = math.ceil(len(X_val_dataset) * 0.3)\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffec683-4605-4b19-99b6-5a586ed819b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = X_val_dataset.window(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63b203-b396-4234-8e2a-3f957c5f5703",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in foo:\n",
    "    print(w)\n",
    "    print(len(w[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe997a61-a349-4779-9ad1-5993bacbd953",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, s in enumerate(foo):\n",
    "    print(i, s)\n",
    "    s.concatenate(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476135f3-5707-4cca-a16a-fde48cce81c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10347d93-c845-4fb2-bd6a-8f4c793b83f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataset (again) and create padded batches\n",
    "batch_size = 32\n",
    "X_train_dataset = X_train_dataset.shuffle(buffer_size=len(X_train), seed=1202).padded_batch(batch_size)\n",
    "X_val_dataset = X_val_dataset.shuffle(buffer_size=len(X_val), seed=1202).padded_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c4b469-a34e-4081-9b88-dc2871357fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optinally repeat the dataset multiple times -> WHY?\n",
    "# rep = 3\n",
    "# X_train_dataset = X_train_dataset.repeat(rep)\n",
    "# X_val_dataset = X_val_dataset.repeat(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e786e-bcec-4fb6-8016-9496f8649f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalen = []\n",
    "ds_iterator = iter(X_train_dataset)\n",
    "for data, label in ds_iterator:\n",
    "    datalen.append(len(data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e27a52-e9e6-4de2-8f7c-5a25af320f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalen[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a047d1ab-f416-4bf0-acbf-f4b863cebc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing if keras can use the dataset\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=(None,4)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.fit(X_train_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2253c3a-538c-4fec-a818-38267aaf87f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
